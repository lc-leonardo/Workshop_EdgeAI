{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6de436",
   "metadata": {},
   "source": [
    "# Workshop 3.1: Quantization for Efficient Inference - Hands-On Practice\n",
    "\n",
    "## Learning Objectives\n",
    "Upon completion of this notebook, you will be able to:\n",
    "- Articulate the theoretical foundations and practical motivations for quantization in deep neural networks, particularly in the context of resource-constrained edge AI deployments.\n",
    "- Analyze the principles and implications of FP16 quantization, including the conversion from 32-bit (FP32) to 16-bit (FP16) floating-point representations.\n",
    "- Implement post-training quantization workflows using PyTorch, and critically assess their impact on model performance.\n",
    "- Quantitatively evaluate the trade-offs between model size, computational efficiency, and predictive accuracy resulting from quantization.\n",
    "- Employ visualization techniques to systematically interpret the effects of quantization on neural network behavior and deployment characteristics.\n",
    "\n",
    "\n",
    "\n",
    "## What is Quantization?\n",
    "Quantization refers to the process of mapping continuous-valued parameters and activations of neural networks to a lower-precision numerical format. In the context of deep learning, this typically involves converting 32-bit floating-point (FP32) representations to reduced-precision formats such as 16-bit floating-point (FP16). The primary objectives are to decrease memory footprint, accelerate inference, and enable deployment on hardware with limited computational resources, while preserving model fidelity within acceptable bounds.\n",
    "\n",
    "\n",
    "\n",
    "This workshop focuses on FP16 quantization, which offers:\n",
    "- Substantial reduction in model storage requirements and memory bandwidth\n",
    "- Negligible degradation in predictive accuracy for most modern architectures\n",
    "- Broad compatibility with PyTorch and contemporary hardware accelerators\n",
    "- Straightforward integration into existing model development pipelines\n",
    "\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1712.05877\n",
    "\n",
    "\n",
    "---\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: This notebook contains code completion exercises marked with `# TODO:` comments. Fill in the missing code to complete the quantization workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d86d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.quantization\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675cd70f",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe80f4",
   "metadata": {},
   "source": [
    "### Understanding CIFAR-10: Why This Dataset?\n",
    "\n",
    "CIFAR-10 is perfect for learning quantization because:\n",
    "- **Small images (32Ã—32)** - Fast training and testing\n",
    "- **Realistic challenge** - 10 different object classes\n",
    "- **Edge AI relevant** - Similar to mobile camera applications\n",
    "- **Resource-friendly** - Doesn't require powerful hardware\n",
    "\n",
    "The small image size makes it ideal for:\n",
    "- Mobile device deployment\n",
    "- Edge computing scenarios\n",
    "- Real-time inference applications\n",
    "- Learning optimization techniques like quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b9aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define transforms for training and testing\n",
    "# HINT: Use transforms.Compose() with RandomCrop, RandomHorizontalFlip, ToTensor, and Normalize\n",
    "# The normalization values for CIFAR-10 are: mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # TODO: Add RandomCrop with size 32 and padding 4\n",
    "    # TODO: Add RandomHorizontalFlip\n",
    "    # TODO: Add ToTensor\n",
    "    # TODO: Add Normalize with the values mentioned above\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    # TODO: Add ToTensor\n",
    "    # TODO: Add Normalize with the same values as above\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=200, shuffle=False, num_workers=2)\n",
    "\n",
    "# CIFAR-10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54b547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images from CIFAR-10 dataset\n",
    "def visualize_cifar10_samples(dataset, classes, num_samples=12):\n",
    "    \"\"\"\n",
    "    Display a grid of sample images from CIFAR-10 with their class labels\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(12, 9))\n",
    "    fig.suptitle('CIFAR-10 Dataset Samples', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Get random samples\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        \n",
    "        # Get image and label\n",
    "        image, label = dataset[idx]\n",
    "        \n",
    "        # Convert tensor to numpy and denormalize for display\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            # Denormalize the image\n",
    "            mean = np.array([0.4914, 0.4822, 0.4465])\n",
    "            std = np.array([0.2023, 0.1994, 0.2010])\n",
    "            image = image.numpy().transpose(1, 2, 0)\n",
    "            image = image * std + mean\n",
    "            image = np.clip(image, 0, 1)\n",
    "        \n",
    "        # Display image\n",
    "        axes[row, col].imshow(image)\n",
    "        axes[row, col].set_title(f'Class: {classes[label]}', fontsize=12, fontweight='bold')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"CIFAR-10 Dataset Details:\")\n",
    "    print(f\"â€¢ Image size: 32Ã—32 pixels (small images)\")\n",
    "    print(f\"â€¢ Color channels: 3 (RGB)\")\n",
    "    print(f\"â€¢ Number of classes: {len(classes)}\")\n",
    "    print(f\"â€¢ Classes: {', '.join(classes)}\")\n",
    "    print(f\"â€¢ Training samples: {len(dataset)}\")\n",
    "    print(f\"â€¢ This is why we use MobileNetV2 - efficient for small images\")\n",
    "\n",
    "# Display sample images and dataset information\n",
    "print(\"CIFAR-10 Dataset Overview:\")\n",
    "visualize_cifar10_samples(trainset, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71cc968",
   "metadata": {},
   "source": [
    "## Step 2: Load Pre-trained MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defd9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the MobileNetV2 adaptation function\n",
    "def create_mobilenetv2_cifar10(num_classes=10, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create MobileNetV2 adapted for CIFAR-10\n",
    "    CIFAR-10 images are 32x32, smaller than ImageNet's 224x224\n",
    "    \"\"\"\n",
    "    # TODO: Load pre-trained MobileNetV2 using models.mobilenet_v2()\n",
    "    model = # Your code here\n",
    "    \n",
    "    # TODO: Modify the first convolution layer for smaller input size\n",
    "    # HINT: use nn.Conv2d()\n",
    "    model.features[0][0] = # Your code here\n",
    "    \n",
    "    # TODO: Modify classifier for CIFAR-10 (10 classes instead of 1000)\n",
    "    # HINT: model.classifier[1] should be a Linear layer with model.last_channel input features\n",
    "    model.classifier[1] = # Your code here\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model instance\n",
    "model = create_mobilenetv2_cifar10(num_classes=10, pretrained=True).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"MobileNetV2 loaded with {total_params:,} total parameters\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model adapted for CIFAR-10 (32x32 images, 10 classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a71285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture and Summary\n",
    "def show_model_info(model, input_shape=(1, 3, 32, 32)):\n",
    "    \"\"\"\n",
    "    Display detailed information about the MobileNetV2 model\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"MOBILENETV2 MODEL ARCHITECTURE OVERVIEW\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Model summary information\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_size_mb = total_params * 4 / (1024 * 1024)  # 4 bytes per FP32 parameter\n",
    "    \n",
    "    print(f\"Model Statistics:\")\n",
    "    print(f\"   â€¢ Total Parameters: {total_params:,}\")\n",
    "    print(f\"   â€¢ Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"   â€¢ Model Size (FP32): {model_size_mb:.2f} MB\")\n",
    "    print(f\"   â€¢ Input Shape: {input_shape}\")\n",
    "    print(f\"   â€¢ Output Classes: 10 (CIFAR-10)\")\n",
    "    \n",
    "    # Show key architectural components\n",
    "    print(f\"\\nKey Architecture Components:\")\n",
    "    print(f\"   â€¢ Features: {len(model.features)} layers (conv blocks)\")\n",
    "    print(f\"   â€¢ Classifier: {len(model.classifier)} layers\")\n",
    "    print(f\"   â€¢ Activation: ReLU6 (mobile-friendly)\")\n",
    "    print(f\"   â€¢ Normalization: Batch Normalization\")\n",
    "    \n",
    "    # Create a visual representation of the model flow\n",
    "    print(f\"\\nModel Flow Overview:\")\n",
    "    print(f\"   Input (3Ã—32Ã—32) â†’ Features Extraction â†’ Global Pooling â†’ Classifier â†’ Output (10)\")\n",
    "    \n",
    "    # Test with a dummy input to show output shapes\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.randn(input_shape).to(device)\n",
    "        dummy_output = model(dummy_input)\n",
    "        print(f\"\\nModel Test:\")\n",
    "        print(f\"   â€¢ Input shape: {dummy_input.shape}\")\n",
    "        print(f\"   â€¢ Output shape: {dummy_output.shape}\")\n",
    "        print(f\"   â€¢ Output represents probabilities for {dummy_output.shape[1]} classes\")\n",
    "    \n",
    "    return total_params, model_size_mb\n",
    "\n",
    "# Analyze our MobileNetV2 model\n",
    "print(\"Examining MobileNetV2 model in detail:\")\n",
    "total_params, model_size_mb = show_model_info(model)\n",
    "\n",
    "# Show the actual model structure (first few layers)\n",
    "print(f\"\\nModel Structure Preview (First Few Layers):\")\n",
    "print(\"=\"*50)\n",
    "for i, (name, layer) in enumerate(model.named_children()):\n",
    "    print(f\"{i+1}. {name}: {layer.__class__.__name__}\")\n",
    "    if i >= 2:  # Show first 3 main components\n",
    "        break\n",
    "\n",
    "print(f\"\\nKey Insight:\")\n",
    "print(f\"   This model has {total_params:,} parameters taking {model_size_mb:.1f} MB.\")\n",
    "print(f\"   With quantization, we'll reduce this to ~{model_size_mb/2:.1f} MB!\")\n",
    "print(f\"   Perfect for deployment on mobile devices and edge computing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a0ca39",
   "metadata": {},
   "source": [
    "## Step 3: Train the Original Model (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735243a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the training function\n",
    "def train_model(model, trainloader, testloader, epochs=10, learning_rate=0.001):\n",
    "    # TODO: Define criterion (loss function) - use CrossEntropyLoss\n",
    "    criterion = # Your code here\n",
    "    \n",
    "    # TODO: Define optimizer - use Adam with the given learning rate\n",
    "    optimizer = # Your code here\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        pbar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # TODO: Zero gradients\n",
    "            optimizer = # Your code here\n",
    "            \n",
    "            # TODO: Forward pass\n",
    "            outputs = # Your code here\n",
    "            \n",
    "            # TODO: Calculate loss\n",
    "            loss = # Your code here\n",
    "            \n",
    "            # TODO: Backward pass\n",
    "            # Your code here\n",
    "            \n",
    "            # TODO: Update weights with .step()\n",
    "            optimizer = # Your code here\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "# Train the model\n",
    "print(\"Training original FP32 model...\")\n",
    "train_losses, train_accuracies = train_model(model, trainloader, testloader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b4ae5",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Original Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3a9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the evaluation function\n",
    "def evaluate_model(model, testloader, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # TODO: Determine if model is quantized (on CPU) or regular (on GPU)\n",
    "    # HINT: Use next(model.parameters()).device to get model device\n",
    "    model_device = # Your code here\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # TODO: Use torch.no_grad() context manager for evaluation\n",
    "    with # Your code here:\n",
    "        for inputs, labels in tqdm(testloader, desc=f'Evaluating {model_name}'):\n",
    "            # TODO: Move inputs to the same device as the model\n",
    "            inputs = # Your code here\n",
    "            labels = labels.to(device)  # Keep labels on original device for comparison\n",
    "            \n",
    "            # TODO: Forward pass\n",
    "            outputs = # Your code here\n",
    "            \n",
    "            # TODO: Get predictions (use outputs.max(1) to get the class with highest probability)\n",
    "            _, predicted = # Your code here\n",
    "            \n",
    "            # TODO: Move predictions back to same device as labels for comparison\n",
    "            predicted = # Your code here\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'{model_name} - Accuracy: {accuracy:.2f}%, Inference Time: {inference_time:.2f}s')\n",
    "    \n",
    "    return accuracy, inference_time\n",
    "\n",
    "def get_model_size(model, model_name=\"Model\"):\n",
    "    # Save model temporarily to measure size\n",
    "    temp_path = f'temp_{model_name.lower().replace(\" \", \"_\")}.pth'\n",
    "    torch.save(model.state_dict(), temp_path)\n",
    "    size_mb = os.path.getsize(temp_path) / (1024 * 1024)\n",
    "    os.remove(temp_path)\n",
    "    \n",
    "    print(f'{model_name} - Size: {size_mb:.2f} MB')\n",
    "    return size_mb\n",
    "\n",
    "# Evaluate original model\n",
    "original_accuracy, original_time = evaluate_model(model, testloader, \"Original FP32\")\n",
    "original_size = get_model_size(model, \"Original FP32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d784cc28",
   "metadata": {},
   "source": [
    "## Step 5: Apply FP16 Quantization (Half Precision)\n",
    "\n",
    "FP16 quantization converts models from 32-bit to 16-bit floating point representation. This approach offers:\n",
    "- **Simple and reliable** - works on all PyTorch installations\n",
    "- **Clear benefits** - exactly 50% model size reduction\n",
    "- **Minimal accuracy loss** - maintains nearly full precision\n",
    "- **Good starting point** - before considering more aggressive quantization techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the FP16 quantization function\n",
    "def apply_fp16_quantization(model):\n",
    "    \"\"\"\n",
    "    Convert model to FP16 (half precision)\n",
    "    This reduces model size by exactly 50% with minimal accuracy loss\n",
    "    \"\"\"\n",
    "    print(\"Converting model to FP16 (half precision)...\")\n",
    "    \n",
    "    # TODO: Create a deep copy of the model to avoid modifying the original with deepcopy()\n",
    "    model_fp16 = # Your code here\n",
    "    \n",
    "    # TODO: Move to CPU first for conversion with .to('cpu') to move the model to CPU\n",
    "    model_fp16 = # Your code here\n",
    "    \n",
    "    # TODO: Convert to half precision (FP16) with .half() method\n",
    "    model_fp16 = # Your code here\n",
    "    \n",
    "    model_fp16.eval()\n",
    "    \n",
    "    # TODO: Move back to device for evaluation\n",
    "    model_fp16 = # Your code here\n",
    "    \n",
    "    print(\"FP16 quantization successful!\")\n",
    "    print(\"   All weights and activations now use 16-bit instead of 32-bit\")\n",
    "    \n",
    "    return model_fp16\n",
    "\n",
    "# Apply FP16 quantization\n",
    "model_fp16 = apply_fp16_quantization(model)\n",
    "\n",
    "print(f\"\\nModel Comparison:\")\n",
    "print(f\"Original model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"FP16 model parameters: {sum(p.numel() for p in model_fp16.parameters()):,}\")\n",
    "print(f\"Parameter count unchanged (same architecture)\")\n",
    "print(f\"Memory usage per parameter: FP32 = 4 bytes, FP16 = 2 bytes\")\n",
    "\n",
    "# Show the actual data types\n",
    "print(f\"\\nData type verification:\")\n",
    "print(f\"Original model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"FP16 model dtype: {next(model_fp16.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24cc954",
   "metadata": {},
   "source": [
    "## Step 5B: Apply INT8 Quantization (Dynamic Quantization)\n",
    "\n",
    "INT8 quantization maps model weights and/or activations to 8-bit integer representations, providing even greater compression and potential speedup compared to FP16. In PyTorch, dynamic quantization is a practical approach for post-training quantization, especially for models with mostly linear layers (e.g., fully connected, LSTM, Transformer).\n",
    "\n",
    "**Key characteristics:**\n",
    "- Converts weights to INT8, activations quantized dynamically at runtime\n",
    "- **Only quantizes specific layer types** (e.g., nn.Linear, nn.LSTM, nn.GRU)\n",
    "- **Convolutional layers are NOT quantized** by dynamic quantization\n",
    "- Size reduction depends on the proportion of quantizable layers in your model\n",
    "- May introduce more accuracy loss than FP16, but often acceptable for many applications\n",
    "\n",
    "**Important for MobileNetV2:** Since MobileNetV2 consists mostly of convolutional layers with only a small final classifier (nn.Linear), dynamic INT8 quantization will show minimal size reduction. This is expected behavior - the size reduction depends on how many Linear layers your model contains.\n",
    "\n",
    "Below, we apply dynamic INT8 quantization to the MobileNetV2 classifier and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the INT8 dynamic quantization function\n",
    "def apply_int8_dynamic_quantization(model):\n",
    "    \"\"\"\n",
    "    Apply dynamic INT8 quantization to supported layers (e.g., nn.Linear) in the model.\n",
    "    Returns a quantized model ready for evaluation.\n",
    "    \"\"\"\n",
    "    print(\"Applying dynamic INT8 quantization...\")\n",
    "    \n",
    "    # TODO: Create a deep copy of the model and move to CPU\n",
    "    # HINT: Use copy.deepcopy(model).cpu().eval()\n",
    "    model_int8 = # Your code here\n",
    "    \n",
    "    # TODO: Apply dynamic quantization to Linear layers\n",
    "    # HINT: Use torch.quantization.quantize_dynamic()\n",
    "    # Parameters: model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "    model_int8_quantized = # Your code here\n",
    "    \n",
    "    print(\"INT8 dynamic quantization successful!\")\n",
    "    return model_int8_quantized\n",
    "\n",
    "# Apply INT8 quantization\n",
    "dynamic_int8_model = apply_int8_dynamic_quantization(model)\n",
    "\n",
    "# TODO: Evaluate INT8 quantized model\n",
    "# HINT: Use the evaluate_model function you completed earlier\n",
    "int8_accuracy, int8_time = # Your code here\n",
    "int8_size = get_model_size(dynamic_int8_model, \"INT8 Quantized\")\n",
    "\n",
    "print(f\"\\nINT8 Quantized Model:\\n  â€¢ Size: {int8_size:.2f} MB\\n  â€¢ Accuracy: {int8_accuracy:.2f}%\\n  â€¢ Inference Time: {int8_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06047aec",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Quantized Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bff3ad0",
   "metadata": {},
   "source": [
    "### Why Batch Size Matters for FP16 Inference Speed\n",
    "\n",
    "Larger batch sizes can significantly improve the inference speed of FP16 quantized models, especially on CUDA-enabled GPUs with Tensor Cores. This is because:\n",
    "- Tensor Cores are most efficient when processing large matrix operations, which occur with larger batches.\n",
    "- Small batches may not fully utilize the GPU's parallelism or Tensor Core hardware, so the speedup from FP16 is less noticeable.\n",
    "- With larger batches, the overhead of data transfer and kernel launch is amortized, and the GPU can process more data in parallel, making FP16 inference faster compared to FP32.\n",
    "\n",
    "**Key takeaway:** If your hardware supports it, increasing the batch size can help you realize the full speed benefits of FP16 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37beab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the FP16 evaluation function\n",
    "def evaluate_fp16_model(model, testloader, model_name=\"FP16 Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate FP16 model performance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(testloader, desc=f'Evaluating {model_name}'):\n",
    "            # TODO: Convert inputs to FP16 to match model precision\n",
    "            # HINT: use .half() method on inputs\n",
    "            inputs = # Your code here\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # TODO: Forward pass\n",
    "            outputs = # Your code here\n",
    "            \n",
    "            # TODO: Get predictions\n",
    "            _, predicted = # Your code here\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'{model_name} - Accuracy: {accuracy:.2f}%, Inference Time: {inference_time:.2f}s')\n",
    "    \n",
    "    return accuracy, inference_time\n",
    "\n",
    "def get_model_size_precise(model, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Calculate precise model size based on parameter types\n",
    "    \"\"\"\n",
    "    total_size = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        # Calculate size based on actual data type\n",
    "        param_size = param.numel() * param.element_size()\n",
    "        total_size += param_size\n",
    "    \n",
    "    # Convert to MB\n",
    "    size_mb = total_size / (1024 * 1024)\n",
    "    \n",
    "    print(f'{model_name} - Size: {size_mb:.2f} MB')\n",
    "    return size_mb\n",
    "\n",
    "# Evaluate FP16 model\n",
    "print(\"Evaluating FP16 Quantized Model...\")\n",
    "fp16_accuracy, fp16_time = evaluate_fp16_model(model_fp16, testloader, \"FP16 Quantized\")\n",
    "fp16_size = get_model_size_precise(model_fp16, \"FP16 Quantized\")\n",
    "\n",
    "# Compare with original and INT8\n",
    "print(f\"\\nQuantization Results:\")\n",
    "print(f\"Original FP32 Model:\")\n",
    "print(f\"  â€¢ Size: {original_size:.2f} MB\")\n",
    "print(f\"  â€¢ Accuracy: {original_accuracy:.2f}%\")\n",
    "print(f\"  â€¢ Inference Time: {original_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nFP16 Quantized Model:\")\n",
    "print(f\"  â€¢ Size: {fp16_size:.2f} MB\")\n",
    "print(f\"  â€¢ Accuracy: {fp16_accuracy:.2f}%\")\n",
    "print(f\"  â€¢ Inference Time: {fp16_time:.2f}s\")\n",
    "\n",
    "print(f\"\\nINT8 Quantized Model:\")\n",
    "print(f\"  â€¢ Size: {int8_size:.2f} MB\")\n",
    "print(f\"  â€¢ Accuracy: {int8_accuracy:.2f}%\")\n",
    "print(f\"  â€¢ Inference Time: {int8_time:.2f}s\")\n",
    "\n",
    "# TODO: Calculate improvements for both FP16 and INT8\n",
    "fp16_size_reduction = # Your code here - calculate FP16 size reduction percentage\n",
    "fp16_accuracy_change = # Your code here - calculate FP16 accuracy difference\n",
    "fp16_speed_change = # Your code here - calculate FP16 speed change percentage\n",
    "\n",
    "int8_size_reduction = # Your code here - calculate INT8 size reduction percentage\n",
    "int8_accuracy_change = # Your code here - calculate INT8 accuracy difference\n",
    "int8_speed_change = # Your code here - calculate INT8 speed change percentage\n",
    "\n",
    "print(f\"\\nImprovements:\")\n",
    "print(f\"FP16 Quantization:\")\n",
    "print(f\"  Size Reduction: {fp16_size_reduction:.1f}%\")\n",
    "print(f\"  Accuracy Change: {fp16_accuracy_change:+.2f}%\")\n",
    "print(f\"  Speed Change: {fp16_speed_change:+.1f}%\")\n",
    "\n",
    "print(f\"INT8 Quantization:\")\n",
    "print(f\"  Size Reduction: {int8_size_reduction:.1f}%\")\n",
    "print(f\"  Accuracy Change: {int8_accuracy_change:+.2f}%\")\n",
    "print(f\"  Speed Change: {int8_speed_change:+.1f}%\")\n",
    "\n",
    "print(f\"\\nSuccess! FP16 achieved {fp16_size_reduction:.0f}% size reduction, INT8 achieved {int8_size_reduction:.0f}% size reduction!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
