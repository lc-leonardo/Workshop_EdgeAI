{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 3.1: Knowledge Distillation for Efficient Inference Hands-On Practice\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand the concept of knowledge distillation and teacher-student paradigm\n",
    "- Learn how to transfer knowledge from a large model to a smaller one\n",
    "- Implement temperature-based distillation with PyTorch\n",
    "- Compare student model performance with and without distillation\n",
    "- Visualize the knowledge transfer process\n",
    "\n",
    "## What is Knowledge Distillation?\n",
    "Knowledge distillation is a model compression technique in which a compact 'student' network is trained to approximate the function learned by a larger, more complex 'teacher' model. Rather than relying solely on hard target labels, the student is supervised using the soft probability distributions (soft targets) produced by the teacher. These soft targets encode richer information about class similarities and inter-class relationships, thereby facilitating more effective knowledge transfer and improved generalization in the student model.\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1503.02531v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0918c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# CIFAR-10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Teacher and Student Models using MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_teacher_mobilenetv2(num_classes=10, width_mult=1.4):\n",
    "    \"\"\"\n",
    "    Create a larger MobileNetV2 teacher model with increased width\n",
    "    \"\"\"\n",
    "    # Create base MobileNetV2 with wider channels\n",
    "    model = models.mobilenet_v2(pretrained=True)\n",
    "    \n",
    "    # Modify the first convolution layer for CIFAR-10\n",
    "    model.features[0][0] = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    \n",
    "    # Add an additional intermediate layer in the classifier for more capacity\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(model.last_channel, 512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "class StudentMobileNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(StudentMobileNetV2, self).__init__()\n",
    "        base_model = models.mobilenet_v2(pretrained=True)\n",
    "        # Truncate features to make it smaller\n",
    "        self.features = nn.Sequential(*list(base_model.features.children())[:14])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        # Get output channels after truncation\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 32, 32)\n",
    "            features_out = self.features(dummy_input)\n",
    "            out_channels = features_out.shape[1]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(out_channels, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def create_student_mobilenetv2(num_classes=10):\n",
    "    return StudentMobileNetV2(num_classes=num_classes)\n",
    "\n",
    "# Create model instances\n",
    "teacher_model = create_teacher_mobilenetv2(num_classes=10).to(device)\n",
    "student_model = create_student_mobilenetv2(num_classes=10).to(device)\n",
    "\n",
    "# Count parameters\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "\n",
    "print(f\"Teacher MobileNetV2 (Enhanced): {teacher_params:,} parameters\")\n",
    "print(f\"Student MobileNetV2 (Compact): {student_params:,} parameters\")\n",
    "print(f\"Size reduction: {(teacher_params - student_params) / teacher_params * 100:.1f}%\")\n",
    "print(f\"Teacher is {teacher_params / student_params:.1f}x larger than student\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher_model(model, trainloader, testloader, epochs=10, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    print(\"Training teacher model...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(trainloader, desc=f'Teacher Epoch {epoch+1}/{epochs}', leave=False)\n",
    "        for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f'Teacher Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "# Train the teacher model (reduced epochs for hands-on efficiency)\n",
    "teacher_losses, teacher_accuracies = train_teacher_model(teacher_model, trainloader, testloader, epochs=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testloader, model_name=\"Model\"):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(testloader, desc=f'Evaluating {model_name}', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'{model_name} - Accuracy: {accuracy:.2f}%, Inference Time: {inference_time:.2f}s')\n",
    "    \n",
    "    return accuracy, inference_time\n",
    "\n",
    "# Evaluate teacher model\n",
    "teacher_accuracy, teacher_time = evaluate_model(teacher_model, testloader, \"Teacher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: TODO - Implement Knowledge Distillation Loss Function\n",
    "\n",
    "**Your Task**: Complete the `DistillationLoss` class by implementing the core knowledge distillation concepts.\n",
    "\n",
    "### Key Concepts:\n",
    "1. **Hard Loss**: Traditional loss between student predictions and true labels\n",
    "2. **Soft Loss**: KL divergence between student and teacher predictions (with temperature scaling)\n",
    "3. **Temperature Scaling**: Makes probability distributions softer for better knowledge transfer\n",
    "4. **Combined Loss**: Weighted combination of hard and soft losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss function for knowledge distillation\n",
    "    Combines hard target loss (student vs true labels) and soft target loss (student vs teacher)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.5, temperature=4.0):\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight for hard target loss\n",
    "        self.temperature = temperature  # Temperature for softening predictions\n",
    "        self.hard_loss = nn.CrossEntropyLoss()\n",
    "        self.soft_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "        \n",
    "    def forward(self, student_outputs, teacher_outputs, labels):\n",
    "        # TODO 1: Calculate hard target loss (student predictions vs true labels)\n",
    "        # Hint: Use self.hard_loss with student_outputs and labels\n",
    "        hard_loss = # YOUR CODE HERE\n",
    "        \n",
    "        # TODO 2: Apply temperature scaling to student outputs\n",
    "        # Hint: Use F.log_softmax with student_outputs divided by self.temperature\n",
    "        student_soft = # YOUR CODE HERE\n",
    "        \n",
    "        # TODO 3: Apply temperature scaling to teacher outputs\n",
    "        # Hint: Use F.softmax with teacher_outputs divided by self.temperature\n",
    "        teacher_soft = # YOUR CODE HERE\n",
    "        \n",
    "        # TODO 4: Calculate soft target loss and scale by temperature squared\n",
    "        # Hint: Use self.soft_loss and multiply by (self.temperature ** 2)\n",
    "        soft_loss = # YOUR CODE HERE\n",
    "        \n",
    "        # TODO 5: Combine hard and soft losses using alpha weighting\n",
    "        # Hint: alpha * hard_loss + (1 - alpha) * soft_loss\n",
    "        total_loss = # YOUR CODE HERE\n",
    "        \n",
    "        return total_loss, hard_loss, soft_loss\n",
    "\n",
    "# Test your implementation\n",
    "test_criterion = DistillationLoss(alpha=0.7, temperature=4.0)\n",
    "test_student_out = torch.randn(4, 10)  # Batch size 4, 10 classes\n",
    "test_teacher_out = torch.randn(4, 10)\n",
    "test_labels = torch.randint(0, 10, (4,))\n",
    "\n",
    "total, hard, soft = test_criterion(test_student_out, test_teacher_out, test_labels)\n",
    "print(f\"Distillation loss implementation test:\")\n",
    "print(f\"Total Loss: {total.item():.4f}\")\n",
    "print(f\"Hard Loss: {hard.item():.4f}\")\n",
    "print(f\"Soft Loss: {soft.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: TODO - Complete the Knowledge Distillation Training Function\n",
    "\n",
    "**Your Task**: Complete the distillation training loop by implementing the key steps.\n",
    "\n",
    "### Key Steps:\n",
    "1. Get teacher predictions (without gradients)\n",
    "2. Get student predictions \n",
    "3. Calculate distillation loss\n",
    "4. Backpropagate and update student parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_with_distillation(student_model, teacher_model, trainloader, epochs=6, \n",
    "                                  learning_rate=0.001, alpha=0.7, temperature=4.0):\n",
    "    \"\"\"\n",
    "    Train student model using knowledge distillation\n",
    "    \"\"\"\n",
    "    distillation_criterion = DistillationLoss(alpha=alpha, temperature=temperature)\n",
    "    optimizer = optim.Adam(student_model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
    "    \n",
    "    # Set teacher to evaluation mode (frozen)\n",
    "    teacher_model.eval()\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    hard_losses = []\n",
    "    soft_losses = []\n",
    "    \n",
    "    print(f\"Training student with distillation (α={alpha}, T={temperature})...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        student_model.train()\n",
    "        running_loss = 0.0\n",
    "        running_hard_loss = 0.0\n",
    "        running_soft_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(trainloader, desc=f'Distillation Epoch {epoch+1}/{epochs}', leave=False)\n",
    "        for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # TODO 6: Get teacher predictions without gradients\n",
    "            # Hint: Use torch.no_grad() context and call teacher_model(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = # YOUR CODE HERE\n",
    "            \n",
    "            # TODO 7: Get student predictions and reset gradients\n",
    "            # Hint: Call optimizer.zero_grad() and student_model(inputs)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = # YOUR CODE HERE\n",
    "            \n",
    "            # TODO 8: Calculate distillation loss\n",
    "            # Hint: Use distillation_criterion with student_outputs, teacher_outputs, labels\n",
    "            total_loss, hard_loss, soft_loss = # YOUR CODE HERE\n",
    "            \n",
    "            # TODO 9: Backpropagate and update parameters\n",
    "            # Hint: Call total_loss.backward() and optimizer.step()\n",
    "            # YOUR CODE HERE\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += total_loss.item()\n",
    "            running_hard_loss += hard_loss.item()\n",
    "            running_soft_loss += soft_loss.item()\n",
    "            \n",
    "            _, predicted = student_outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "                'Hard': f'{running_hard_loss/(batch_idx+1):.3f}',\n",
    "                'Soft': f'{running_soft_loss/(batch_idx+1):.3f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_hard_loss = running_hard_loss / len(trainloader)\n",
    "        epoch_soft_loss = running_soft_loss / len(trainloader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        hard_losses.append(epoch_hard_loss)\n",
    "        soft_losses.append(epoch_soft_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f'Distillation Epoch {epoch+1}: Total Loss: {epoch_loss:.4f}, '\n",
    "              f'Hard Loss: {epoch_hard_loss:.4f}, Soft Loss: {epoch_soft_loss:.4f}, '\n",
    "              f'Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies, hard_losses, soft_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train Student with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of student model for distillation\n",
    "student_distilled = copy.deepcopy(student_model)\n",
    "\n",
    "# Train student with distillation\n",
    "distill_losses, distill_accuracies, hard_losses, soft_losses = train_student_with_distillation(\n",
    "    student_distilled, teacher_model, trainloader, epochs=6, alpha=0.7, temperature=4.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train Student Model Without Distillation (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student_baseline(model, trainloader, epochs=6, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train student model without distillation (baseline)\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    print(\"Training student baseline (without distillation)...\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(trainloader, desc=f'Baseline Epoch {epoch+1}/{epochs}', leave=False)\n",
    "        for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f'Baseline Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "# Create another copy for baseline training\n",
    "student_baseline = create_student_mobilenetv2(num_classes=10).to(device)\n",
    "\n",
    "# Train baseline student\n",
    "baseline_losses, baseline_accuracies = train_student_baseline(student_baseline, trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate and Compare All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "print(\"\\nEvaluating all models...\")\n",
    "\n",
    "# Teacher model (already evaluated)\n",
    "print(f\"Teacher: {teacher_accuracy:.2f}% accuracy\")\n",
    "\n",
    "# Student baseline\n",
    "baseline_accuracy, baseline_time = evaluate_model(student_baseline, testloader, \"Student Baseline\")\n",
    "\n",
    "# Student with distillation\n",
    "distilled_accuracy, distilled_time = evaluate_model(student_distilled, testloader, \"Student Distilled\")\n",
    "\n",
    "# Calculate improvements\n",
    "distillation_improvement = distilled_accuracy - baseline_accuracy\n",
    "speed_improvement = teacher_time / distilled_time\n",
    "size_reduction = (teacher_params - student_params) / teacher_params * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"                    KNOWLEDGE DISTILLATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Teacher Model:          {teacher_accuracy:.2f}% accuracy, {teacher_params:,} params\")\n",
    "print(f\"Student Baseline:       {baseline_accuracy:.2f}% accuracy, {student_params:,} params\")\n",
    "print(f\"Student Distilled:      {distilled_accuracy:.2f}% accuracy, {student_params:,} params\")\n",
    "print(\"\\nImprovements:\")\n",
    "print(f\"Distillation boost:     +{distillation_improvement:.2f}% accuracy\")\n",
    "print(f\"Speed improvement:      {speed_improvement:.1f}x faster than teacher\")\n",
    "print(f\"Model size reduction:   {size_reduction:.1f}% fewer parameters\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Visualize Knowledge Transfer Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves comparison and loss breakdown\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training curves comparison\n",
    "epochs_range = range(1, len(baseline_accuracies) + 1)\n",
    "ax1.plot(epochs_range, baseline_accuracies, 'b-', label='Student Baseline', linewidth=2, marker='o')\n",
    "ax1.plot(epochs_range, distill_accuracies, 'r-', label='Student Distilled', linewidth=2, marker='s')\n",
    "ax1.axhline(y=teacher_accuracy, color='g', linestyle='--', label='Teacher', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Training Accuracy (%)')\n",
    "ax1.set_title('Training Accuracy Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss breakdown for distillation\n",
    "ax2.plot(epochs_range, hard_losses, 'b-', label='Hard Loss (vs labels)', linewidth=2, marker='o')\n",
    "ax2.plot(epochs_range, soft_losses, 'r-', label='Soft Loss (vs teacher)', linewidth=2, marker='s')\n",
    "ax2.plot(epochs_range, distill_losses, 'g-', label='Total Loss', linewidth=2, marker='^')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Distillation Loss Components')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Final Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Comparison\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Final accuracy comparison\n",
    "model_names = ['Teacher', 'Student\\nBaseline', 'Student\\nDistilled']\n",
    "accuracies = [teacher_accuracy, baseline_accuracy, distilled_accuracy]\n",
    "colors = ['green', 'blue', 'red']\n",
    "bars = ax1.bar(model_names, accuracies, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Test Accuracy (%)')\n",
    "ax1.set_title('Final Model Comparison')\n",
    "ax1.set_ylim([min(accuracies) - 2, max(accuracies) + 2])\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             f'{acc:.2f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Model complexity comparison\n",
    "params = [teacher_params/1000, student_params/1000, student_params/1000]  # In thousands\n",
    "bars = ax2.bar(model_names, params, color=colors, alpha=0.7)\n",
    "ax2.set_ylabel('Parameters (thousands)')\n",
    "ax2.set_title('Model Complexity')\n",
    "for bar, param in zip(bars, params):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "             f'{param:.0f}K', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Inference time comparison\n",
    "times = [teacher_time, baseline_time, distilled_time]\n",
    "bars = ax3.bar(model_names, times, color=colors, alpha=0.7)\n",
    "ax3.set_ylabel('Inference Time (seconds)')\n",
    "ax3.set_title('Inference Speed')\n",
    "for bar, time_val in zip(bars, times):\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: TODO - Experiment with Different Distillation Parameters\n",
    "\n",
    "**Your Task**: Complete the parameter experiment function and find the best parameters.\n",
    "\n",
    "### Key Parameters to Tune:\n",
    "- **Temperature**: Controls how soft the probability distributions become\n",
    "- **Alpha**: Balances hard loss vs soft loss (0=only soft, 1=only hard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_distillation_experiment(teacher_model, temperature=4.0, alpha=0.5, epochs=2):\n",
    "    \"\"\"\n",
    "    Quick experiment to test different distillation parameters\n",
    "    \"\"\"\n",
    "    # Create a new student model\n",
    "    student = create_student_mobilenetv2(num_classes=10).to(device)\n",
    "    \n",
    "    # TODO 10: Create distillation criterion with given parameters\n",
    "    # Hint: Use DistillationLoss with alpha and temperature\n",
    "    distillation_criterion = # YOUR CODE HERE\n",
    "    \n",
    "    optimizer = optim.Adam(student.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    teacher_model.eval()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        \n",
    "        for inputs, labels in trainloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # TODO 11: Get teacher outputs (similar to previous TODOs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = # YOUR CODE HERE\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "            \n",
    "            # TODO 12: Calculate loss and backpropagate\n",
    "            total_loss, _, _ = # YOUR CODE HERE\n",
    "            # YOUR CODE HERE  # backward\n",
    "            # YOUR CODE HERE  # optimizer step\n",
    "    \n",
    "    # Quick evaluation\n",
    "    student.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = student(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: TODO - Run Parameter Experiments\n",
    "\n",
    "**Your Task**: Complete the parameter experiment loops and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different parameters\n",
    "print(\"Experimenting with different distillation parameters...\")\n",
    "\n",
    "temperatures = [1.0, 3.0, 5.0, 8.0]\n",
    "alphas = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "# TODO 13: Temperature experiment\n",
    "temp_results = []\n",
    "for temp in temperatures:\n",
    "    print(f\"Testing temperature {temp}...\")\n",
    "    # TODO: Call quick_distillation_experiment with temperature=temp, alpha=0.7, epochs=2\n",
    "    acc = # YOUR CODE HERE\n",
    "    temp_results.append(acc)\n",
    "    print(f\"Temperature {temp}: {acc:.2f}% accuracy\")\n",
    "\n",
    "# TODO 14: Alpha experiment\n",
    "alpha_results = []\n",
    "for alpha in alphas:\n",
    "    print(f\"Testing alpha {alpha}...\")\n",
    "    # TODO: Call quick_distillation_experiment with temperature=4.0, alpha=alpha, epochs=2\n",
    "    acc = # YOUR CODE HERE\n",
    "    alpha_results.append(acc)\n",
    "    print(f\"Alpha {alpha}: {acc:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Visualize Parameter Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize parameter sensitivity\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Temperature sensitivity\n",
    "ax1.plot(temperatures, temp_results, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Temperature')\n",
    "ax1.set_ylabel('Test Accuracy (%)')\n",
    "ax1.set_title('Temperature Sensitivity')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (temp, acc) in enumerate(zip(temperatures, temp_results)):\n",
    "    ax1.annotate(f'{acc:.1f}%', (temp, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "# Alpha sensitivity\n",
    "ax2.plot(alphas, alpha_results, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Alpha (Hard Loss Weight)')\n",
    "ax2.set_ylabel('Test Accuracy (%)')\n",
    "ax2.set_title('Alpha Sensitivity')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for i, (alpha, acc) in enumerate(zip(alphas, alpha_results)):\n",
    "    ax2.annotate(f'{acc:.1f}%', (alpha, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPARAMETER OPTIMIZATION RESULTS:\")\n",
    "print(f\"Best temperature: {temperatures[np.argmax(temp_results)]} (accuracy: {max(temp_results):.2f}%)\")\n",
    "print(f\"Best alpha: {alphas[np.argmax(alpha_results)]} (accuracy: {max(alpha_results):.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
