{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "185ad3e5",
   "metadata": {},
   "source": [
    "# Workshop 3.1: Fine-Grained Pruning for Efficient Inference - Hands-On Practice\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand what fine-grained (unstructured) pruning is and its importance for edge AI\n",
    "- Analyze weight distributions before pruning using histograms\n",
    "- Learn magnitude-based pruning strategies\n",
    "- Implement unstructured magnitude pruning with PyTorch\n",
    "- Compare model sparsity, speed, and accuracy trade-offs\n",
    "- Visualize pruning effects on network parameters\n",
    "\n",
    "## What is Fine-Grained Pruning?\n",
    "Fine-grained pruning (also called unstructured pruning) removes individual weights from neural networks based on their magnitude. Unlike structured pruning that removes entire channels or filters, fine-grained pruning can remove any weight regardless of its position, creating sparse networks with irregular patterns.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Individual weight removal** - Any weight can be pruned\n",
    "- **Magnitude-based selection** - Smallest weights are removed first\n",
    "- **Higher sparsity potential** - Can achieve very high compression ratios\n",
    "- **Irregular sparsity patterns** - Requires special hardware/software for speedup\n",
    "\n",
    "Paper: https://arxiv.org/pdf/1506.02626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ee2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d950eb",
   "metadata": {},
   "source": [
    "## Step 1: Load and Prepare CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc65bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# CIFAR-10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "print(f\"Training samples: {len(trainset)}\")\n",
    "print(f\"Test samples: {len(testset)}\")\n",
    "print(f\"Classes: {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd33904c",
   "metadata": {},
   "source": [
    "## Step 2: Load and Modify MobileNetV2 for Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f05934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and modify MobileNetV2 for CIFAR-10 pruning\n",
    "def create_mobilenetv2_cifar10(num_classes=10, pretrained=True):\n",
    "    \"\"\"\n",
    "    Create MobileNetV2 adapted for CIFAR-10\n",
    "    CIFAR-10 images are 32x32, smaller than ImageNet's 224x224\n",
    "    \"\"\"\n",
    "    # Load pre-trained MobileNetV2\n",
    "    model = models.mobilenet_v2(pretrained=pretrained)\n",
    "    \n",
    "    # Modify the first convolution layer for smaller input size\n",
    "    # Original: stride=2, now stride=1 to preserve spatial dimensions\n",
    "    model.features[0][0] = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    \n",
    "    # Modify classifier for CIFAR-10 (10 classes instead of 1000)\n",
    "    model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model instance\n",
    "model = create_mobilenetv2_cifar10(num_classes=10, pretrained=True).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"MobileNetV2 loaded with {total_params:,} total parameters\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model adapted for CIFAR-10 and ready for pruning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b7a988",
   "metadata": {},
   "source": [
    "## Step 3: Train the Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, trainloader, testloader, epochs=8, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        # Training loop with progress bar\n",
    "        pbar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.3f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "# Train the model\n",
    "print(\"Training original model...\")\n",
    "train_losses, train_accuracies = train_model(model, trainloader, testloader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d098c",
   "metadata": {},
   "source": [
    "## Step 4: Analyze Weight Distributions Before Pruning\n",
    "\n",
    "Before applying pruning, let's examine the weight distributions in our trained model. This helps us understand:\n",
    "- How weights are distributed across different layers\n",
    "- Which weights are candidates for removal (small magnitude)\n",
    "- The impact of pruning on different layer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13a809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for weight analysis\n",
    "def collect_layer_weights(model):\n",
    "    \"\"\"\n",
    "    Collect weights from different layer types for analysis\n",
    "    \"\"\"\n",
    "    conv_weights = []\n",
    "    linear_weights = []\n",
    "    layer_info = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weights = module.weight.data.cpu().flatten().numpy()\n",
    "            conv_weights.extend(weights)\n",
    "            layer_info.append({\n",
    "                'name': name,\n",
    "                'type': 'Conv2d',\n",
    "                'shape': list(module.weight.shape),\n",
    "                'total_params': module.weight.numel(),\n",
    "                'mean_abs': np.mean(np.abs(weights)),\n",
    "                'std': np.std(weights)\n",
    "            })\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            weights = module.weight.data.cpu().flatten().numpy()\n",
    "            linear_weights.extend(weights)\n",
    "            layer_info.append({\n",
    "                'name': name,\n",
    "                'type': 'Linear',\n",
    "                'shape': list(module.weight.shape),\n",
    "                'total_params': module.weight.numel(),\n",
    "                'mean_abs': np.mean(np.abs(weights)),\n",
    "                'std': np.std(weights)\n",
    "            })\n",
    "    \n",
    "    return conv_weights, linear_weights, layer_info\n",
    "\n",
    "def print_weight_statistics(layer_info, all_weights):\n",
    "    \"\"\"\n",
    "    Print detailed weight statistics in a formatted table\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WEIGHT DISTRIBUTION ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Layer':<25} {'Type':<8} {'Shape':<15} {'Params':<10} {'Mean |W|':<10} {'Std':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for info in layer_info:\n",
    "        shape_str = f\"{info['shape']}\"\n",
    "        if len(shape_str) > 14:\n",
    "            shape_str = shape_str[:11] + \"...\"\n",
    "        print(f\"{info['name']:<25} {info['type']:<8} {shape_str:<15} \"\n",
    "              f\"{info['total_params']:<10} {info['mean_abs']:<10.4f} {info['std']:<10.4f}\")\n",
    "    \n",
    "    abs_weights = np.abs(all_weights)\n",
    "    print(f\"\\nOverall Statistics:\")\n",
    "    print(f\"Total weights: {len(all_weights):,}\")\n",
    "    print(f\"Mean absolute weight: {np.mean(abs_weights):.4f}\")\n",
    "    print(f\"Median absolute weight: {np.median(abs_weights):.4f}\")\n",
    "    print(f\"Weights near zero (|w| < 0.001): {(abs_weights < 0.001).sum():,} ({(abs_weights < 0.001).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect weights from the trained model\n",
    "print(\"Collecting weights from different layer types...\")\n",
    "conv_weights, linear_weights, layer_info = collect_layer_weights(model)\n",
    "all_weights = conv_weights + linear_weights\n",
    "\n",
    "print(f\"Found {len(layer_info)} layers with weights:\")\n",
    "print(f\"  - Conv2d layers: {len([info for info in layer_info if info['type'] == 'Conv2d'])}\")\n",
    "print(f\"  - Linear layers: {len([info for info in layer_info if info['type'] == 'Linear'])}\")\n",
    "print(f\"  - Total weights: {len(all_weights):,}\")\n",
    "\n",
    "# Print detailed statistics\n",
    "print_weight_statistics(layer_info, all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41328933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive weight distribution visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Overall weight distribution\n",
    "ax1.hist(all_weights, bins=300, alpha=0.7, color='blue', density=True)\n",
    "ax1.axvline(x=0, color='red', linestyle='--', alpha=0.8, label='Zero')\n",
    "ax1.set_xlabel('Weight Value')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Overall Weight Distribution (All Layers)')\n",
    "ax1.set_xlim(-0.5, 0.5)  # Focus on the sharp distribution around zero\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Add statistics\n",
    "mean_weight = np.mean(all_weights)\n",
    "std_weight = np.std(all_weights)\n",
    "ax1.text(0.02, 0.98, f'Mean: {mean_weight:.4f}\\\\nStd: {std_weight:.4f}', \n",
    "         transform=ax1.transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 2. Weight magnitude distribution with pruning thresholds\n",
    "abs_weights = np.abs(all_weights)\n",
    "ax2.hist(abs_weights, bins=100, alpha=0.7, color='purple', density=True)\n",
    "ax2.set_xlabel('|Weight| (Absolute Value)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Weight Magnitude Distribution')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add pruning threshold lines for different ratios\n",
    "sorted_abs_weights = np.sort(abs_weights)\n",
    "for ratio in [0.3, 0.5, 0.7, 0.9]:\n",
    "    threshold = sorted_abs_weights[int(len(sorted_abs_weights) * ratio)]\n",
    "    ax2.axvline(x=threshold, linestyle=':', alpha=0.8, \n",
    "               label=f'{ratio:.0%} pruning threshold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a01e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDS-ON EXERCISE: Implement Core Pruning Functions\n",
    "# In this exercise, you'll implement the key functions for fine-grained pruning\n",
    "\n",
    "# Helper functions for model evaluation and parameter counting\n",
    "def evaluate_model(model, testloader, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy and measure inference time\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(testloader, desc=f'Evaluating {model_name}'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    print(f'{model_name} - Accuracy: {accuracy:.2f}%, Inference Time: {inference_time:.2f}s')\n",
    "    \n",
    "    return accuracy, inference_time\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count total trainable parameters\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_nonzero_parameters(model):\n",
    "    \"\"\"Count non-zero parameters (useful for pruned models)\"\"\"\n",
    "    nonzero_params = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            # Check if the module has been pruned (has weight_mask)\n",
    "            if hasattr(module, 'weight_mask'):\n",
    "                # Count non-zero elements in the effective weight (weight * mask)\n",
    "                effective_weight = module.weight * module.weight_mask\n",
    "                nonzero_params += (effective_weight != 0).sum().item()\n",
    "            else:\n",
    "                # If not pruned, count all non-zero parameters\n",
    "                nonzero_params += (module.weight != 0).sum().item()\n",
    "            \n",
    "            # Also count bias if it exists\n",
    "            if hasattr(module, 'bias') and module.bias is not None:\n",
    "                nonzero_params += (module.bias != 0).sum().item()\n",
    "    \n",
    "    return nonzero_params\n",
    "\n",
    "# TODO 1: Implement the calculate_sparsity function\n",
    "def calculate_sparsity(model):\n",
    "    \"\"\"\n",
    "    Calculate sparsity percentage of a model\n",
    "    \n",
    "    HINT: \n",
    "    - Iterate through Conv2d and Linear layers\n",
    "    - Check if module has 'weight_mask' attribute (indicates pruning)\n",
    "    - If pruned: use effective_weight = module.weight * module.weight_mask\n",
    "    - If not pruned: use module.weight directly\n",
    "    - Count total parameters and zero parameters\n",
    "    - Return: 100.0 * zero_params / total_params\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    # TODO: Iterate through all named modules\n",
    "    for name, module in model.named_modules():\n",
    "        # TODO: Check if module is Conv2d or Linear layer\n",
    "        if isinstance(module, (_____, _____)):  # Fill in the layer types\n",
    "            # TODO: Check if the module has been pruned (has weight_mask)\n",
    "            if hasattr(module, '_____'):  # Fill in the attribute name\n",
    "                # TODO: Calculate effective weight (weight * mask)\n",
    "                effective_weight = module._____ * module._____  # Fill in the calculation\n",
    "                total_params += effective_weight.numel()\n",
    "                zero_params += (effective_weight == 0).sum().item()\n",
    "            else:\n",
    "                # TODO: If not pruned, count normally\n",
    "                total_params += module._____.numel()  # Fill in the weight attribute\n",
    "                zero_params += (module._____ == 0).sum().item()  # Fill in the weight attribute\n",
    "    \n",
    "    # TODO: Calculate and return sparsity percentage\n",
    "    sparsity = _____ * _____ / _____ if total_params > 0 else 0.0  # Fill in the formula\n",
    "    return sparsity\n",
    "\n",
    "# TODO 2: Implement the apply_fine_grained_pruning function  \n",
    "def apply_fine_grained_pruning(model, pruning_ratio):\n",
    "    \"\"\"\n",
    "    Apply fine-grained (unstructured) pruning to all Conv2d and Linear layers\n",
    "    \n",
    "    🔍 HINT:\n",
    "    - Use PyTorch's prune.l1_unstructured() function\n",
    "    - Apply to 'weight' parameter of each layer\n",
    "    - Use the pruning_ratio as the 'amount' parameter\n",
    "    - Iterate through all Conv2d and Linear layers\n",
    "    \"\"\"\n",
    "    print(f\"Applying {pruning_ratio:.0%} fine-grained pruning...\")\n",
    "    \n",
    "    # TODO: Apply pruning to all Conv2d and Linear layers\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (_____, _____)):  # Fill in the layer types\n",
    "            # TODO: Apply L1 unstructured pruning to the weight parameter\n",
    "            prune._____(module, name='_____', amount=_____)  # Fill in the function and parameters\n",
    "            print(f\"  Pruned layer: {name}\")\n",
    "    \n",
    "    print(f\"Fine-grained pruning complete - {pruning_ratio:.0%} of weights removed\")\n",
    "    return model\n",
    "\n",
    "def fine_tune_pruned_model(pruned_model, trainloader, testloader, epochs=3, learning_rate=0.0001):\n",
    "    \"\"\"\n",
    "    Fine-tune a pruned model to recover accuracy\n",
    "    \"\"\"\n",
    "    print(f\"Fine-tuning pruned model for {epochs} epochs with learning rate {learning_rate}\")\n",
    "    \n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = optim.Adam(pruned_model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Track progress\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    pruned_model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        \n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = pruned_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(len(trainloader)):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_accuracy)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_accuracy:.2f}%')\n",
    "    \n",
    "    print(f\"Fine-tuning complete! Final training accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "    \n",
    "    return pruned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate original model performance (baseline)\n",
    "print(\"Evaluating original model performance...\")\n",
    "original_accuracy, original_time = evaluate_model(model, testloader, \"Original\")\n",
    "original_params = count_parameters(model)\n",
    "\n",
    "print(f\"\\nOriginal Model Summary:\")\n",
    "print(f\"  Total parameters: {original_params:,}\")\n",
    "print(f\"  Accuracy: {original_accuracy:.2f}%\")\n",
    "print(f\"  Inference time: {original_time:.2f}s\")\n",
    "print(f\"  This will be our baseline for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae1b41",
   "metadata": {},
   "source": [
    "## Step 5: Hands-On Challenge - Optimize Model for Edge Deployment\n",
    "\n",
    "### Challenge Objective\n",
    "Your task is to create a compressed model that maintains **at least 85% accuracy** after fine-tuning. This simulates real-world scenarios where you need to deploy AI models on resource-constrained devices.\n",
    "\n",
    "### Challenge Requirements\n",
    "1. **Pruning Strategy**: Apply fine-grained pruning with a single pruning ratio to the entire model\n",
    "2. **Target Accuracy**: Final model must achieve ≥85% accuracy after fine-tuning\n",
    "3. **Approach**: Prune → Fine-tune → Evaluate\n",
    "4. **Analysis**: Visualize the impact of pruning on model performance\n",
    "\n",
    "### Challenge Steps\n",
    "1. **Choose Pruning Ratio**: Select a global pruning ratio (e.g., 50%, 70%, 80%)\n",
    "2. **Apply Pruning**: Remove weights uniformly across all layers\n",
    "3. **Fine-tune Model**: Recover accuracy through targeted training\n",
    "4. **Evaluate Results**: Measure final accuracy, sparsity, and compression\n",
    "\n",
    "### Success Metrics\n",
    "- **Primary**: Accuracy ≥ 85% after fine-tuning\n",
    "- **Secondary**: Maximum sparsity/compression achieved\n",
    "- **Bonus**: Maintain inference speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge Setup: Simple pruning challenge with single ratio\n",
    "\n",
    "def evaluate_challenge_model(model, testloader, model_name, target_accuracy=85.0):\n",
    "    \"\"\"\n",
    "    Evaluate if a model meets the challenge requirements\n",
    "    \"\"\"\n",
    "    accuracy, inference_time = evaluate_model(model, testloader, model_name)\n",
    "    sparsity = calculate_sparsity(model)\n",
    "    params = count_nonzero_parameters(model)\n",
    "    \n",
    "    meets_requirement = accuracy >= target_accuracy\n",
    "    status = \"PASS\" if meets_requirement else \"FAIL\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CHALLENGE EVALUATION: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}% (Target: ≥{target_accuracy:.0f}%) - {status}\")\n",
    "    print(f\"Sparsity: {sparsity:.1f}% (Higher is better)\")\n",
    "    print(f\"Active Parameters: {params:,} (Lower is better)\")\n",
    "    print(f\"Inference Time: {inference_time:.2f}s (Lower is better)\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'sparsity': sparsity,\n",
    "        'params': params,\n",
    "        'inference_time': inference_time,\n",
    "        'meets_requirement': meets_requirement\n",
    "    }\n",
    "\n",
    "def create_pruning_visualization(original_result, pruned_result, finetuned_result, pruning_ratio):\n",
    "    \"\"\"\n",
    "    Create visualization comparing original, pruned, and fine-tuned models\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Accuracy comparison\n",
    "    models = ['Original', f'{pruning_ratio:.0%} Pruned', f'{pruning_ratio:.0%} Pruned\\n+ Fine-tuned']\n",
    "    accuracies = [original_result['accuracy'], pruned_result['accuracy'], finetuned_result['accuracy']]\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    \n",
    "    bars1 = ax1.bar(models, accuracies, color=colors, alpha=0.7)\n",
    "    ax1.axhline(y=85, color='red', linestyle='--', alpha=0.8, label='Target (85%)')\n",
    "    ax1.set_ylabel('Accuracy (%)')\n",
    "    ax1.set_title('Model Accuracy Comparison')\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars1, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Model size comparison\n",
    "    params = [original_result['params'], pruned_result['params'], finetuned_result['params']]\n",
    "    params_mb = [p / 1e6 for p in params]  # Convert to millions\n",
    "    \n",
    "    bars2 = ax2.bar(models, params_mb, color=colors, alpha=0.7)\n",
    "    ax2.set_ylabel('Parameters (Millions)')\n",
    "    ax2.set_title('Model Size Comparison')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, param in zip(bars2, params_mb):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{param:.2f}M', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print compression stats\n",
    "    size_reduction = ((original_result['params'] - finetuned_result['params']) / original_result['params']) * 100\n",
    "    print(f\"\\nCOMPRESSION SUMMARY:\")\n",
    "    print(f\"  • Original model: {original_result['params']:,} parameters\")\n",
    "    print(f\"  • Final model: {finetuned_result['params']:,} parameters\")\n",
    "    print(f\"  • Size reduction: {size_reduction:.1f}%\")\n",
    "    print(f\"  • Sparsity achieved: {finetuned_result['sparsity']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238e201a",
   "metadata": {},
   "source": [
    "### Important Note: Understanding Fine-Grained Pruning\n",
    "\n",
    "**Why does a pruned model still have the same parameter count?**\n",
    "\n",
    "Fine-grained (unstructured) pruning works differently than you might expect:\n",
    "\n",
    "1. **Weights are set to zero, not removed**: Pruning sets individual weights to zero but keeps the same model structure\n",
    "2. **Parameter count stays the same**: The model still has the same number of parameters, but many are zero\n",
    "3. **Sparsity is the key metric**: We measure \"sparsity\" (percentage of zero weights) rather than parameter reduction\n",
    "4. **Benefits come from sparsity**: Sparse models can be accelerated with specialized hardware or software\n",
    "\n",
    "**Key Metrics to Track:**\n",
    "- **Total Parameters**: Always the same (model structure unchanged)\n",
    "- **Non-zero Parameters**: Actual \"active\" parameters doing computation\n",
    "- **Sparsity**: Percentage of weights that are zero\n",
    "- **Effective Compression**: Based on non-zero parameters\n",
    "\n",
    "**Example**: A 70% pruned model has:\n",
    "- Same total parameters as original\n",
    "- 70% of weights set to zero (70% sparsity)\n",
    "- Only 30% of weights are non-zero (30% effective size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fee196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDS-ON CHALLENGE: Edge Deployment Optimization\n",
    "# YOUR MISSION: Compress a model to achieve ≥85% accuracy after fine-tuning\n",
    "\n",
    "print(\" EDGE DEPLOYMENT CHALLENGE\")\n",
    "print(\"=\"*50)\n",
    "print(\"Goal: Compress model to 85% accuracy after fine-tuning\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Evaluate baseline model\n",
    "print(\"\\nStep 1: Baseline Assessment\")\n",
    "original_result = evaluate_challenge_model(model, testloader, \"Original Model\", target_accuracy=85.0)\n",
    "\n",
    "# TODO 3: Choose and implement your pruning strategy\n",
    "print(\"\\n TODO 3: Choose your pruning strategy\")\n",
    "print(\"Step 2: Apply Pruning - Choose your compression level\")\n",
    "\n",
    "# TODO: Set your pruning ratio (experiment with different values!)\n",
    "# HINT: Start with 0.5 (50%) and experiment with 0.6, 0.7, 0.8\n",
    "# Higher ratios = more compression but potentially lower accuracy\n",
    "PRUNING_RATIO = _____  # Fill in your chosen ratio (e.g., 0.7 for 70% pruning)\n",
    "\n",
    "print(f\"\\n Applying {PRUNING_RATIO:.0%} pruning to entire model\")\n",
    "print(f\"This will remove {PRUNING_RATIO:.0%} of all weights uniformly across layers\")\n",
    "print(\" Think: What happens to model accuracy immediately after pruning?\")\n",
    "\n",
    "# TODO: Apply your pruning function to the model\n",
    "# HINT: Use copy.deepcopy(model) to avoid modifying the original\n",
    "# Use the apply_fine_grained_pruning function you implemented above\n",
    "pruned_model = _____(copy.deepcopy(_____), _____)  # Fill in function call\n",
    "\n",
    "# TODO: Evaluate the pruned model\n",
    "# HINT: Use evaluate_challenge_model with appropriate parameters\n",
    "pruned_result = evaluate_challenge_model(_____, testloader, f\"{PRUNING_RATIO:.0%} Pruned Model\", target_accuracy=85.0)\n",
    "\n",
    "# Calculate and analyze the pruning impact\n",
    "accuracy_drop = original_result['accuracy'] - pruned_result['accuracy']\n",
    "print(f\"\\n Pruning Impact Analysis:\")\n",
    "print(f\"  • Accuracy drop: {accuracy_drop:.2f}%\")\n",
    "print(f\"  • Sparsity achieved: {pruned_result['sparsity']:.1f}%\")\n",
    "print(f\"  • Active parameters: {pruned_result['params']:,}\")\n",
    "print(f\" Observation: Pruning typically causes accuracy drop initially\")\n",
    "\n",
    "# TODO 4: Implement fine-tuning recovery\n",
    "print(f\"\\n TODO 4: Fine-tune to recover accuracy\")\n",
    "print(f\"Step 3: Fine-tuning the {PRUNING_RATIO:.0%} pruned model\")\n",
    "print(\" Goal: Recover accuracy through targeted training\")\n",
    "\n",
    "# TODO: Fine-tune the pruned model\n",
    "# HINT: Use the fine_tune_pruned_model function\n",
    "# Experiment with different epochs (3-10) and learning rates (0.0001-0.001)\n",
    "finetuned_model = fine_tune_pruned_model(\n",
    "    copy.deepcopy(pruned_model), \n",
    "    trainloader, \n",
    "    testloader, \n",
    "    epochs=_____,        # TODO: Choose number of epochs (try 5)\n",
    "    learning_rate=_____  # TODO: Choose learning rate (try 0.0001)\n",
    ")\n",
    "\n",
    "# TODO: Evaluate the fine-tuned model\n",
    "# HINT: Use evaluate_challenge_model to check if you met the challenge\n",
    "finetuned_result = evaluate_challenge_model(\n",
    "    _____, testloader, f\"{PRUNING_RATIO:.0%} Pruned + Fine-tuned\", target_accuracy=85.0\n",
    ")\n",
    "\n",
    "# Analyze your results\n",
    "recovery = finetuned_result['accuracy'] - pruned_result['accuracy']\n",
    "final_compression = ((original_result['params'] - finetuned_result['params']) / original_result['params']) * 100\n",
    "\n",
    "print(f\"\\n YOUR CHALLENGE RESULTS:\")\n",
    "print(f\"  • Accuracy recovery: +{recovery:.2f}%\")\n",
    "print(f\"  • Final accuracy: {finetuned_result['accuracy']:.2f}%\")\n",
    "print(f\"  • Total compression: {final_compression:.1f}%\")\n",
    "print(f\"  • Challenge status: {'🏆 SUCCESS!' if finetuned_result['meets_requirement'] else '❌ Try Again'}\")\n",
    "\n",
    "# TODO 5: Experiment and optimize\n",
    "print(f\"\\n TODO 5: Optimization Challenge\")\n",
    "if finetuned_result['meets_requirement']:\n",
    "    print(\" CHALLENGE COMPLETED! Try these advanced experiments:\")\n",
    "    print(\"  • Increase pruning ratio (try 0.8 or 0.9)\")\n",
    "    print(\"  • Experiment with different learning rates\")\n",
    "    print(\"  • Try more fine-tuning epochs\")\n",
    "    print(\"  • Compare layer-wise vs global pruning strategies\")\n",
    "else:\n",
    "    print(\" Need to improve! Try these strategies:\")\n",
    "    print(\"  • Reduce pruning ratio (try 0.5 or 0.6)\")\n",
    "    print(\"  • Increase fine-tuning epochs\")\n",
    "    print(\"  • Adjust learning rate (try 0.0005)\")\n",
    "    print(\"  • Use different optimization strategies\")\n",
    "\n",
    "print(f\"\\n Learning Objectives Check:\")\n",
    "print(f\" Applied magnitude-based fine-grained pruning\")\n",
    "print(f\" Measured sparsity: {finetuned_result['sparsity']:.1f}%\")\n",
    "print(f\" Analyzed accuracy vs compression trade-offs\")\n",
    "print(f\" Implemented fine-tuning for accuracy recovery\")\n",
    "\n",
    "# Step 4: Create visualization and final analysis\n",
    "print(f\"\\nStep 4: Results Visualization and Analysis\")\n",
    "create_pruning_visualization(original_result, pruned_result, finetuned_result, PRUNING_RATIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b2ccb",
   "metadata": {},
   "source": [
    "## Step 6: Compare Pruning Thresholds and Weight Selection\n",
    "\n",
    "Let's analyze how magnitude-based pruning selects weights and understand the threshold mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baac909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for threshold analysis\n",
    "def collect_model_weights(model):\n",
    "    \"\"\"\n",
    "    Collect all weights from Conv2d and Linear layers\n",
    "    \"\"\"\n",
    "    all_weights = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            weights = module.weight.data.cpu().flatten().numpy()\n",
    "            all_weights.extend(weights)\n",
    "    return np.array(all_weights)\n",
    "\n",
    "def calculate_pruning_thresholds(weights, pruning_ratios):\n",
    "    \"\"\"\n",
    "    Calculate the magnitude thresholds for different pruning ratios\n",
    "    \"\"\"\n",
    "    abs_weights = np.abs(weights)\n",
    "    sorted_abs_weights = np.sort(abs_weights)\n",
    "    \n",
    "    thresholds = {}\n",
    "    for ratio in pruning_ratios:\n",
    "        threshold_idx = int(len(sorted_abs_weights) * ratio)\n",
    "        thresholds[ratio] = sorted_abs_weights[threshold_idx]\n",
    "    \n",
    "    return thresholds, sorted_abs_weights\n",
    "\n",
    "def create_threshold_visualization(abs_weights, sorted_abs_weights, thresholds, pruning_ratios):\n",
    "    \"\"\"\n",
    "    Create comprehensive threshold visualization\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Weight magnitude distribution with thresholds\n",
    "    ax1.hist(abs_weights, bins=100, alpha=0.7, color='skyblue', density=True, edgecolor='black')\n",
    "    ax1.set_xlabel('|Weight| Value')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.set_title('Weight Magnitude Distribution with Pruning Thresholds')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add threshold lines\n",
    "    colors = ['red', 'orange', 'green', 'purple']\n",
    "    for i, (ratio, threshold) in enumerate(thresholds.items()):\n",
    "        ax1.axvline(x=threshold, color=colors[i], linestyle='--', linewidth=2,\n",
    "                   label=f'{ratio:.0%} pruning (t={threshold:.4f})')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Cumulative distribution of weights\n",
    "    sorted_weights_normalized = sorted_abs_weights / np.max(sorted_abs_weights)\n",
    "    cumulative_prob = np.linspace(0, 100, len(sorted_weights_normalized))\n",
    "    \n",
    "    ax2.plot(sorted_weights_normalized, cumulative_prob, 'b-', linewidth=2)\n",
    "    ax2.set_xlabel('Normalized |Weight| Value')\n",
    "    ax2.set_ylabel('Cumulative Percentage (%)')\n",
    "    ax2.set_title('Cumulative Distribution of Weight Magnitudes')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add pruning ratio lines\n",
    "    for i, ratio in enumerate(pruning_ratios):\n",
    "        ax2.axhline(y=ratio*100, color=colors[i], linestyle='--', \n",
    "                   label=f'{ratio:.0%} pruning line')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Weight survival probability for 50% pruning\n",
    "    survival_rates = []\n",
    "    magnitude_bins = np.linspace(0, np.max(abs_weights), 50)\n",
    "    bin_centers = (magnitude_bins[:-1] + magnitude_bins[1:]) / 2\n",
    "    \n",
    "    for bin_center in bin_centers:\n",
    "        survival_50 = 1.0 if bin_center >= thresholds[0.5] else 0.0\n",
    "        survival_rates.append(survival_50)\n",
    "    \n",
    "    ax3.plot(bin_centers, survival_rates, 'r-', linewidth=3, label='50% Pruning')\n",
    "    ax3.fill_between(bin_centers, 0, survival_rates, alpha=0.3, color='red')\n",
    "    ax3.set_xlabel('|Weight| Value')\n",
    "    ax3.set_ylabel('Survival Probability')\n",
    "    ax3.set_title('Weight Survival Probability (50% Pruning)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Threshold statistics table\n",
    "    ax4.axis('off')\n",
    "    table_data = []\n",
    "    table_data.append(['Pruning Ratio', 'Threshold', 'Weights Removed', 'Weights Kept'])\n",
    "    \n",
    "    total_weights = len(abs_weights)\n",
    "    for ratio in pruning_ratios:\n",
    "        weights_removed = int(total_weights * ratio)\n",
    "        weights_kept = total_weights - weights_removed\n",
    "        table_data.append([\n",
    "            f'{ratio:.0%}',\n",
    "            f'{thresholds[ratio]:.4f}',\n",
    "            f'{weights_removed:,}',\n",
    "            f'{weights_kept:,}'\n",
    "        ])\n",
    "    \n",
    "    table = ax4.table(cellText=table_data[1:], colLabels=table_data[0],\n",
    "                     cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1, 2)\n",
    "    ax4.set_title('Pruning Threshold Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_layer_sensitivity(model, testloader, pruning_ratios=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]):\n",
    "    \"\"\"\n",
    "    Analyze how sensitive each layer is to pruning by measuring accuracy drop.\n",
    "    \"\"\"\n",
    "    layer_sensitivity = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "            print(f\"\\nAnalyzing sensitivity of layer: {name}\")\n",
    "            \n",
    "            # Store original weights\n",
    "            original_weight = module.weight.data.clone()\n",
    "            \n",
    "            # Track accuracy for different pruning ratios\n",
    "            accuracies = []\n",
    "            \n",
    "            for ratio in pruning_ratios:\n",
    "                if ratio == 0.0:\n",
    "                    # No pruning\n",
    "                    accuracy = test_accuracy(model, testloader)\n",
    "                else:\n",
    "                    # Apply pruning to this layer only\n",
    "                    prune.l1_unstructured(module, name='weight', amount=ratio)\n",
    "                    accuracy = test_accuracy(model, testloader)\n",
    "                    \n",
    "                    # Remove pruning to restore original weights\n",
    "                    prune.remove(module, 'weight')\n",
    "                    module.weight.data = original_weight.clone()\n",
    "                \n",
    "                accuracies.append(accuracy)\n",
    "            \n",
    "            # Calculate sensitivity (accuracy drop per pruning ratio)\n",
    "            sensitivity = (accuracies[0] - accuracies[-1]) / pruning_ratios[-1]\n",
    "            layer_sensitivity[name] = {\n",
    "                'sensitivity': sensitivity,\n",
    "                'accuracies': accuracies,\n",
    "                'pruning_ratios': pruning_ratios\n",
    "            }\n",
    "    \n",
    "    return layer_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0574ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDS-ON EXPLORATION: Pruning Threshold Analysis\n",
    "# Analyze how magnitude-based pruning selects weights\n",
    "\n",
    "print(\"🔍 PRUNING THRESHOLD EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "print(\"Understanding how pruning selects which weights to remove\")\n",
    "\n",
    "# TODO 6: Analyze weight distributions and thresholds\n",
    "print(\"\\n🎯 TODO 6: Weight Distribution Analysis\")\n",
    "\n",
    "# TODO: Collect all weights from your models\n",
    "# HINT: Use the collect_model_weights function that's defined above\n",
    "print(\"Collecting weights for threshold analysis...\")\n",
    "\n",
    "# First, let's collect weights from the original model\n",
    "all_weights = collect_model_weights(model)\n",
    "abs_weights = np.abs(all_weights)\n",
    "\n",
    "# TODO: Choose test ratios to analyze (including your challenge ratio)\n",
    "# HINT: Include common ratios like [0.3, 0.5, 0.7, 0.9] and your PRUNING_RATIO\n",
    "test_ratios = [0.3, 0.5, _____, 0.9]  # Fill in your PRUNING_RATIO from the challenge\n",
    "\n",
    "# Calculate thresholds for different pruning ratios\n",
    "thresholds, sorted_abs_weights = calculate_pruning_thresholds(all_weights, test_ratios)\n",
    "\n",
    "# Create comprehensive threshold visualization\n",
    "create_threshold_visualization(abs_weights, sorted_abs_weights, thresholds, test_ratios)\n",
    "\n",
    "# TODO 7: Analyze and interpret the results\n",
    "print(f\"\\n TODO 7: Threshold Analysis Results\")\n",
    "print(f\"{'='*80}\")\n",
    "print(\"MAGNITUDE-BASED PRUNING THRESHOLD ANALYSIS\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "print(f\"Total weights in model: {len(all_weights):,}\")\n",
    "print(f\"Weight magnitude range: {np.min(abs_weights):.6f} to {np.max(abs_weights):.6f}\")\n",
    "print(f\"Mean absolute weight: {np.mean(abs_weights):.6f}\")\n",
    "print(f\"Median absolute weight: {np.median(abs_weights):.6f}\")\n",
    "\n",
    "print(f\"\\n Pruning Thresholds Analysis:\")\n",
    "for ratio in test_ratios:\n",
    "    threshold = thresholds[ratio]\n",
    "    num_pruned = int(len(all_weights) * ratio)\n",
    "    percentage_below = (abs_weights < threshold).mean() * 100\n",
    "    \n",
    "    # TODO: Identify which ratio was used in your challenge\n",
    "    status = \" (YOUR CHALLENGE RATIO)\" if ratio == _____ else \"\"  # Fill in your PRUNING_RATIO\n",
    "    print(f\"  {ratio:.0%} pruning{status}:\")\n",
    "    print(f\"    - Threshold magnitude: {threshold:.6f}\")\n",
    "    print(f\"    - Weights removed: {num_pruned:,}\")\n",
    "    print(f\"    - Percentage below threshold: {percentage_below:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b153240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced layer-wise sparsity analysis\n",
    "def analyze_layer_sparsity_detailed(model, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Detailed analysis of sparsity in each layer\n",
    "    \"\"\"\n",
    "    layer_info = []\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            if hasattr(module, 'weight_mask'):\n",
    "                weight = module.weight * module.weight_mask\n",
    "            else:\n",
    "                weight = module.weight\n",
    "            \n",
    "            total_params = weight.numel()\n",
    "            zero_params = (weight == 0).sum().item()\n",
    "            sparsity = 100.0 * zero_params / total_params\n",
    "            \n",
    "            # Additional statistics\n",
    "            non_zero_weights = weight[weight != 0].detach().cpu().numpy()\n",
    "            mean_abs_weight = np.mean(np.abs(non_zero_weights)) if len(non_zero_weights) > 0 else 0\n",
    "            \n",
    "            layer_info.append({\n",
    "                'name': name,\n",
    "                'type': type(module).__name__,\n",
    "                'shape': list(weight.shape),\n",
    "                'total_params': total_params,\n",
    "                'zero_params': zero_params,\n",
    "                'sparsity': sparsity,\n",
    "                'mean_abs_weight': mean_abs_weight\n",
    "            })\n",
    "    \n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"LAYER-WISE SPARSITY ANALYSIS: {model_name}\")\n",
    "    print(f\"{'='*90}\")\n",
    "    print(f\"{'Layer':<20} {'Type':<8} {'Shape':<15} {'Total':<8} {'Zeros':<8} {'Sparsity':<8} {'Mean |W|':<10}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for info in layer_info:\n",
    "        shape_str = str(info['shape'])\n",
    "        if len(shape_str) > 14:\n",
    "            shape_str = shape_str[:11] + \"...\"\n",
    "        print(f\"{info['name']:<20} {info['type']:<8} {shape_str:<15} \"\n",
    "              f\"{info['total_params']:<8} {info['zero_params']:<8} \"\n",
    "              f\"{info['sparsity']:<7.1f}% {info['mean_abs_weight']:<10.4f}\")\n",
    "    \n",
    "    return layer_info\n",
    "\n",
    "# Compare layer-wise effects between original and pruned models\n",
    "print(\"\\nComparing layer-wise sparsity between original and pruned models...\")\n",
    "\n",
    "# Analyze original model\n",
    "original_layer_info = analyze_layer_sparsity_detailed(model, \"Original Model\")\n",
    "\n",
    "# Analyze the pruned model from our challenge\n",
    "pruned_layer_info = analyze_layer_sparsity_detailed(pruned_model, f\"{PRUNING_RATIO:.0%} Pruned Model (Challenge)\")\n",
    "\n",
    "print(f\"\\nKey Observations:\")\n",
    "print(f\"- Original model has 0% sparsity across all layers\")\n",
    "print(f\"- {PRUNING_RATIO:.0%} pruned model has uniform {PRUNING_RATIO:.0%} sparsity across all layers\")\n",
    "print(f\"- Global pruning creates consistent sparsity patterns\")\n",
    "print(f\"- Mean weight magnitude increases after pruning (smallest weights removed)\")\n",
    "\n",
    "print(\"\\nLayer-wise sparsity analysis complete!\")\n",
    "print(\"Global pruning creates uniform sparsity across all layers, as expected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e000e",
   "metadata": {},
   "source": [
    "## Step 7: Performance Summary and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results summary for the challenge\n",
    "def create_results_summary_table():\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary table of challenge pruning results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"                      FINE-GRAINED PRUNING RESULTS SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"{'Model':<25} {'Sparsity':<12} {'Accuracy':<12} {'Acc Drop':<12} {'Active Params':<15}\")\n",
    "    print(\"-\" * 85)\n",
    "    \n",
    "    # Original model\n",
    "    print(f\"{'Original':<25} {'0.0%':<12} {original_result['accuracy']:<11.2f}% {'0.0%':<12} {original_result['params']:<15,}\")\n",
    "    \n",
    "    # Pruned model from challenge\n",
    "    pruned_acc_drop = original_result['accuracy'] - pruned_result['accuracy']\n",
    "    print(f\"{f'{PRUNING_RATIO:.0%} Pruned':<25} {pruned_result['sparsity']:<11.1f}% {pruned_result['accuracy']:<11.2f}% {pruned_acc_drop:<11.2f}% {pruned_result['params']:<15,}\")\n",
    "    \n",
    "    # Fine-tuned model from challenge\n",
    "    ft_acc_drop = original_result['accuracy'] - finetuned_result['accuracy']\n",
    "    print(f\"{f'{PRUNING_RATIO:.0%} Pruned + Fine-tuned':<25} {finetuned_result['sparsity']:<11.1f}% {finetuned_result['accuracy']:<11.2f}% {ft_acc_drop:<11.2f}% {finetuned_result['params']:<15,}\")\n",
    "\n",
    "def highlight_key_insights():\n",
    "    \"\"\"\n",
    "    Highlight the key insights from the challenge pruning experiment\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY INSIGHTS FROM FINE-GRAINED PRUNING CHALLENGE:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate key metrics from challenge results\n",
    "    param_reduction = ((original_result['params'] - finetuned_result['params']) / original_result['params']) * 100\n",
    "    ft_acc_drop = original_result['accuracy'] - finetuned_result['accuracy']\n",
    "    recovery = finetuned_result['accuracy'] - pruned_result['accuracy']\n",
    "    \n",
    "    print(f\"• {PRUNING_RATIO:.0%} pruning reduces active parameters by ~{param_reduction:.0f}% with only {ft_acc_drop:.1f}% final accuracy drop\")\n",
    "    print(f\"• Fine-tuning recovers {recovery:.1f}% accuracy after aggressive pruning\")\n",
    "    print(f\"• Challenge target: ≥85% accuracy - {'ACHIEVED' if finetuned_result['accuracy'] >= 85 else 'NOT ACHIEVED'}\")\n",
    "    print(f\"• Model compression: {original_result['params']:,} → {finetuned_result['params']:,} parameters\")\n",
    "    print(f\"• Sparsity achieved: {finetuned_result['sparsity']:.1f}%\")\n",
    "    print(f\"• Fine-grained pruning enables efficient model deployment\")\n",
    "    \n",
    "    print(f\"\\nPractical Applications:\")\n",
    "    print(f\"• Mobile and edge device deployment\")\n",
    "    print(f\"• Reducing memory bandwidth requirements\")\n",
    "    print(f\"• Enabling larger models on constrained hardware\")\n",
    "    print(f\"• Battery life improvement in mobile devices\")\n",
    "    \n",
    "    print(f\"\\nNext Steps for Further Optimization:\")\n",
    "    print(f\"• Try different pruning ratios (50%, 60%, 80%)\")\n",
    "    print(f\"• Experiment with structured pruning for better hardware acceleration\")\n",
    "    print(f\"• Combine pruning with quantization for maximum compression\")\n",
    "    print(f\"• Test layer-wise sensitivity for strategic pruning\")\n",
    "\n",
    "# Generate the comprehensive summary\n",
    "create_results_summary_table()\n",
    "highlight_key_insights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
