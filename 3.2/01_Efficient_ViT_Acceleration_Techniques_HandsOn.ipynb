{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d817b5d7",
   "metadata": {},
   "source": [
    "# Efficient Vision Transformer (ViT) Acceleration Techniques - Hands-On Practice\n",
    "\n",
    "## Learning Objectives\n",
    "Upon completion of this notebook, you will be able to:\n",
    "- **Implement** Window Attention mechanisms for spatial locality in vision transformers\n",
    "- **Code** Linear Attention using kernel approximations for linear complexity scaling\n",
    "- **Develop** Sparse Attention patterns for efficient long-range dependencies\n",
    "- **Analyze** computational complexity trade-offs between different attention mechanisms\n",
    "- **Benchmark** performance differences across attention variants\n",
    "- **Visualize** attention patterns and efficiency characteristics\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Vision Transformers (ViTs) have demonstrated remarkable performance in computer vision tasks, but their quadratic computational complexity with respect to sequence length poses significant challenges for practical deployment. This notebook presents a comprehensive analysis of three prominent acceleration techniques: Window Attention, Linear Attention, and Sparse Attention.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The self-attention mechanism in Vision Transformers exhibits O(n²) complexity, where n represents the number of image patches. For high-resolution images, this becomes computationally prohibitive. We investigate three distinct approaches to address this limitation:\n",
    "\n",
    "1. **Window Attention**: Restricts attention computation to local windows, reducing complexity to O(n)\n",
    "2. **Linear Attention**: Approximates full attention using kernel methods with O(n) complexity\n",
    "3. **Sparse Attention**: Selectively attends to a subset of tokens based on learned or fixed patterns\n",
    "\n",
    "Each technique represents a different trade-off between computational efficiency and representational capacity.\n",
    "\n",
    "---\n",
    "**🔥 HANDS-ON PRACTICE**: This notebook contains code completion exercises marked with `# TODO:` comments. Complete the missing implementations to build efficient attention mechanisms from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69416be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe00b9",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "### Standard Self-Attention Mechanism\n",
    "\n",
    "The standard self-attention mechanism works by computing relationships between all pairs of input tokens. For each token, it calculates how much attention to pay to every other token in the sequence. This process involves three main steps:\n",
    "\n",
    "1. **Query, Key, Value Creation**: Each input token is transformed into three vectors - a query vector (what information this token is looking for), a key vector (what information this token contains), and a value vector (the actual information this token carries).\n",
    "\n",
    "2. **Attention Score Calculation**: For each token, we compute similarity scores between its query vector and the key vectors of all other tokens. These scores determine how much each token should attend to every other token.\n",
    "\n",
    "3. **Weighted Combination**: The attention scores are normalized using softmax to create attention weights, then used to compute a weighted combination of all value vectors.\n",
    "\n",
    "**Computational Challenge**: The main bottleneck is computing attention scores between all pairs of tokens, which requires n-squared operations where n is the number of tokens. This becomes prohibitively expensive for long sequences.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "For high-resolution images, Vision Transformers divide the image into patches (typically 16x16 pixels each). A 224x224 image creates 196 patches, but a 1024x1024 image creates 4,096 patches. The attention computation scales quadratically with the number of patches, making standard ViT computationally intractable for high-resolution images. This motivates the development of efficient attention mechanisms that maintain the quality of standard attention while reducing computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Vision Transformer Components - HANDS-ON EXERCISE\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Convert image to patch embeddings\"\"\"\n",
    "    def __init__(self, img_size: int = 224, patch_size: int = 16, in_channels: int = 3, embed_dim: int = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # TODO: Create a convolutional layer that converts patches to embeddings\n",
    "        # HINT: Use nn.Conv2d with kernel_size=patch_size, stride=patch_size\n",
    "        # Input channels: in_channels, Output channels: embed_dim\n",
    "        self.projection = # Your code here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Apply the projection and reshape for transformer input\n",
    "        # HINT: 1) Apply self.projection, 2) Flatten spatial dims, 3) Transpose to (B, N, embed_dim)\n",
    "        # x: (B, C, H, W) -> (B, embed_dim, H/P, W/P) -> (B, N, embed_dim)\n",
    "        x = # Your code here - apply projection\n",
    "        x = # Your code here - flatten and transpose\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Standard multi-head self-attention mechanism - COMPLETE THE MISSING PARTS\"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        # TODO: Calculate head dimension\n",
    "        # HINT: embed_dim divided by num_heads\n",
    "        self.head_dim = # Your code here\n",
    "        \n",
    "        # TODO: Calculate scaling factor for attention scores\n",
    "        # HINT: Use head_dim ** -0.5 (1/sqrt(head_dim))\n",
    "        self.scale = # Your code here\n",
    "        \n",
    "        # TODO: Create linear layer for Q, K, V projections\n",
    "        # HINT: Input: embed_dim, Output: embed_dim * 3 (for Q, K, V)\n",
    "        self.qkv = # Your code here\n",
    "        \n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # TODO: Generate Q, K, V matrices and reshape for multi-head attention\n",
    "        # HINT: 1) Apply self.qkv, 2) Reshape to (B, N, 3, num_heads, head_dim)\n",
    "        # 3) Permute to (3, B, num_heads, N, head_dim), 4) Split into q, k, v\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        # TODO: Compute scaled dot-product attention\n",
    "        # HINT: 1) Compute q @ k.transpose(-2, -1), 2) Multiply by self.scale\n",
    "        # 3) Apply softmax, 4) Apply dropout\n",
    "        attn = # Your code here - compute attention scores\n",
    "        attn = # Your code here - apply softmax\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # TODO: Apply attention to values and reshape output\n",
    "        # HINT: 1) Multiply attn @ v, 2) Transpose and reshape back to (B, N, C)\n",
    "        x = # Your code here\n",
    "        x = self.proj(x)\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec479019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Standard Transformer block with attention and FFN\"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int = 8, mlp_ratio: float = 4.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        mlp_dim = int(embed_dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pre-norm architecture\n",
    "        attn_out, attn_weights = self.attn(self.norm1(x))\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeec1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Attention Complexity Analysis\n",
    "\n",
    "def measure_attention_complexity():\n",
    "    \"\"\"Empirically measure attention computational complexity\"\"\"\n",
    "    sequence_lengths = [64, 128, 256, 512, 1024]\n",
    "    embed_dim = 256\n",
    "    times = []\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        x = torch.randn(4, seq_len, embed_dim).to(device)\n",
    "        attn = MultiHeadAttention(embed_dim).to(device)\n",
    "        \n",
    "        # Warm up\n",
    "        for _ in range(10):\n",
    "            _ = attn(x)\n",
    "        \n",
    "        # Measure time\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(100):\n",
    "            _ = attn(x)\n",
    "            \n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        times.append((end_time - start_time) / 100)\n",
    "    \n",
    "    return sequence_lengths, times\n",
    "\n",
    "seq_lens, standard_times = measure_attention_complexity()\n",
    "print(\"Standard Attention Complexity Analysis:\")\n",
    "for seq_len, exec_time in zip(seq_lens, standard_times):\n",
    "    print(f\"Sequence length {seq_len}: {exec_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6c6bcb",
   "metadata": {},
   "source": [
    "## 1. Window Attention Mechanism\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "Window Attention, popularized by the Swin Transformer, restricts the self-attention computation to non-overlapping local windows. This approach is motivated by the observation that natural images exhibit strong local correlations.\n",
    "\n",
    "**Key Principles:**\n",
    "1. **Locality Bias**: Most relevant information for a patch lies in its spatial neighborhood\n",
    "2. **Computational Efficiency**: Reduces complexity from quadratic to linear scaling where n is the total number of patches\n",
    "3. **Hierarchical Processing**: Enables multi-scale feature learning through window shifting\n",
    "\n",
    "**How Window Attention Works:**\n",
    "\n",
    "Instead of computing attention between all possible patch pairs in an image, Window Attention divides the image into non-overlapping rectangular windows (typically 7x7 patches). Within each window, standard self-attention is computed, but tokens cannot attend to patches outside their window.\n",
    "\n",
    "For example, if an image has 196 patches (14x14) and we use 7x7 windows, we create 4 windows of 49 patches each. Instead of computing attention across all 196×196 = 38,416 pairs, we compute attention within each of the 4 windows: 4×(49×49) = 9,604 pairs - a significant reduction.\n",
    "\n",
    "To maintain some global connectivity, Swin Transformer uses **shifted windows** in alternating layers, where the window boundaries are moved by half the window size, allowing information to flow between previously separate windows.\n",
    "\n",
    "**Complexity Analysis:**\n",
    "- Standard Attention: Quadratic scaling with sequence length\n",
    "- Window Attention: Linear scaling (assuming fixed window size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c109eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Attention Helper Functions\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Partition feature map into non-overlapping windows\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size: int\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Merge windows back to feature map\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size: int\n",
    "        H, W: height and width of feature map\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0118800f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDS-ON EXERCISE: Implement Window Attention\n",
    "\n",
    "class SimpleWindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Window Attention for arbitrary sequence lengths - COMPLETE THE IMPLEMENTATION\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, window_size, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        # TODO: Calculate head dimension and scale factor (similar to standard attention)\n",
    "        self.head_dim = # Your code here\n",
    "        self.scale = # Your code here\n",
    "        \n",
    "        # TODO: Create QKV projection and output projection layers\n",
    "        self.qkv = # Your code here\n",
    "        self.proj = # Your code here\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # TODO: Pad sequence to be divisible by window_size\n",
    "        # HINT: Calculate how much padding is needed using modulo operation\n",
    "        padding_length = # Your code here - calculate padding needed\n",
    "        if padding_length > 0:\n",
    "            # TODO: Create padding tensor and concatenate with input\n",
    "            padding = # Your code here - create zero padding\n",
    "            x_padded = # Your code here - concatenate padding\n",
    "        else:\n",
    "            x_padded = x\n",
    "        \n",
    "        N_padded = x_padded.shape[1]\n",
    "        # TODO: Calculate number of windows\n",
    "        num_windows = # Your code here\n",
    "        \n",
    "        # TODO: Reshape input into windows\n",
    "        # HINT: First reshape to (B, num_windows, window_size, C), then to (B*num_windows, window_size, C)\n",
    "        x_windows = x_padded.view(B, num_windows, self.window_size, C)\n",
    "        x_windows = # Your code here - reshape for processing\n",
    "        \n",
    "        # TODO: Apply attention within each window\n",
    "        # HINT: This is similar to standard attention but operates on windows\n",
    "        qkv = self.qkv(x_windows).reshape(B * num_windows, self.window_size, 3, self.num_heads, self.head_dim)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        # TODO: Compute attention scores, apply softmax, and get attended values\n",
    "        attn = # Your code here - compute scaled attention scores\n",
    "        attn = # Your code here - apply softmax\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # TODO: Apply attention to values and project\n",
    "        x_attended = # Your code here - apply attention to values and reshape\n",
    "        x_attended = self.proj(x_attended)\n",
    "        \n",
    "        # TODO: Reshape back to original format and remove padding\n",
    "        # HINT: 1) Reshape to (B, num_windows, window_size, C), 2) View as (B, N_padded, C), 3) Remove padding\n",
    "        x_attended = x_attended.view(B, num_windows, self.window_size, C)\n",
    "        x_attended = # Your code here - reshape to sequence format\n",
    "        \n",
    "        # TODO: Remove padding to get back to original sequence length\n",
    "        x_attended = # Your code here - slice to remove padding\n",
    "        \n",
    "        return x_attended\n",
    "\n",
    "# Test your implementation\n",
    "print(\"Testing SimpleWindowAttention implementation...\")\n",
    "try:\n",
    "    test_window_attn = SimpleWindowAttention(embed_dim=256, window_size=8, num_heads=8)\n",
    "    test_input = torch.randn(2, 100, 256)  # Batch=2, Seq=100, Dim=256\n",
    "    test_output = test_window_attn(test_input)\n",
    "    print(f\"✅ Success! Input shape: {test_input.shape}, Output shape: {test_output.shape}\")\n",
    "    print(f\"Window size: {test_window_attn.window_size}, Windows needed: {(100 + 8 - 1) // 8}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in implementation: {e}\")\n",
    "    print(\"💡 Check your TODO implementations above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window-based Multi-Head Self-Attention with relative position bias\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        # Define relative position bias table\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n",
    "\n",
    "        # Get pair-wise relative position indices\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))\n",
    "        coords_flatten = torch.flatten(coords, 1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "        else:\n",
    "            attn = attn.softmax(dim=-1)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915a857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"Swin Transformer Block with Window Attention\"\"\"\n",
    "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, \n",
    "                 mlp_ratio=4., qkv_bias=True, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        \n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, dropout=dropout)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            H, W = self.input_resolution\n",
    "            img_mask = torch.zeros((1, H, W, 1))\n",
    "            h_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            w_slices = (slice(0, -self.window_size),\n",
    "                        slice(-self.window_size, -self.shift_size),\n",
    "                        slice(-self.shift_size, None))\n",
    "            cnt = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    img_mask[:, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "            mask_windows = window_partition(img_mask, self.window_size)\n",
    "            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"Input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # Cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # Partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows, attention_weights = self.attn(x_windows, mask=self.attn_mask)\n",
    "\n",
    "        # Merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)\n",
    "\n",
    "        # Reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dacb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Attention Complexity Analysis\n",
    "\n",
    "def analyze_window_attention_complexity():\n",
    "    \"\"\"Analyze computational complexity of window attention\"\"\"\n",
    "    sequence_lengths = [64, 128, 256, 512]\n",
    "    window_sizes = [8, 16, 32]\n",
    "    embed_dim = 96\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        x = torch.randn(4, seq_len, embed_dim).to(device)\n",
    "        \n",
    "        for window_size in window_sizes:\n",
    "            if window_size <= seq_len:\n",
    "                # Simple window attention\n",
    "                window_attn = SimpleWindowAttention(\n",
    "                    embed_dim=embed_dim,\n",
    "                    window_size=window_size,\n",
    "                    num_heads=3\n",
    "                ).to(device)\n",
    "                \n",
    "                # Measure time\n",
    "                torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                start_time = time.time()\n",
    "                \n",
    "                for _ in range(50):\n",
    "                    _ = window_attn(x)\n",
    "                \n",
    "                torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "                end_time = time.time()\n",
    "                \n",
    "                avg_time = (end_time - start_time) / 50\n",
    "                results[f'Window-{window_size}'].append((seq_len, avg_time))\n",
    "    \n",
    "    return results\n",
    "\n",
    "window_complexity_results = analyze_window_attention_complexity()\n",
    "print(\"Window Attention Complexity Analysis:\")\n",
    "for method, timing_data in window_complexity_results.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    for seq_len, exec_time in timing_data:\n",
    "        print(f\"  Sequence length {seq_len}: {exec_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690a5433",
   "metadata": {},
   "source": [
    "## 2. Linear Attention Mechanism\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "Linear Attention approximates the standard softmax attention using kernel methods, achieving linear complexity while maintaining global receptive fields. The key insight is to reformulate attention computation using the associative property of matrix multiplication.\n",
    "\n",
    "**How Linear Attention Works:**\n",
    "\n",
    "Standard attention computes attention weights by taking the softmax of query-key similarities, then multiplying by values. Linear attention approximates this process using kernel functions that map queries and keys to a higher-dimensional feature space.\n",
    "\n",
    "The main innovation is reordering the computation:\n",
    "1. **Standard approach**: First compute all query-key similarities (expensive), then combine with values\n",
    "2. **Linear approach**: First combine keys with values using kernel functions (cheaper), then compute with queries\n",
    "\n",
    "This reordering changes the computational complexity from quadratic to linear in sequence length.\n",
    "\n",
    "**Kernel Functions:**\n",
    "\n",
    "Linear attention uses kernel functions to approximate the softmax operation:\n",
    "\n",
    "- **ELU-based kernel**: Uses the ELU activation function plus 1 to ensure positive values, providing a good approximation to softmax while being computationally efficient\n",
    "- **ReLU-based kernel**: Uses ReLU activation, which is very fast but may provide a coarser approximation\n",
    "- **Feature mapping kernel**: Splits each element into positive and negative parts, doubling the feature dimension but often improving approximation quality\n",
    "\n",
    "**Key Properties:**\n",
    "1. **Complexity Reduction**: Changes from quadratic to linear scaling with sequence length\n",
    "2. **Global Receptive Field**: Unlike window attention, maintains ability to attend to all tokens\n",
    "3. **Approximation Quality**: Trade-off between efficiency and attention accuracy - kernel choice significantly affects how well it approximates standard attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8adcd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDS-ON EXERCISE: Implement Linear Attention\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Attention mechanism using kernel approximation - COMPLETE THE IMPLEMENTATION\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=8, kernel_type='elu', dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.kernel_type = kernel_type\n",
    "        \n",
    "        # TODO: Create QKV projection and output projection\n",
    "        self.qkv = # Your code here\n",
    "        self.proj = # Your code here\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # TODO: Set up kernel function based on kernel_type\n",
    "        # HINT: Use if-elif statements to assign the correct kernel function\n",
    "        if kernel_type == 'elu':\n",
    "            self.kernel_fn = # Your code here\n",
    "        elif kernel_type == 'relu':\n",
    "            self.kernel_fn = # Your code here\n",
    "        elif kernel_type == 'feature_map':\n",
    "            self.kernel_fn = # Your code here\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n",
    "    \n",
    "    def _elu_kernel(self, x):\n",
    "        \"\"\"ELU-based kernel function\"\"\"\n",
    "        # TODO: Apply ELU activation and add 1 to ensure positive values\n",
    "        # HINT: F.elu(x) + 1\n",
    "        return # Your code here\n",
    "    \n",
    "    def _relu_kernel(self, x):\n",
    "        \"\"\"ReLU-based kernel function\"\"\"\n",
    "        # TODO: Apply ReLU activation\n",
    "        return # Your code here\n",
    "    \n",
    "    def _feature_map_kernel(self, x):\n",
    "        \"\"\"Feature mapping kernel: [x+, x-]\"\"\"\n",
    "        # TODO: Split into positive and negative parts, then concatenate\n",
    "        # HINT: Use F.relu(x) for positive part and F.relu(-x) for negative part\n",
    "        x_pos = # Your code here\n",
    "        x_neg = # Your code here\n",
    "        return # Your code here - concatenate along last dimension\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        # Generate Q, K, V (same as standard attention)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)  # (B, num_heads, N, head_dim)\n",
    "        \n",
    "        # TODO: Apply kernel function to Q and K\n",
    "        # HINT: Use self.kernel_fn() on both q and k\n",
    "        q_kernel = # Your code here\n",
    "        k_kernel = # Your code here\n",
    "        \n",
    "        # TODO: Implement linear attention computation: φ(Q)(φ(K)^T V)\n",
    "        # Step 1: Compute φ(K)^T V using einsum\n",
    "        # HINT: Use 'bhnd,bhnf->bhdf' to multiply k_kernel^T with v\n",
    "        kv = # Your code here\n",
    "        \n",
    "        # Step 2: Compute φ(Q)(φ(K)^T V) using einsum\n",
    "        # HINT: Use 'bhnd,bhdf->bhnf' to multiply q_kernel with kv\n",
    "        qkv_out = # Your code here\n",
    "        \n",
    "        # TODO: Compute normalization factor\n",
    "        # Step 1: Sum k_kernel across sequence dimension\n",
    "        k_sum = # Your code here - sum along sequence dimension, keep dims\n",
    "        \n",
    "        # Step 2: Compute normalizer using einsum\n",
    "        # HINT: Use 'bhnd,bhnd->bhn' to compute dot product of q_kernel and k_sum\n",
    "        normalizer = # Your code here\n",
    "        normalizer = normalizer.unsqueeze(-1)  # Add dimension for broadcasting\n",
    "        \n",
    "        # TODO: Apply normalization\n",
    "        qkv_out = # Your code here - divide by normalizer (add small epsilon)\n",
    "        \n",
    "        # TODO: Reshape and project output\n",
    "        # HINT: 1) Transpose to (B, N, num_heads, head_dim), 2) Reshape to (B, N, C), 3) Apply projection\n",
    "        qkv_out = qkv_out.transpose(1, 2).reshape(B, N, C)\n",
    "        output = # Your code here - apply projection and dropout\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test your implementation\n",
    "print(\"Testing LinearAttention implementation...\")\n",
    "try:\n",
    "    test_linear_attn = LinearAttention(embed_dim=256, num_heads=8, kernel_type='elu')\n",
    "    test_input = torch.randn(2, 100, 256)\n",
    "    test_output = test_linear_attn(test_input)\n",
    "    print(f\"✅ Success! Input shape: {test_input.shape}, Output shape: {test_output.shape}\")\n",
    "    print(f\"Kernel type: {test_linear_attn.kernel_type}\")\n",
    "    \n",
    "    # Test different kernels\n",
    "    for kernel in ['elu', 'relu', 'feature_map']:\n",
    "        test_attn = LinearAttention(embed_dim=256, kernel_type=kernel)\n",
    "        output = test_attn(test_input)\n",
    "        print(f\"✅ {kernel} kernel works! Output shape: {output.shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in implementation: {e}\")\n",
    "    print(\"💡 Check your TODO implementations above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8c708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalLinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal Linear Attention for autoregressive tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=8, kernel_type='elu', dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.kernel_type = kernel_type\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        if kernel_type == 'elu':\n",
    "            self.kernel_fn = lambda x: F.elu(x) + 1\n",
    "        elif kernel_type == 'relu':\n",
    "            self.kernel_fn = F.relu\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown kernel type: {kernel_type}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        # Apply kernel function\n",
    "        q_kernel = self.kernel_fn(q)\n",
    "        k_kernel = self.kernel_fn(k)\n",
    "        \n",
    "        # Causal linear attention using cumulative sums\n",
    "        output = torch.zeros_like(q)\n",
    "        kv_state = torch.zeros(B, self.num_heads, self.head_dim, self.head_dim).to(x.device)\n",
    "        k_state = torch.zeros(B, self.num_heads, self.head_dim).to(x.device)\n",
    "        \n",
    "        for i in range(N):\n",
    "            # Update states\n",
    "            kv_state = kv_state + torch.einsum('bhd,bhf->bhdf', k_kernel[:, :, i], v[:, :, i])\n",
    "            k_state = k_state + k_kernel[:, :, i]\n",
    "            \n",
    "            # Compute output\n",
    "            output[:, :, i] = torch.einsum('bhd,bhdf->bhf', q_kernel[:, :, i], kv_state)\n",
    "            normalizer = torch.einsum('bhd,bhd->bh', q_kernel[:, :, i], k_state).unsqueeze(-1)\n",
    "            output[:, :, i] = output[:, :, i] / (normalizer + 1e-6)\n",
    "        \n",
    "        # Reshape and project\n",
    "        output = output.transpose(1, 2).reshape(B, N, C)\n",
    "        output = self.proj(output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Attention Analysis Functions\n",
    "\n",
    "def compare_attention_approximation_quality():\n",
    "    \"\"\"Compare approximation quality of linear attention vs standard attention\"\"\"\n",
    "    embed_dim = 256\n",
    "    seq_len = 128\n",
    "    batch_size = 4\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, embed_dim).to(device)\n",
    "    \n",
    "    # Standard attention\n",
    "    std_attn = MultiHeadAttention(embed_dim).to(device)\n",
    "    std_output, std_attn_weights = std_attn(x)\n",
    "    \n",
    "    # Linear attention variants\n",
    "    kernels = ['elu', 'relu', 'feature_map']\n",
    "    results = {}\n",
    "    \n",
    "    for kernel_type in kernels:\n",
    "        linear_attn = LinearAttention(embed_dim, kernel_type=kernel_type).to(device)\n",
    "        linear_output = linear_attn(x)\n",
    "        \n",
    "        # Compute approximation error\n",
    "        mse_error = F.mse_loss(linear_output, std_output).item()\n",
    "        cosine_sim = F.cosine_similarity(\n",
    "            linear_output.view(-1), std_output.view(-1), dim=0\n",
    "        ).item()\n",
    "        \n",
    "        results[kernel_type] = {\n",
    "            'mse_error': mse_error,\n",
    "            'cosine_similarity': cosine_sim\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def measure_linear_attention_complexity():\n",
    "    \"\"\"Measure computational complexity of linear attention\"\"\"\n",
    "    sequence_lengths = [64, 128, 256, 512, 1024]\n",
    "    embed_dim = 256\n",
    "    \n",
    "    std_times = []\n",
    "    linear_times = []\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        x = torch.randn(4, seq_len, embed_dim).to(device)\n",
    "        \n",
    "        # Standard attention\n",
    "        std_attn = MultiHeadAttention(embed_dim).to(device)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start_time = time.time()\n",
    "        for _ in range(50):\n",
    "            _ = std_attn(x)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        std_times.append((time.time() - start_time) / 50)\n",
    "        \n",
    "        # Linear attention\n",
    "        linear_attn = LinearAttention(embed_dim, kernel_type='elu').to(device)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start_time = time.time()\n",
    "        for _ in range(50):\n",
    "            _ = linear_attn(x)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        linear_times.append((time.time() - start_time) / 50)\n",
    "    \n",
    "    return sequence_lengths, std_times, linear_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d43e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Linear Attention Analyses\n",
    "\n",
    "print(\"Linear Attention Approximation Quality Analysis:\")\n",
    "approx_results = compare_attention_approximation_quality()\n",
    "for kernel, metrics in approx_results.items():\n",
    "    print(f\"{kernel} kernel:\")\n",
    "    print(f\"  MSE Error: {metrics['mse_error']:.6f}\")\n",
    "    print(f\"  Cosine Similarity: {metrics['cosine_similarity']:.4f}\")\n",
    "\n",
    "print(\"\\nLinear Attention Complexity Analysis:\")\n",
    "seq_lens, std_times, linear_times = measure_linear_attention_complexity()\n",
    "for seq_len, std_time, linear_time in zip(seq_lens, std_times, linear_times):\n",
    "    speedup = std_time / linear_time\n",
    "    print(f\"Seq len {seq_len}: Standard {std_time:.4f}s, Linear {linear_time:.4f}s, Speedup: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0aee5",
   "metadata": {},
   "source": [
    "## 3. Sparse Attention Mechanism\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "Sparse Attention reduces computational complexity by selectively attending to a subset of input tokens. This approach is motivated by the observation that not all token pairs contribute equally to the final representation.\n",
    "\n",
    "**How Sparse Attention Works:**\n",
    "\n",
    "Instead of computing attention between every pair of tokens, sparse attention uses predefined or learned patterns to determine which tokens can attend to which other tokens. This creates a sparse attention matrix where many entries are zero.\n",
    "\n",
    "**Key Insight**: In many tasks, tokens primarily need to attend to:\n",
    "- **Local neighbors** (for capturing local patterns)\n",
    "- **A few global tokens** (for capturing long-range dependencies)\n",
    "- **Specific positions** based on task requirements\n",
    "\n",
    "**Standard vs Sparse Attention:**\n",
    "\n",
    "**Standard Attention** computes a full attention matrix where every token can attend to every other token. For a sequence of length n, this requires n² attention computations.\n",
    "\n",
    "**Sparse Attention** restricts attention to a sparse pattern where each token only attends to a small subset of other tokens. If each token attends to only s other tokens (where s << n), the computation becomes linear in sequence length.\n",
    "\n",
    "**Sparsity Patterns:**\n",
    "\n",
    "1. **Random Sparse**: Randomly sample a subset of connections for each token\n",
    "2. **Local Window**: Each token attends only to tokens within a fixed window around it\n",
    "3. **Strided**: Each token attends to every k-th token (useful for capturing regular patterns)\n",
    "4. **Global + Local**: Combine a few global tokens (that attend to everything) with local windows\n",
    "5. **Learned Sparse**: Use machine learning to discover which connections are most important\n",
    "\n",
    "**Complexity Analysis:**\n",
    "- **Standard Attention**: Quadratic scaling - every token attends to every other token\n",
    "- **Sparse Attention**: Approximately linear scaling - each token attends to only a fixed number of other tokens\n",
    "\n",
    "The **BigBird** model exemplifies effective sparse attention by combining global attention (some tokens attend to all others), local attention (sliding windows), and random attention (random long-range connections) to maintain both efficiency and modeling capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c0889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDS-ON EXERCISE: Implement Sparse Attention Patterns\n",
    "\n",
    "def create_sparse_mask(seq_len, pattern_type='random', sparsity_factor=4, window_size=None):\n",
    "    \"\"\"\n",
    "    Create different types of sparse attention masks - COMPLETE THE IMPLEMENTATION\n",
    "    \"\"\"\n",
    "    if pattern_type == 'random':\n",
    "        # TODO: Create random sparse pattern\n",
    "        # HINT: 1) Create zero matrix, 2) Generate random indices, 3) Set those positions to 1\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        # Calculate how many connections to keep\n",
    "        total_connections = seq_len * seq_len\n",
    "        connections_to_keep = total_connections // sparsity_factor\n",
    "        \n",
    "        # TODO: Generate random indices and set them to 1\n",
    "        indices = # Your code here - use torch.randperm to get random indices\n",
    "        mask.view(-1)[indices] = 1\n",
    "        return mask.bool()\n",
    "    \n",
    "    elif pattern_type == 'local_window':\n",
    "        # TODO: Create local window pattern\n",
    "        # HINT: For each position i, allow attention to positions within window_size//2\n",
    "        window_size = window_size or seq_len // 8\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            # TODO: Calculate start and end positions for the window\n",
    "            start = # Your code here - max(0, i - window_size // 2)\n",
    "            end = # Your code here - min(seq_len, i + window_size // 2 + 1)\n",
    "            # TODO: Set the window positions to 1\n",
    "            mask[i, start:end] = # Your code here\n",
    "            \n",
    "        return mask.bool()\n",
    "    \n",
    "    elif pattern_type == 'strided':\n",
    "        # TODO: Create strided pattern\n",
    "        # HINT: For each position i, attend to every sparsity_factor-th position starting from i\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        for i in range(seq_len):\n",
    "            # TODO: Set every sparsity_factor-th position to 1\n",
    "            mask[i, i::sparsity_factor] = # Your code here\n",
    "        return mask.bool()\n",
    "    \n",
    "    elif pattern_type == 'global_local':\n",
    "        # TODO: Create global + local pattern (BigBird style)\n",
    "        # HINT: 1) Some tokens attend globally, 2) All tokens have local attention\n",
    "        mask = torch.zeros(seq_len, seq_len)\n",
    "        window_size = window_size or 32\n",
    "        global_tokens = min(seq_len // 16, 64)\n",
    "        \n",
    "        # TODO: Global connections for first few tokens\n",
    "        # HINT: First global_tokens can attend to all positions\n",
    "        mask[:global_tokens, :] = # Your code here\n",
    "        mask[:, :global_tokens] = # Your code here\n",
    "        \n",
    "        # TODO: Local connections for all tokens\n",
    "        # HINT: Same as local_window pattern\n",
    "        for i in range(seq_len):\n",
    "            start = # Your code here\n",
    "            end = # Your code here\n",
    "            mask[i, start:end] = # Your code here\n",
    "        \n",
    "        return mask.bool()\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown pattern type: {pattern_type}\")\n",
    "\n",
    "# Test your sparse mask implementation\n",
    "print(\"🧪 Testing Sparse Mask Creation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def test_sparse_patterns():\n",
    "    \"\"\"Test all sparse attention patterns\"\"\"\n",
    "    seq_len = 64\n",
    "    patterns = ['random', 'local_window', 'strided', 'global_local']\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        try:\n",
    "            # TODO: Create mask using your implementation\n",
    "            mask = create_sparse_mask(seq_len, pattern, sparsity_factor=4)\n",
    "            \n",
    "            # Calculate sparsity statistics\n",
    "            total_connections = seq_len * seq_len\n",
    "            active_connections = mask.sum().item()\n",
    "            sparsity_ratio = active_connections / total_connections\n",
    "            \n",
    "            print(f\"✅ {pattern:12s}: {active_connections:4.0f}/{total_connections} connections \"\n",
    "                  f\"({sparsity_ratio:.3f} density)\")\n",
    "            \n",
    "            # TODO: Verify mask properties\n",
    "            # Check that mask is square and has correct dimensions\n",
    "            assert mask.shape == (seq_len, seq_len), f\"Wrong shape: {mask.shape}\"\n",
    "            assert mask.dtype == torch.bool, f\"Wrong dtype: {mask.dtype}\"\n",
    "            \n",
    "            # Pattern-specific checks\n",
    "            if pattern == 'local_window':\n",
    "                # TODO: Verify diagonal elements are True (self-attention)\n",
    "                diagonal_check = # Your code here - check mask.diag().all()\n",
    "                assert diagonal_check, \"Local window should include self-attention\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {pattern:12s}: Error - {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_sparse_patterns()\n",
    "\n",
    "print(f\"\\n🎯 Understanding Sparsity Patterns:\")\n",
    "print(\"- Random: Unstructured sparsity, good for general pruning\")\n",
    "print(\"- Local Window: Captures local dependencies, good for images\")  \n",
    "print(\"- Strided: Regular patterns, good for periodic data\")\n",
    "print(\"- Global+Local: Combines global and local attention, best balance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38909264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDS-ON EXERCISE: Implement Sparse Attention\n",
    "\n",
    "class SparseAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse Multi-Head Self-Attention with configurable sparsity patterns - COMPLETE THE IMPLEMENTATION\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=8, sparsity_pattern='random', \n",
    "                 sparsity_factor=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        # TODO: Set up basic attention parameters (similar to standard attention)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = # Your code here\n",
    "        self.scale = # Your code here\n",
    "        \n",
    "        # Store sparsity configuration\n",
    "        self.sparsity_pattern = sparsity_pattern\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        \n",
    "        # TODO: Create QKV projection and output projection layers\n",
    "        self.qkv = # Your code here\n",
    "        self.proj = # Your code here\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Register buffer for attention mask (will be set during forward)\n",
    "        self.register_buffer('attention_mask', None)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # TODO: Create sparse mask if not exists or sequence length changed\n",
    "        # HINT: Check if self.attention_mask is None or has wrong shape\n",
    "        if # Your condition here:\n",
    "            self.attention_mask = create_sparse_mask(\n",
    "                N, self.sparsity_pattern, self.sparsity_factor\n",
    "            ).to(x.device)\n",
    "        \n",
    "        # TODO: Generate Q, K, V matrices (same as standard attention)\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        # TODO: Compute attention scores\n",
    "        # HINT: Standard dot-product attention with scaling\n",
    "        attn = # Your code here\n",
    "        \n",
    "        # TODO: Apply sparse mask to attention scores\n",
    "        # HINT: 1) Add batch and head dimensions to mask, 2) Use masked_fill with negative infinity\n",
    "        sparse_mask = self.attention_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, N, N)\n",
    "        attn = # Your code here - apply mask using masked_fill\n",
    "        \n",
    "        # TODO: Apply softmax and dropout\n",
    "        attn = # Your code here - apply softmax\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        # TODO: Apply attention to values and reshape output\n",
    "        # HINT: 1) Multiply attn @ v, 2) Transpose and reshape, 3) Apply projection\n",
    "        x = # Your code here - apply attention to values\n",
    "        x = # Your code here - apply output projection\n",
    "        \n",
    "        return x, attn\n",
    "\n",
    "# Test your implementation\n",
    "print(\"Testing SparseAttention implementation...\")\n",
    "try:\n",
    "    # Test different sparsity patterns\n",
    "    patterns = ['random', 'local_window', 'strided', 'global_local']\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        test_sparse_attn = SparseAttention(\n",
    "            embed_dim=256, \n",
    "            num_heads=8, \n",
    "            sparsity_pattern=pattern,\n",
    "            sparsity_factor=4\n",
    "        )\n",
    "        test_input = torch.randn(2, 64, 256)\n",
    "        test_output, test_attn = test_sparse_attn(test_input)\n",
    "        \n",
    "        # Calculate actual sparsity\n",
    "        mask = test_sparse_attn.attention_mask\n",
    "        sparsity = mask.float().mean().item()\n",
    "        \n",
    "        print(f\"✅ {pattern} pattern works!\")\n",
    "        print(f\"   Input: {test_input.shape}, Output: {test_output.shape}\")\n",
    "        print(f\"   Attention sparsity: {sparsity:.3f} ({sparsity*100:.1f}% connections)\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error in implementation: {e}\")\n",
    "    print(\"💡 Check your TODO implementations above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8271735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigBirdAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    BigBird-style sparse attention with global, local, and random components\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads=8, window_size=64, num_global_tokens=64,\n",
    "                 num_random_tokens=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.window_size = window_size\n",
    "        self.num_global_tokens = num_global_tokens\n",
    "        self.num_random_tokens = num_random_tokens\n",
    "        \n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def create_bigbird_mask(self, seq_len):\n",
    "        \"\"\"Create BigBird attention mask\"\"\"\n",
    "        mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)\n",
    "        \n",
    "        # Global attention for first num_global_tokens\n",
    "        global_tokens = min(self.num_global_tokens, seq_len)\n",
    "        mask[:global_tokens, :] = True\n",
    "        mask[:, :global_tokens] = True\n",
    "        \n",
    "        # Local sliding window\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - self.window_size // 2)\n",
    "            end = min(seq_len, i + self.window_size // 2 + 1)\n",
    "            mask[i, start:end] = True\n",
    "        \n",
    "        # Random connections\n",
    "        for i in range(seq_len):\n",
    "            if i >= global_tokens:  # Skip global tokens\n",
    "                random_indices = torch.randperm(seq_len)[:self.num_random_tokens]\n",
    "                mask[i, random_indices] = True\n",
    "        \n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = self.create_bigbird_mask(N).to(x.device)\n",
    "        \n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Apply attention mask\n",
    "        sparse_mask = attention_mask.unsqueeze(0).unsqueeze(0)\n",
    "        attn = attn.masked_fill(~sparse_mask, float('-inf'))\n",
    "        \n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Attention Analysis Functions\n",
    "\n",
    "def analyze_sparsity_patterns():\n",
    "    \"\"\"Analyze different sparsity patterns\"\"\"\n",
    "    seq_len = 256\n",
    "    patterns = ['random', 'local_window', 'strided', 'global_local']\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, pattern in enumerate(patterns):\n",
    "        mask = create_sparse_mask(seq_len, pattern, sparsity_factor=8)\n",
    "        \n",
    "        axes[i].imshow(mask.float().numpy(), cmap='Blues', aspect='auto')\n",
    "        axes[i].set_title(f'{pattern.replace(\"_\", \" \").title()} Pattern')\n",
    "        axes[i].set_xlabel('Key Position')\n",
    "        axes[i].set_ylabel('Query Position')\n",
    "        \n",
    "        # Calculate sparsity\n",
    "        sparsity = mask.float().mean().item()\n",
    "        axes[i].text(0.02, 0.98, f'Sparsity: {sparsity:.3f}', \n",
    "                    transform=axes[i].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def measure_sparse_attention_efficiency():\n",
    "    \"\"\"Measure efficiency of different sparse attention patterns\"\"\"\n",
    "    embed_dim = 256\n",
    "    seq_len = 512\n",
    "    batch_size = 4\n",
    "    \n",
    "    x = torch.randn(batch_size, seq_len, embed_dim).to(device)\n",
    "    \n",
    "    # Standard attention baseline\n",
    "    std_attn = MultiHeadAttention(embed_dim).to(device)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.time()\n",
    "    for _ in range(30):\n",
    "        _ = std_attn(x)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    std_time = (time.time() - start_time) / 30\n",
    "    \n",
    "    # Sparse attention variants\n",
    "    patterns = ['random', 'local_window', 'strided', 'global_local']\n",
    "    sparsity_factors = [2, 4, 8, 16]\n",
    "    \n",
    "    results = defaultdict(dict)\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        for factor in sparsity_factors:\n",
    "            sparse_attn = SparseAttention(\n",
    "                embed_dim, sparsity_pattern=pattern, sparsity_factor=factor\n",
    "            ).to(device)\n",
    "            \n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            start_time = time.time()\n",
    "            for _ in range(30):\n",
    "                _ = sparse_attn(x)\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            sparse_time = (time.time() - start_time) / 30\n",
    "            \n",
    "            speedup = std_time / sparse_time\n",
    "            results[pattern][factor] = {\n",
    "                'time': sparse_time,\n",
    "                'speedup': speedup\n",
    "            }\n",
    "    \n",
    "    return std_time, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa06821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Sparse Attention Analyses\n",
    "\n",
    "print(\"Analyzing Sparsity Patterns:\")\n",
    "analyze_sparsity_patterns()\n",
    "\n",
    "print(\"\\nSparse Attention Efficiency Analysis:\")\n",
    "std_time, sparse_results = measure_sparse_attention_efficiency()\n",
    "print(f\"Standard Attention: {std_time:.4f}s\")\n",
    "\n",
    "for pattern, factor_results in sparse_results.items():\n",
    "    print(f\"\\n{pattern.replace('_', ' ').title()} Pattern:\")\n",
    "    for factor, metrics in factor_results.items():\n",
    "        print(f\"  Factor {factor}: {metrics['time']:.4f}s (Speedup: {metrics['speedup']:.2f}x)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8387f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Benchmark - Main Function\n",
    "\n",
    "def comprehensive_benchmark():\n",
    "    \"\"\"\n",
    "    Comprehensive benchmark comparing all attention mechanisms across \n",
    "    multiple dimensions: speed, memory, and approximation quality\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test configurations\n",
    "    test_configs = [\n",
    "        {'seq_len': 64, 'embed_dim': 256, 'batch_size': 8},\n",
    "        {'seq_len': 128, 'embed_dim': 256, 'batch_size': 8},\n",
    "        {'seq_len': 256, 'embed_dim': 256, 'batch_size': 4},\n",
    "        {'seq_len': 512, 'embed_dim': 256, 'batch_size': 2},\n",
    "    ]\n",
    "    \n",
    "    results = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for config in test_configs:\n",
    "        seq_len = config['seq_len']\n",
    "        embed_dim = config['embed_dim']\n",
    "        batch_size = config['batch_size']\n",
    "        \n",
    "        print(f\"Testing config: seq_len={seq_len}, embed_dim={embed_dim}, batch_size={batch_size}\")\n",
    "        \n",
    "        # Generate test input\n",
    "        x = torch.randn(batch_size, seq_len, embed_dim).to(device)\n",
    "        \n",
    "        # Standard Attention (baseline)\n",
    "        std_attn = MultiHeadAttention(embed_dim).to(device)\n",
    "        std_output, std_attn_weights = std_attn(x)\n",
    "        \n",
    "        # Measure standard attention\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start_time = time.time()\n",
    "        for _ in range(50):\n",
    "            _ = std_attn(x)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        std_time = (time.time() - start_time) / 50\n",
    "        \n",
    "        # Memory usage (approximate)\n",
    "        std_memory = seq_len * seq_len * batch_size * 4  # 4 bytes per float32\n",
    "        \n",
    "        # Store baseline results\n",
    "        results['Standard'][seq_len] = {\n",
    "            'time': std_time,\n",
    "            'memory': std_memory,\n",
    "            'speedup': 1.0,\n",
    "            'memory_reduction': 1.0,\n",
    "            'approximation_error': 0.0\n",
    "        }\n",
    "        \n",
    "        # Test other attention mechanisms\n",
    "        results = test_window_attention(results, x, std_output, std_time, std_memory, seq_len, embed_dim, batch_size)\n",
    "        results = test_linear_attention(results, x, std_output, std_time, std_memory, seq_len, embed_dim, batch_size)\n",
    "        results = test_sparse_attention(results, x, std_output, std_time, std_memory, seq_len, embed_dim, batch_size)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ae111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HANDS-ON EXERCISE: Compare All Attention Mechanisms\n",
    "\n",
    "def compare_attention_mechanisms_exercise():\n",
    "    \"\"\"\n",
    "    EXERCISE: Complete this function to compare all attention mechanisms\n",
    "    \"\"\"\n",
    "    print(\"🔬 HANDS-ON COMPARISON: Attention Mechanisms\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test configuration\n",
    "    batch_size, seq_len, embed_dim = 4, 128, 256\n",
    "    num_heads = 8\n",
    "    \n",
    "    # TODO: Create test input tensor\n",
    "    # HINT: Use torch.randn with the dimensions above\n",
    "    test_input = # Your code here\n",
    "    \n",
    "    print(f\"Test input shape: {test_input.shape}\")\n",
    "    print(f\"Sequence length: {seq_len}, Embed dim: {embed_dim}\")\n",
    "    print()\n",
    "    \n",
    "    # TODO: Initialize all attention mechanisms\n",
    "    print(\"🏗️  Initializing attention mechanisms...\")\n",
    "    \n",
    "    # Standard attention (baseline)\n",
    "    std_attn = # Your code here - use MultiHeadAttention\n",
    "    \n",
    "    # Window attention\n",
    "    window_attn = # Your code here - use SimpleWindowAttention with window_size=16\n",
    "    \n",
    "    # Linear attention  \n",
    "    linear_attn = # Your code here - use LinearAttention with kernel_type='elu'\n",
    "    \n",
    "    # Sparse attention\n",
    "    sparse_attn = # Your code here - use SparseAttention with sparsity_pattern='global_local'\n",
    "    \n",
    "    attention_models = {\n",
    "        'Standard': std_attn,\n",
    "        'Window': window_attn, \n",
    "        'Linear': linear_attn,\n",
    "        'Sparse': sparse_attn\n",
    "    }\n",
    "    \n",
    "    # TODO: Test each attention mechanism\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in attention_models.items():\n",
    "        print(f\"Testing {name} Attention...\")\n",
    "        \n",
    "        try:\n",
    "            # TODO: Measure execution time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Run the model multiple times for accurate timing\n",
    "            for _ in range(10):\n",
    "                if name == 'Standard':\n",
    "                    # TODO: Handle standard attention (returns output and attention weights)\n",
    "                    output, attn_weights = # Your code here\n",
    "                else:\n",
    "                    # TODO: Handle other attention types\n",
    "                    if name == 'Sparse':\n",
    "                        # Sparse attention returns output and attention\n",
    "                        output, _ = # Your code here\n",
    "                    else:\n",
    "                        # Window and Linear attention return only output\n",
    "                        output = # Your code here\n",
    "            \n",
    "            end_time = time.time()\n",
    "            exec_time = (end_time - start_time) / 10  # Average time\n",
    "            \n",
    "            # TODO: Calculate approximate memory usage\n",
    "            if name == 'Standard':\n",
    "                # Standard attention: O(n²) memory\n",
    "                memory_usage = # Your code here - seq_len * seq_len\n",
    "            elif name == 'Window':\n",
    "                # Window attention: O(n * w) memory where w is window size\n",
    "                memory_usage = # Your code here - seq_len * 16 (window size)\n",
    "            elif name == 'Linear':\n",
    "                # Linear attention: O(n) memory\n",
    "                memory_usage = # Your code here - seq_len\n",
    "            elif name == 'Sparse':\n",
    "                # Sparse attention: reduced quadratic\n",
    "                sparsity = sparse_attn.attention_mask.float().mean().item()\n",
    "                memory_usage = # Your code here - seq_len * seq_len * sparsity\n",
    "            \n",
    "            # Store results\n",
    "            results[name] = {\n",
    "                'time': exec_time,\n",
    "                'memory': memory_usage,\n",
    "                'output_shape': output.shape\n",
    "            }\n",
    "            \n",
    "            print(f\"  ✅ Success! Time: {exec_time:.4f}s, Memory: ~{memory_usage:.0f} units\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error: {e}\")\n",
    "            results[name] = {'error': str(e)}\n",
    "    \n",
    "    # TODO: Calculate and display speedups\n",
    "    print(f\"\\n📊 PERFORMANCE COMPARISON:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if 'Standard' in results and 'time' in results['Standard']:\n",
    "        baseline_time = results['Standard']['time']\n",
    "        baseline_memory = results['Standard']['memory']\n",
    "        \n",
    "        for name, result in results.items():\n",
    "            if 'time' in result:\n",
    "                # TODO: Calculate speedup and memory reduction\n",
    "                speedup = # Your code here - baseline_time / result['time']\n",
    "                memory_reduction = # Your code here - baseline_memory / result['memory']\n",
    "                \n",
    "                print(f\"{name:10s}: {result['time']:.4f}s | \"\n",
    "                      f\"Speedup: {speedup:.2f}x | \"\n",
    "                      f\"Memory: {memory_reduction:.2f}x less\")\n",
    "    \n",
    "    print(\"\\n🎯 Key Insights:\")\n",
    "    print(\"- Window Attention: Best for images with spatial locality\")\n",
    "    print(\"- Linear Attention: Best scaling for very long sequences\") \n",
    "    print(\"- Sparse Attention: Balanced trade-off between efficiency and global context\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comparison exercise\n",
    "print(\"🚀 Running Attention Mechanisms Comparison Exercise...\")\n",
    "print(\"Complete the TODO items above, then run this cell!\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    comparison_results = compare_attention_mechanisms_exercise()\n",
    "    print(f\"\\n🎉 Exercise completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Please complete the TODO items above. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c260d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Helper Functions\n",
    "\n",
    "def test_window_attention(results, x, std_output, std_time, std_memory, seq_len, embed_dim, batch_size):\n",
    "    \"\"\"Test window attention performance\"\"\"\n",
    "    if seq_len >= 8:  # Minimum size for window attention\n",
    "        window_size = min(8, seq_len // 2)\n",
    "        window_size = max(1, min(window_size, seq_len))\n",
    "        \n",
    "        window_attn = SimpleWindowAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            window_size=window_size,\n",
    "            num_heads=8\n",
    "        ).to(device)\n",
    "        \n",
    "        window_output = window_attn(x)\n",
    "        \n",
    "        # Measure time\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        start_time = time.time()\n",
    "        for _ in range(50):\n",
    "            _ = window_attn(x)\n",
    "        torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "        window_time = (time.time() - start_time) / 50\n",
    "        \n",
    "        # Approximation quality\n",
    "        approx_error = F.mse_loss(window_output, std_output).item()\n",
    "        \n",
    "        # Memory (approximately)\n",
    "        num_windows = (seq_len + window_size - 1) // window_size\n",
    "        window_memory = window_size * window_size * num_windows * batch_size * 4\n",
    "        \n",
    "        results['Window'][seq_len] = {\n",
    "            'time': window_time,\n",
    "            'memory': window_memory,\n",
    "            'speedup': std_time / window_time,\n",
    "            'memory_reduction': std_memory / window_memory,\n",
    "            'approximation_error': approx_error\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_linear_attention(results, x, std_output, std_time, std_memory, seq_len, embed_dim, batch_size):\n",
    "    \"\"\"Test linear attention performance\"\"\"\n",
    "    linear_attn = LinearAttention(embed_dim, kernel_type='elu').to(device)\n",
    "    linear_output = linear_attn(x)\n",
    "    \n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.time()\n",
    "    for _ in range(50):\n",
    "        _ = linear_attn(x)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    linear_time = (time.time() - start_time) / 50\n",
    "    \n",
    "    approx_error = F.mse_loss(linear_output, std_output).item()\n",
    "    linear_memory = seq_len * embed_dim * batch_size * 4  # Linear in sequence length\n",
    "    \n",
    "    results['Linear'][seq_len] = {\n",
    "        'time': linear_time,\n",
    "        'memory': linear_memory,\n",
    "        'speedup': std_time / linear_time,\n",
    "        'memory_reduction': std_memory / linear_memory,\n",
    "        'approximation_error': approx_error\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_sparse_attention(results, x, std_output, std_time, std_memory, seq_len, embed_dim, batch_size):\n",
    "    \"\"\"Test sparse attention performance\"\"\"\n",
    "    sparse_attn = SparseAttention(embed_dim, sparsity_pattern='global_local', sparsity_factor=4).to(device)\n",
    "    sparse_output, _ = sparse_attn(x)\n",
    "    \n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    start_time = time.time()\n",
    "    for _ in range(50):\n",
    "        _ = sparse_attn(x)\n",
    "    torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "    sparse_time = (time.time() - start_time) / 50\n",
    "    \n",
    "    approx_error = F.mse_loss(sparse_output, std_output).item()\n",
    "    sparse_memory = (seq_len * seq_len // 4) * batch_size * 4  # Reduced by sparsity factor\n",
    "    \n",
    "    results['Sparse'][seq_len] = {\n",
    "        'time': sparse_time,\n",
    "        'memory': sparse_memory,\n",
    "        'speedup': std_time / sparse_time,\n",
    "        'memory_reduction': std_memory / sparse_memory,\n",
    "        'approximation_error': approx_error\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e847219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complexity Scaling Analysis\n",
    "\n",
    "def analyze_complexity_scaling():\n",
    "    \"\"\"Analyze how each method scales with sequence length\"\"\"\n",
    "    sequence_lengths = [64, 128, 256, 512]\n",
    "    embed_dim = 256\n",
    "    \n",
    "    methods = {\n",
    "        'Standard': lambda: MultiHeadAttention(embed_dim),\n",
    "        'Linear': lambda: LinearAttention(embed_dim, kernel_type='elu'),\n",
    "        'Sparse': lambda: SparseAttention(embed_dim, sparsity_pattern='random', sparsity_factor=4),\n",
    "        'Window': lambda: SimpleWindowAttention(embed_dim, window_size=8)\n",
    "    }\n",
    "    \n",
    "    scaling_results = defaultdict(list)\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        x = torch.randn(4, seq_len, embed_dim).to(device)\n",
    "        \n",
    "        for method_name, method_factory in methods.items():\n",
    "            model = method_factory().to(device)\n",
    "            \n",
    "            # Measure time\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            start_time = time.time()\n",
    "            for _ in range(30):\n",
    "                _ = model(x)\n",
    "            torch.cuda.synchronize() if torch.cuda.is_available() else None\n",
    "            avg_time = (time.time() - start_time) / 30\n",
    "            \n",
    "            scaling_results[method_name].append((seq_len, avg_time))\n",
    "    \n",
    "    return scaling_results\n",
    "\n",
    "def theoretical_memory_complexity():\n",
    "    \"\"\"Compare theoretical memory complexities\"\"\"\n",
    "    sequence_lengths = np.array([64, 128, 256, 512, 1024])\n",
    "    \n",
    "    # Theoretical complexities (normalized)\n",
    "    standard_memory = sequence_lengths ** 2\n",
    "    linear_memory = sequence_lengths\n",
    "    window_memory = sequence_lengths  # Assuming fixed window size\n",
    "    sparse_memory = sequence_lengths ** 2 / 4  # 4x sparsity\n",
    "    \n",
    "    return {\n",
    "        'sequence_lengths': sequence_lengths,\n",
    "        'Standard': standard_memory,\n",
    "        'Linear': linear_memory,\n",
    "        'Window': window_memory,\n",
    "        'Sparse': sparse_memory\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476cbbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Comprehensive Benchmark\n",
    "\n",
    "print(\"Running Comprehensive Benchmark...\")\n",
    "benchmark_results = comprehensive_benchmark()\n",
    "\n",
    "print(\"\\nBenchmark Results Summary:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for method_name, seq_results in benchmark_results.items():\n",
    "    print(f\"\\n{method_name} Attention:\")\n",
    "    print(\"-\" * 40)\n",
    "    for seq_len, metrics in seq_results.items():\n",
    "        print(f\"  Seq Len {seq_len:3d}: Time={metrics['time']:.4f}s, \"\n",
    "              f\"Speedup={metrics['speedup']:.2f}x, \"\n",
    "              f\"Memory Reduction={metrics['memory_reduction']:.2f}x, \"\n",
    "              f\"Error={metrics['approximation_error']:.6f}\")\n",
    "\n",
    "# Analyze scaling behavior\n",
    "print(\"\\nAnalyzing Complexity Scaling...\")\n",
    "scaling_data = analyze_complexity_scaling()\n",
    "\n",
    "print(\"\\nComplexity Scaling Results:\")\n",
    "print(\"=\" * 50)\n",
    "for method_name, timing_data in scaling_data.items():\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    for seq_len, exec_time in timing_data:\n",
    "        print(f\"  Length {seq_len:3d}: {exec_time:.4f}s\")\n",
    "\n",
    "# Memory complexity analysis\n",
    "memory_analysis = theoretical_memory_complexity()\n",
    "print(\"\\nTheoretical Memory Complexity (normalized):\")\n",
    "print(\"=\" * 60)\n",
    "for i, seq_len in enumerate(memory_analysis['sequence_lengths']):\n",
    "    print(f\"Seq Len {seq_len:4d}: Standard={memory_analysis['Standard'][i]:8.0f}, \"\n",
    "          f\"Linear={memory_analysis['Linear'][i]:6.0f}, \"\n",
    "          f\"Window={memory_analysis['Window'][i]:6.0f}, \"\n",
    "          f\"Sparse={memory_analysis['Sparse'][i]:8.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Visualization Functions - Part 1\n",
    "\n",
    "def create_performance_visualizations():\n",
    "    \"\"\"Create comprehensive performance visualizations\"\"\"\n",
    "    \n",
    "    # Generate sample data for visualization (replace with actual benchmark results if available)\n",
    "    sequence_lengths = np.array([64, 128, 256, 512])\n",
    "    \n",
    "    # Execution times (in milliseconds)\n",
    "    execution_times = {\n",
    "        'Standard': np.array([5.2, 18.7, 72.3, 285.1]),\n",
    "        'Window': np.array([3.1, 8.9, 23.4, 67.8]),\n",
    "        'Linear': np.array([2.8, 5.6, 11.2, 22.4]),\n",
    "        'Sparse': np.array([3.5, 10.2, 28.7, 89.3])\n",
    "    }\n",
    "    \n",
    "    # Memory usage (in MB)\n",
    "    memory_usage = {\n",
    "        'Standard': sequence_lengths ** 2 * 0.001,  # Quadratic scaling\n",
    "        'Window': sequence_lengths * 0.02,          # Linear scaling\n",
    "        'Linear': sequence_lengths * 0.015,         # Linear scaling\n",
    "        'Sparse': sequence_lengths ** 2 * 0.00025   # Reduced quadratic\n",
    "    }\n",
    "    \n",
    "    # Approximation errors\n",
    "    approximation_errors = {\n",
    "        'Standard': np.zeros_like(sequence_lengths, dtype=float),\n",
    "        'Window': np.array([0.001, 0.002, 0.003, 0.004]),\n",
    "        'Linear': np.array([0.05, 0.06, 0.07, 0.08]),\n",
    "        'Sparse': np.array([0.01, 0.015, 0.02, 0.025])\n",
    "    }\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # 1. Execution Time Comparison\n",
    "    ax1 = plt.subplot(3, 3, 1)\n",
    "    for method, times in execution_times.items():\n",
    "        plt.plot(sequence_lengths, times, 'o-', label=method, linewidth=2, markersize=8)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Execution Time (ms)')\n",
    "    plt.title('Execution Time vs Sequence Length')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # 2. Memory Usage Comparison\n",
    "    ax2 = plt.subplot(3, 3, 2)\n",
    "    for method, memory in memory_usage.items():\n",
    "        plt.plot(sequence_lengths, memory, 's-', label=method, linewidth=2, markersize=8)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.title('Memory Usage vs Sequence Length')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # 3. Speedup Comparison\n",
    "    ax3 = plt.subplot(3, 3, 3)\n",
    "    for method, times in execution_times.items():\n",
    "        if method != 'Standard':\n",
    "            speedup = execution_times['Standard'] / times\n",
    "            plt.plot(sequence_lengths, speedup, '^-', label=method, linewidth=2, markersize=8)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Speedup (vs Standard)')\n",
    "    plt.title('Speedup vs Sequence Length')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=1, color='k', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    return fig, execution_times, memory_usage, approximation_errors, sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Visualization Functions - Part 2\n",
    "\n",
    "def create_additional_visualizations(fig, execution_times, memory_usage, approximation_errors, sequence_lengths):\n",
    "    \"\"\"Add remaining visualizations to the figure\"\"\"\n",
    "    \n",
    "    # 4. Approximation Error\n",
    "    ax4 = plt.subplot(3, 3, 4)\n",
    "    for method, errors in approximation_errors.items():\n",
    "        if method != 'Standard':\n",
    "            plt.plot(sequence_lengths, errors, 'd-', label=method, linewidth=2, markersize=8)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('MSE Error vs Standard')\n",
    "    plt.title('Approximation Error')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # 5. Efficiency vs Accuracy Trade-off\n",
    "    ax5 = plt.subplot(3, 3, 5)\n",
    "    for i, seq_len in enumerate([128, 256, 512]):\n",
    "        idx = np.where(sequence_lengths == seq_len)[0][0]\n",
    "        for method in ['Window', 'Linear', 'Sparse']:\n",
    "            speedup = execution_times['Standard'][idx] / execution_times[method][idx]\n",
    "            error = approximation_errors[method][idx]\n",
    "            plt.scatter(error, speedup, s=100, label=f'{method} (L={seq_len})', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Approximation Error (MSE)')\n",
    "    plt.ylabel('Speedup')\n",
    "    plt.title('Efficiency vs Accuracy Trade-off')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xscale('log')\n",
    "    \n",
    "    # 6. Theoretical Complexity Visualization\n",
    "    ax6 = plt.subplot(3, 3, 6)\n",
    "    extended_lengths = np.logspace(1, 4, 100)  # 10 to 10000\n",
    "    \n",
    "    # Theoretical time complexities (normalized)\n",
    "    standard_complexity = extended_lengths ** 2\n",
    "    linear_complexity = extended_lengths\n",
    "    window_complexity = extended_lengths  # Assuming fixed window\n",
    "    sparse_complexity = extended_lengths ** 2 / 4\n",
    "    \n",
    "    plt.loglog(extended_lengths, standard_complexity, label='O(n²) - Standard', linewidth=2)\n",
    "    plt.loglog(extended_lengths, linear_complexity, label='O(n) - Linear/Window', linewidth=2)\n",
    "    plt.loglog(extended_lengths, sparse_complexity, label='O(n²/s) - Sparse', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Sequence Length (n)')\n",
    "    plt.ylabel('Computational Complexity')\n",
    "    plt.title('Theoretical Complexity Scaling')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Visualization Functions - Part 3\n",
    "\n",
    "def create_final_visualizations(fig, execution_times, memory_usage, approximation_errors, sequence_lengths):\n",
    "    \"\"\"Add the final set of visualizations\"\"\"\n",
    "    \n",
    "    # 7. Attention Pattern Visualization\n",
    "    ax7 = plt.subplot(3, 3, 7)\n",
    "    seq_len = 64\n",
    "    patterns = {\n",
    "        'Standard': torch.ones(seq_len, seq_len),\n",
    "        'Window': create_sparse_mask(seq_len, 'local_window', window_size=8),\n",
    "        'Sparse': create_sparse_mask(seq_len, 'global_local', sparsity_factor=4)\n",
    "    }\n",
    "    \n",
    "    # Create composite visualization\n",
    "    composite = torch.zeros(seq_len, seq_len * 3)\n",
    "    for i, (name, pattern) in enumerate(patterns.items()):\n",
    "        start_col = i * seq_len\n",
    "        end_col = (i + 1) * seq_len\n",
    "        composite[:, start_col:end_col] = pattern.float()\n",
    "    \n",
    "    plt.imshow(composite.numpy(), cmap='Blues', aspect='auto')\n",
    "    plt.title('Attention Patterns Comparison')\n",
    "    plt.ylabel('Query Position')\n",
    "    \n",
    "    # Add labels\n",
    "    plt.xticks([seq_len//2, seq_len + seq_len//2, 2*seq_len + seq_len//2], \n",
    "               ['Standard', 'Window', 'Sparse'])\n",
    "    \n",
    "    # 8. Memory Efficiency Bar Chart\n",
    "    ax8 = plt.subplot(3, 3, 8)\n",
    "    methods = ['Standard', 'Window', 'Linear', 'Sparse']\n",
    "    seq_len_idx = 2  # 256 tokens\n",
    "    memory_values = [memory_usage[method][seq_len_idx] for method in methods]\n",
    "    colors = ['red', 'green', 'blue', 'orange']\n",
    "    \n",
    "    bars = plt.bar(methods, memory_values, color=colors, alpha=0.7)\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "    plt.title(f'Memory Usage Comparison\\n(Sequence Length: {sequence_lengths[seq_len_idx]})')\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, memory_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                f'{value:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 9. Comprehensive Performance Radar Chart\n",
    "    ax9 = plt.subplot(3, 3, 9, projection='polar')\n",
    "    \n",
    "    # Metrics for radar chart (normalized 0-1, higher is better)\n",
    "    metrics = ['Speed', 'Memory\\nEfficiency', 'Accuracy', 'Scalability', 'Implementation\\nComplexity']\n",
    "    \n",
    "    # Sample scores (0-1, higher is better)\n",
    "    scores = {\n",
    "        'Window': [0.8, 0.9, 0.85, 0.9, 0.7],\n",
    "        'Linear': [0.9, 0.95, 0.6, 0.95, 0.8],\n",
    "        'Sparse': [0.7, 0.8, 0.8, 0.8, 0.6]\n",
    "    }\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False)\n",
    "    angles = np.concatenate((angles, [angles[0]]))  # Complete the circle\n",
    "    \n",
    "    for method, values in scores.items():\n",
    "        values_plot = np.concatenate((values, [values[0]]))  # Complete the circle\n",
    "        ax9.plot(angles, values_plot, 'o-', linewidth=2, label=method)\n",
    "        ax9.fill(angles, values_plot, alpha=0.25)\n",
    "    \n",
    "    ax9.set_xticks(angles[:-1])\n",
    "    ax9.set_xticklabels(metrics)\n",
    "    ax9.set_ylim(0, 1)\n",
    "    ax9.set_title('Comprehensive Performance Comparison')\n",
    "    ax9.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax9.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e69aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Summary Functions\n",
    "\n",
    "def create_detailed_analysis_summary():\n",
    "    \"\"\"Create a detailed analysis summary\"\"\"\n",
    "    \n",
    "    print(\"COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n1. COMPUTATIONAL COMPLEXITY ANALYSIS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Standard Attention:     O(n²·d) - Quadratic scaling\")\n",
    "    print(\"Window Attention:       O(n·W²·d) ≈ O(n·d) - Linear scaling\")\n",
    "    print(\"Linear Attention:       O(n·d·d_φ) ≈ O(n·d) - Linear scaling\")\n",
    "    print(\"Sparse Attention:       O(s·n·d) where s << n - Reduced quadratic\")\n",
    "    \n",
    "    print(\"\\n2. TRADE-OFF ANALYSIS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Window Attention:\")\n",
    "    print(\"  ✓ Excellent speed improvement (5-10x)\")\n",
    "    print(\"  ✓ Maintains local spatial relationships\")\n",
    "    print(\"  ✓ Low approximation error\")\n",
    "    print(\"  ✗ Limited global context\")\n",
    "    print(\"  ✗ Image-specific (requires 2D structure)\")\n",
    "    \n",
    "    print(\"\\nLinear Attention:\")\n",
    "    print(\"  ✓ Best scalability (truly linear)\")\n",
    "    print(\"  ✓ Maintains global receptive field\")\n",
    "    print(\"  ✓ Hardware efficient\")\n",
    "    print(\"  ✗ Higher approximation error\")\n",
    "    print(\"  ✗ Kernel choice affects performance\")\n",
    "    \n",
    "    print(\"\\nSparse Attention:\")\n",
    "    print(\"  ✓ Configurable sparsity patterns\")\n",
    "    print(\"  ✓ Good balance of efficiency and accuracy\")\n",
    "    print(\"  ✓ Maintains some global connections\")\n",
    "    print(\"  ✗ Pattern design complexity\")\n",
    "    print(\"  ✗ Implementation complexity\")\n",
    "    \n",
    "    print(\"\\n3. PRACTICAL RECOMMENDATIONS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"• For image tasks (ViT): Window Attention\")\n",
    "    print(\"• For very long sequences: Linear Attention\")\n",
    "    print(\"• For balanced performance: Sparse Attention with global+local pattern\")\n",
    "    print(\"• For mobile/edge deployment: Linear or Window Attention\")\n",
    "    \n",
    "    print(\"\\n4. FUTURE DIRECTIONS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"• Learnable sparse patterns\")\n",
    "    print(\"• Hybrid attention mechanisms\")\n",
    "    print(\"• Hardware-specific optimizations\")\n",
    "    print(\"• Dynamic attention routing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac979af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Visualizations and Analysis\n",
    "\n",
    "print(\"Creating Comprehensive Performance Visualizations...\")\n",
    "fig, execution_times, memory_usage, approximation_errors, sequence_lengths = create_performance_visualizations()\n",
    "\n",
    "# Add remaining visualizations\n",
    "fig = create_additional_visualizations(fig, execution_times, memory_usage, approximation_errors, sequence_lengths)\n",
    "fig = create_final_visualizations(fig, execution_times, memory_usage, approximation_errors, sequence_lengths)\n",
    "\n",
    "# Generate analysis summary\n",
    "create_detailed_analysis_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
