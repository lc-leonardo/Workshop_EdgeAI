{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a28b3b",
   "metadata": {},
   "source": [
    "# Case Study: Edge AI and Cybersecurity in Action\n",
    "## Module 1: Network Traffic Analysis - **HANDS-ON VERSION**\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Welcome to the first module of our Edge AI and Cybersecurity workshop! In this notebook, we'll explore and analyze real-world network traffic data to understand the characteristics of both benign and malicious network flows.\n",
    "\n",
    "**Objectives:**\n",
    "- Explore labeled network traffic data from the CICIDS2017 dataset\n",
    "- Understand the key features that distinguish normal traffic from cyber attacks\n",
    "- Preprocess the data for machine learning models\n",
    "- Prepare a clean dataset for edge-based anomaly detection in future modules\n",
    "\n",
    "**Why Network Traffic Analysis?**\n",
    "Network traffic analysis is fundamental to cybersecurity. By understanding patterns in network flows, we can:\n",
    "- Detect anomalous behavior that might indicate attacks\n",
    "- Build lightweight ML models suitable for edge devices\n",
    "- Create real-time intrusion detection systems\n",
    "\n",
    "**Dataset Source:**\n",
    "We'll be working with a preprocessed subset of the CICIDS2017 dataset, which contains labeled network flows including various types of attacks (DDoS, Port Scan, Brute Force, etc.) alongside normal traffic patterns.\n",
    "\n",
    "---\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: This notebook contains code completion exercises marked with `# TODO:` comments. Fill in the missing code to complete the network traffic analysis workflow!\n",
    "\n",
    "Let's begin our journey into cybersecurity data analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Visualization settings configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample network traffic dataset (simulating CICIDS2017-like data)\n",
    "# In a real scenario, you would load from 'data/clean_traffic.csv'\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 8000\n",
    "\n",
    "# Generate synthetic network traffic features\n",
    "data = {\n",
    "    'Flow Duration': np.random.exponential(1000000, n_samples),\n",
    "    'Total Fwd Packets': np.random.poisson(50, n_samples),\n",
    "    'Total Bwd Packets': np.random.poisson(30, n_samples),\n",
    "    'Total Length of Fwd Packets': np.random.exponential(2000, n_samples),\n",
    "    'Total Length of Bwd Packets': np.random.exponential(1500, n_samples),\n",
    "    'Fwd Packet Length Max': np.random.exponential(500, n_samples),\n",
    "    'Fwd Packet Length Min': np.random.exponential(50, n_samples),\n",
    "    'Fwd Packet Length Mean': np.random.exponential(200, n_samples),\n",
    "    'Bwd Packet Length Max': np.random.exponential(400, n_samples),\n",
    "    'Bwd Packet Length Min': np.random.exponential(40, n_samples),\n",
    "    'Bwd Packet Length Mean': np.random.exponential(150, n_samples),\n",
    "    'Flow Bytes/s': np.random.exponential(10000, n_samples),\n",
    "    'Flow Packets/s': np.random.exponential(100, n_samples),\n",
    "    'Flow IAT Mean': np.random.exponential(50000, n_samples),\n",
    "    'Flow IAT Max': np.random.exponential(100000, n_samples),\n",
    "    'Flow IAT Min': np.random.exponential(1000, n_samples),\n",
    "    'Fwd IAT Mean': np.random.exponential(40000, n_samples),\n",
    "    'Bwd IAT Mean': np.random.exponential(60000, n_samples),\n",
    "    'Fwd PSH Flags': np.random.binomial(1, 0.3, n_samples),\n",
    "    'Bwd PSH Flags': np.random.binomial(1, 0.2, n_samples),\n",
    "    'Fwd URG Flags': np.random.binomial(1, 0.05, n_samples),\n",
    "    'Bwd URG Flags': np.random.binomial(1, 0.03, n_samples),\n",
    "    'Fwd Header Length': np.random.normal(20, 5, n_samples),\n",
    "    'Bwd Header Length': np.random.normal(20, 5, n_samples),\n",
    "    'Fwd Packets/s': np.random.exponential(50, n_samples),\n",
    "    'Bwd Packets/s': np.random.exponential(30, n_samples),\n",
    "    'Packet Length Min': np.random.exponential(40, n_samples),\n",
    "    'Packet Length Max': np.random.exponential(500, n_samples),\n",
    "    'Packet Length Mean': np.random.exponential(200, n_samples),\n",
    "    'Packet Length Std': np.random.exponential(100, n_samples),\n",
    "    'Packet Length Variance': np.random.exponential(10000, n_samples),\n",
    "}\n",
    "\n",
    "# Create labels (20% attacks, 80% benign)\n",
    "attack_indices = np.random.choice(n_samples, size=int(0.2 * n_samples), replace=False)\n",
    "labels = ['BENIGN'] * n_samples\n",
    "attack_types = ['DDoS', 'PortScan', 'FTP-Patator', 'SSH-Patator', 'DoS Hulk']\n",
    "\n",
    "for idx in attack_indices:\n",
    "    labels[idx] = np.random.choice(attack_types)\n",
    "    # Make attack traffic slightly different\n",
    "    data['Flow Duration'][idx] *= 0.5  # Shorter duration\n",
    "    data['Total Fwd Packets'][idx] *= 2  # More packets\n",
    "    data['Flow Bytes/s'][idx] *= 3  # Higher throughput\n",
    "\n",
    "data['Label'] = labels\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Generated synthetic network traffic dataset:\")\n",
    "print(f\"   â€¢ Total samples: {len(df):,}\")\n",
    "print(f\"   â€¢ Features: {len(df.columns)-1}\")\n",
    "print(f\"   â€¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e143fba5",
   "metadata": {},
   "source": [
    "## Step 1: Initial Data Exploration\n",
    "\n",
    "Let's examine our network traffic dataset to understand its structure and characteristics. Each row represents a network flow (a sequence of packets between two endpoints), and each column represents a feature extracted from that flow.\n",
    "\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: Complete the missing code sections marked with `# TODO:` to explore the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the basic dataset information display\n",
    "# HINT: Use df.shape to get dimensions, df.columns to get column names, and df.info() for detailed info\n",
    "\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"=\" * 50)\n",
    "# TODO: Print the shape of the dataset using f-string formatting\n",
    "print(f\"Shape: {}\")\n",
    "# TODO: Print the list of features (all columns except 'Label')\n",
    "print(f\"Features: {}\")\n",
    "# TODO: Print the target column name (the last column)\n",
    "print(f\"Target: {}\")\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "# TODO: Display detailed dataset information using the info() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display sample data and feature descriptions\n",
    "# HINT: Use display() for nice table formatting and create a dictionary for feature descriptions\n",
    "\n",
    "print(\"\\nSample Data (First 5 rows):\")\n",
    "print(\"=\" * 80)\n",
    "# TODO: Display the first 5 rows of the dataset using head() method\n",
    "display()\n",
    "\n",
    "print(\"\\nFeature Descriptions:\")\n",
    "print(\"=\" * 50)\n",
    "feature_descriptions = {\n",
    "    'Flow Duration': 'Duration of the flow in microseconds',\n",
    "    'Total Fwd Packets': 'Total packets in forward direction',\n",
    "    'Total Bwd Packets': 'Total packets in backward direction',\n",
    "    'Total Length of Fwd Packets': 'Total size of packet in forward direction',\n",
    "    'Total Length of Bwd Packets': 'Total size of packet in backward direction',\n",
    "    'Flow Bytes/s': 'Number of flow bytes per second',\n",
    "    'Flow Packets/s': 'Number of flow packets per second',\n",
    "    'Flow IAT Mean': 'Mean time between two packets sent in the flow',\n",
    "    'Fwd PSH Flags': 'Number of times PSH flag was set in forward direction',\n",
    "    'Bwd PSH Flags': 'Number of times PSH flag was set in backward direction',\n",
    "    'Label': 'Traffic type (BENIGN or attack type)'\n",
    "}\n",
    "\n",
    "# TODO: Display the first 6 feature descriptions using a loop\n",
    "for feature, description in list(feature_descriptions.items())[:6]:\n",
    "    print(f\"â€¢ {feature:25}: {description}\")\n",
    "print(\"  ... and more network flow features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7639963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze class distribution and create visualizations\n",
    "# HINT: Use value_counts() for counting, normalize=True for percentages\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "# TODO: Count the occurrences of each label using value_counts()\n",
    "label_counts = \n",
    "print(label_counts)\n",
    "print(f\"\\nTotal samples: {len(df):,}\")\n",
    "\n",
    "# TODO: Calculate percentages using value_counts with normalize=True\n",
    "label_percentages = \n",
    "print(\"\\nPercentage Distribution:\")\n",
    "for label, percentage in label_percentages.items():\n",
    "    print(f\"â€¢ {label:15}: {percentage:.1f}%\")\n",
    "\n",
    "# TODO: Create visualizations for class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# TODO: Create a bar plot using label_counts.plot()\n",
    "# HINT: Use kind='bar', specify colors and edge colors\n",
    "label_counts.plot(kind=, ax=ax1, color=, edgecolor=)\n",
    "ax1.set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Traffic Type')\n",
    "ax1.set_ylabel('Number of Samples')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# TODO: Create a pie chart using ax2.pie()\n",
    "# HINT: Use label_counts.values and label_counts.index for data and labels\n",
    "ax2.pie(, labels=, autopct='%1.1f%%', \n",
    "        colors=plt.cm.Set3.colors, startangle=90)\n",
    "ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Calculate class imbalance ratio\n",
    "benign_count = label_counts['BENIGN']\n",
    "attack_count = label_counts.sum() - benign_count\n",
    "# TODO: Calculate the ratio of benign to attack samples\n",
    "imbalance_ratio = \n",
    "print(f\"\\nClass Imbalance Ratio (Benign:Attack): {imbalance_ratio:.2f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create statistical summary and visualizations for key features\n",
    "# HINT: Use describe() method and create comparison plots between benign and attack traffic\n",
    "\n",
    "print(\"Statistical Summary of Key Features:\")\n",
    "print(\"=\" * 60)\n",
    "# TODO: Define a list of key features to analyze\n",
    "key_features = ['Flow Duration', 'Total Fwd Packets', 'Total Bwd Packets', \n",
    "                'Flow Bytes/s', 'Flow Packets/s']\n",
    "# TODO: Display statistical summary using describe() method\n",
    "display()\n",
    "\n",
    "# TODO: Create histograms comparing benign vs attack traffic\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    # TODO: Filter data for benign and attack traffic\n",
    "    # HINT: Use boolean indexing with df['Label'] == 'BENIGN' and df['Label'] != 'BENIGN'\n",
    "    benign_data = \n",
    "    attack_data = \n",
    "    \n",
    "    # TODO: Create overlapping histograms\n",
    "    # HINT: Use axes[i].hist() with alpha=0.7, different colors, and density=True\n",
    "    axes[i].hist(, bins=50, alpha=0.7, label='Benign', color='lightblue', density=True)\n",
    "    axes[i].hist(, bins=50, alpha=0.7, label='Attack', color='salmon', density=True)\n",
    "    axes[i].set_title(f'Distribution: {feature}', fontweight='bold')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].remove()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Create box plots for detailed comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    # TODO: Create a copy of dataframe and add binary traffic type column\n",
    "    df_viz = df.copy()\n",
    "    # TODO: Create 'Traffic_Type' column using lambda function\n",
    "    # HINT: Use apply() with lambda x: 'Benign' if x == 'BENIGN' else 'Attack'\n",
    "    df_viz['Traffic_Type'] = \n",
    "    \n",
    "    # TODO: Create box plot using seaborn\n",
    "    # HINT: Use sns.boxplot() with data=df_viz, x='Traffic_Type', y=feature\n",
    "    sns.boxplot()\n",
    "    axes[i].set_title(f'Box Plot: {feature}', fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[-1].remove()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0fe54",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Feature Selection\n",
    "\n",
    "Now let's prepare our data for machine learning by:\n",
    "1. Handling missing or infinite values\n",
    "2. Removing highly correlated features\n",
    "3. Selecting the most relevant features\n",
    "4. Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502960de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement data quality checks and cleaning\n",
    "# HINT: Use isnull().sum() for missing values, apply() with np.isinf() for infinite values\n",
    "\n",
    "print(\"Data Quality Check:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# TODO: Separate features and labels from the dataset\n",
    "# HINT: Use drop() method to remove 'Label' column for features\n",
    "features = \n",
    "labels = \n",
    "\n",
    "print(f\"Missing values per column:\")\n",
    "# TODO: Check for missing values in each column\n",
    "# HINT: Use isnull().sum() and filter where count > 0\n",
    "missing_counts = \n",
    "print(missing_counts[missing_counts > 0] if missing_counts.sum() > 0 else \"No missing values found\")\n",
    "\n",
    "print(f\"\\nInfinite values per column:\")\n",
    "# TODO: Check for infinite values using apply() and np.isinf()\n",
    "# HINT: Use lambda x: np.isinf(x).sum() inside apply()\n",
    "inf_counts = \n",
    "print(inf_counts[inf_counts > 0] if inf_counts.sum() > 0 else \"No infinite values found\")\n",
    "\n",
    "# TODO: Handle infinite values by replacing them with NaN, then fill with median\n",
    "# HINT: Use replace() method with [np.inf, -np.inf] and np.nan\n",
    "features_clean = \n",
    "if features_clean.isnull().sum().sum() > 0:\n",
    "    # TODO: Fill NaN values with column median\n",
    "    # HINT: Use fillna() with median() method\n",
    "    features_clean = \n",
    "    print(\"Infinite values replaced with column medians\")\n",
    "\n",
    "# TODO: Check data types distribution\n",
    "print(f\"\\nData types:\")\n",
    "print()\n",
    "\n",
    "print(f\"\\nCleaned dataset shape: {features_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement correlation analysis and feature selection\n",
    "# HINT: Use corr() method and nested loops to find highly correlated pairs\n",
    "\n",
    "print(\"Correlation Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# TODO: Calculate correlation matrix\n",
    "# HINT: Use .corr() method on features_clean\n",
    "correlation_matrix = \n",
    "\n",
    "# TODO: Find highly correlated feature pairs (threshold > 0.95)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        # TODO: Check if absolute correlation is greater than 0.95\n",
    "        # HINT: Use abs() and correlation_matrix.iloc[i, j]\n",
    "        if abs() > 0.95:\n",
    "            # TODO: Append tuple with (feature1, feature2, correlation_value)\n",
    "            high_corr_pairs.append((\n",
    "                ,  # feature 1 name\n",
    "                ,  # feature 2 name\n",
    "                   # correlation value\n",
    "            ))\n",
    "\n",
    "print(f\"Found {len(high_corr_pairs)} highly correlated feature pairs (|r| > 0.95):\")\n",
    "for feat1, feat2, corr in high_corr_pairs[:5]:  # Show first 5\n",
    "    print(f\"â€¢ {feat1} â†” {feat2}: {corr:.3f}\")\n",
    "\n",
    "# TODO: Remove highly correlated features (keep one from each pair)\n",
    "features_to_remove = set()\n",
    "for feat1, feat2, corr in high_corr_pairs:\n",
    "    # TODO: Add the second feature to removal set\n",
    "    # HINT: Use .add() method to add feat2\n",
    "    \n",
    "\n",
    "# TODO: Drop the highly correlated features\n",
    "# HINT: Use drop() method with columns parameter\n",
    "features_selected = \n",
    "print(f\"\\nRemoved {len(features_to_remove)} highly correlated features\")\n",
    "print(f\"Features after correlation filtering: {features_selected.shape[1]}\")\n",
    "\n",
    "# TODO: Create correlation heatmap visualization\n",
    "plt.figure(figsize=(14, 12))\n",
    "if features_selected.shape[1] > 20:\n",
    "    # Sample features for visualization\n",
    "    sample_features = features_selected.sample(n=15, axis=1, random_state=42)\n",
    "    # TODO: Create heatmap using seaborn\n",
    "    # HINT: Use sns.heatmap() with sample_features.corr()\n",
    "    sns.heatmap(, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.2f', cbar_kws={'shrink': .8})\n",
    "    plt.title('Correlation Heatmap (Sample of Features)', fontsize=16, fontweight='bold')\n",
    "else:\n",
    "    # TODO: Create heatmap for all features if <= 20\n",
    "    sns.heatmap(, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, fmt='.2f', cbar_kws={'shrink': .8})\n",
    "    plt.title('Correlation Heatmap (All Features)', fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d14499",
   "metadata": {},
   "source": [
    "## Step 3: Label Encoding\n",
    "\n",
    "Convert our categorical labels into binary format suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert categorical labels to binary format for machine learning\n",
    "# HINT: Use boolean indexing and .astype(int) to create binary labels\n",
    "\n",
    "print(\"Converting Labels to Binary Format:\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# TODO: Create binary labels: 0 = BENIGN, 1 = ATTACK\n",
    "# HINT: Use (labels != 'BENIGN') and convert to int\n",
    "binary_labels = \n",
    "\n",
    "print(\"Original label distribution:\")\n",
    "print(labels.value_counts())\n",
    "\n",
    "print(\"\\nBinary label distribution:\")\n",
    "# TODO: Count benign (0) and attack (1) samples\n",
    "print(\"0 (Benign):\", )\n",
    "print(\"1 (Attack):\", )\n",
    "\n",
    "# TODO: Calculate percentages for binary labels\n",
    "# HINT: Use .mean() * 100 for percentage calculation\n",
    "benign_pct = \n",
    "attack_pct = \n",
    "\n",
    "print(f\"\\nPercentages:\")\n",
    "print(f\"â€¢ Benign: {benign_pct:.1f}%\")\n",
    "print(f\"â€¢ Attack: {attack_pct:.1f}%\")\n",
    "\n",
    "# TODO: Create visualizations for binary distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "# TODO: Create count list and labels list for visualization\n",
    "counts = [, ]\n",
    "labels_viz = ['Benign (0)', 'Attack (1)']\n",
    "colors = ['lightgreen', 'coral']\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "# TODO: Create bar plot\n",
    "# HINT: Use plt.bar() with labels_viz, counts, colors, and edgecolor\n",
    "plt.bar(, , color=, edgecolor='black')\n",
    "plt.title('Binary Label Distribution', fontweight='bold')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# TODO: Create pie chart\n",
    "# HINT: Use plt.pie() with counts, labels_viz, autopct, colors, and startangle\n",
    "plt.pie(, labels=, autopct='%1.1f%%', colors=, startangle=90)\n",
    "plt.title('Binary Label Percentage', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nLabels successfully converted to binary format\")\n",
    "print(f\"Final dataset shape: {features_selected.shape[0]} samples Ã— {features_selected.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdbc2a",
   "metadata": {},
   "source": [
    "## Step 4: Feature Importance Analysis\n",
    "\n",
    "Let's identify which features are most important for distinguishing between benign and malicious traffic using Random Forest feature importance and mutual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123539cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement feature importance analysis using Random Forest and Mutual Information\n",
    "# HINT: Use RandomForestClassifier and mutual_info_classif for feature ranking\n",
    "\n",
    "print(\"Random Forest Feature Importance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Create and train Random Forest for feature importance\n",
    "# HINT: Use RandomForestClassifier with n_estimators=100, random_state=42\n",
    "rf = RandomForestClassifier()\n",
    "# TODO: Fit the model on features_selected and binary_labels\n",
    "rf.fit(, )\n",
    "\n",
    "# TODO: Create feature importance DataFrame\n",
    "# HINT: Use pd.DataFrame with feature names and importances, then sort by importance\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': ,\n",
    "    'importance': \n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 most important features (Random Forest):\")\n",
    "print(rf_importance.head(10))\n",
    "\n",
    "# TODO: Implement Mutual Information analysis\n",
    "print(\"\\nMutual Information Analysis:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# TODO: Calculate mutual information scores\n",
    "# HINT: Use mutual_info_classif() with features_selected, binary_labels, and random_state=42\n",
    "mi_scores = \n",
    "\n",
    "# TODO: Create mutual information DataFrame\n",
    "mi_importance = pd.DataFrame({\n",
    "    'feature': ,\n",
    "    'mi_score': \n",
    "}).sort_values('mi_score', ascending=False)\n",
    "\n",
    "print(\"Top 10 most important features (Mutual Information):\")\n",
    "print(mi_importance.head(10))\n",
    "\n",
    "# TODO: Create feature importance visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# TODO: Plot Random Forest importance (top 15 features)\n",
    "top_rf = rf_importance.head(15)\n",
    "# HINT: Use ax1.barh() with range(len(top_rf)), importance values, colors\n",
    "ax1.barh(, , color='lightblue', edgecolor='navy')\n",
    "ax1.set_yticks(range(len(top_rf)))\n",
    "ax1.set_yticklabels(top_rf['feature'])\n",
    "ax1.set_xlabel('Feature Importance')\n",
    "ax1.set_title('Top 15 Features - Random Forest Importance', fontweight='bold')\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# TODO: Plot Mutual Information scores (top 15 features)\n",
    "top_mi = mi_importance.head(15)\n",
    "# HINT: Use ax2.barh() with range(len(top_mi)), mi_score values, colors\n",
    "ax2.barh(, , color='lightcoral', edgecolor='darkred')\n",
    "ax2.set_yticks(range(len(top_mi)))\n",
    "ax2.set_yticklabels(top_mi['feature'])\n",
    "ax2.set_xlabel('Mutual Information Score')\n",
    "ax2.set_title('Top 15 Features - Mutual Information', fontweight='bold')\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# TODO: Select top features based on both methods\n",
    "# HINT: Use set() to get unique features from both methods, then union them\n",
    "top_rf_features = set(rf_importance.head(15)['feature'])\n",
    "top_mi_features = set(mi_importance.head(15)['feature'])\n",
    "# TODO: Combine both feature sets using union\n",
    "top_features = list()\n",
    "\n",
    "print(f\"\\nSelected {len(top_features)} top features for final dataset\")\n",
    "print(\"Selected features:\", top_features[:10], \"...\" if len(top_features) > 10 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f42c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement feature normalization and create before/after comparison\n",
    "# HINT: Use StandardScaler to normalize features and create comparison plots\n",
    "\n",
    "print(\"Data Normalization:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# TODO: Select final features from features_selected using top_features list\n",
    "# HINT: Use features_selected[top_features]\n",
    "final_features = \n",
    "\n",
    "# TODO: Create and apply StandardScaler\n",
    "# HINT: Initialize StandardScaler(), then use fit_transform()\n",
    "scaler = StandardScaler()\n",
    "features_normalized = \n",
    "# TODO: Create DataFrame with normalized features\n",
    "# HINT: Use pd.DataFrame() with features_normalized and final_features.columns\n",
    "features_normalized_df = \n",
    "\n",
    "print(f\"Normalized {features_normalized_df.shape[1]} features\")\n",
    "print(f\"Final feature statistics:\")\n",
    "# TODO: Display descriptive statistics of normalized features\n",
    "print()\n",
    "\n",
    "# TODO: Create before/after normalization comparison\n",
    "# HINT: Select first 3 features for visualization\n",
    "sample_features = \n",
    "fig, axes = plt.subplots(2, len(sample_features), figsize=(15, 8))\n",
    "\n",
    "for i, feature in enumerate(sample_features):\n",
    "    # TODO: Create histogram for original features (before normalization)\n",
    "    # HINT: Use axes[0, i].hist() with final_features[feature]\n",
    "    axes[0, i].hist(, bins=50, alpha=0.7, color='lightblue', edgecolor='navy')\n",
    "    axes[0, i].set_title(f'Before: {feature}', fontweight='bold')\n",
    "    axes[0, i].set_ylabel('Frequency')\n",
    "    \n",
    "    # TODO: Create histogram for normalized features (after normalization)\n",
    "    # HINT: Use axes[1, i].hist() with features_normalized_df[feature]\n",
    "    axes[1, i].hist(, bins=50, alpha=0.7, color='lightgreen', edgecolor='darkgreen')\n",
    "    axes[1, i].set_title(f'After: {feature}', fontweight='bold')\n",
    "    axes[1, i].set_ylabel('Frequency')\n",
    "    axes[1, i].set_xlabel('Value')\n",
    "\n",
    "plt.suptitle('Feature Normalization Comparison', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nData preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f624ae",
   "metadata": {},
   "source": [
    "## Step 5: Data Export\n",
    "\n",
    "Save our cleaned and preprocessed dataset for use in the next module where we'll build lightweight ML models for edge-based anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create final dataset and save for future use\n",
    "# HINT: Combine normalized features with binary labels and save to CSV\n",
    "\n",
    "# TODO: Create final dataset by copying normalized features\n",
    "final_dataset = \n",
    "# TODO: Add binary labels to the final dataset\n",
    "# HINT: Use .values to get numpy array from binary_labels\n",
    "final_dataset['Label'] = \n",
    "\n",
    "print(\"Final Dataset Summary:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Shape: {final_dataset.shape}\")\n",
    "print(f\"Features: {final_dataset.shape[1] - 1}\")\n",
    "print(f\"Samples: {final_dataset.shape[0]:,}\")\n",
    "# TODO: Calculate memory usage in MB\n",
    "# HINT: Use memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"\\nMemory usage: { / 1024**2:.2f} MB\")\n",
    "\n",
    "print(f\"\\nClass distribution in final dataset:\")\n",
    "# TODO: Calculate and display class distribution percentages\n",
    "# HINT: Use (final_dataset['Label'] == 0).sum() and .mean()*100\n",
    "print(f\"â€¢ Benign (0): {:,} ({:.1f}%)\")\n",
    "print(f\"â€¢ Attack (1): {:,} ({:.1f}%)\")\n",
    "\n",
    "# Display sample of final dataset\n",
    "print(\"\\nSample of Final Dataset:\")\n",
    "# TODO: Display first 5 rows of final dataset\n",
    "display()\n",
    "\n",
    "# TODO: Save dataset to CSV file\n",
    "# HINT: Create 'data' directory and save using to_csv()\n",
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "output_file = 'data/cleaned_network_traffic.csv'\n",
    "# TODO: Save final_dataset to CSV without index\n",
    "final_dataset.to_csv(, index=False)\n",
    "\n",
    "print(f\"\\nDataset saved to: {output_file}\")\n",
    "print(f\"Ready for edge AI model development!\")\n",
    "\n",
    "# TODO: Save feature names and scaler for future deployment\n",
    "# HINT: Use pickle to save feature names and scaler objects\n",
    "import pickle\n",
    "with open('data/feature_names.pkl', 'wb') as f:\n",
    "    # TODO: Save list of normalized feature column names\n",
    "    pickle.dump(, f)\n",
    "    \n",
    "with open('data/scaler.pkl', 'wb') as f:\n",
    "    # TODO: Save the fitted scaler object\n",
    "    pickle.dump(, f)\n",
    "\n",
    "print(\"Saved feature names and scaler for future model deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
