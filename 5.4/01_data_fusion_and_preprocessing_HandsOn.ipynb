{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f141597",
   "metadata": {},
   "source": [
    "# Module 5.4: Data Fusion and Preprocessing - **HANDS-ON VERSION**\n",
    "\n",
    "## Combined Case Study: Cybersecurity, Edge AI and Autonomous Driving\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Demonstrate how to combine and preprocess multimodal data from autonomous driving and cybersecurity contexts, preparing it for Edge AI modeling.\n",
    "\n",
    "**Key Learning Goals:**\n",
    "- Understand data fusion challenges in Connected and Autonomous Vehicles (CAVs)\n",
    "- Learn to synchronize asynchronous multimodal data streams\n",
    "- Master preprocessing techniques for heterogeneous data types\n",
    "- Prepare integrated datasets for edge AI deployment\n",
    "\n",
    "---\n",
    "\n",
    "## Why Data Fusion Matters in CAVs\n",
    "\n",
    "Connected and Autonomous Vehicles (CAVs) operate in complex environments where:\n",
    "\n",
    "1. **Physical Safety**: Vehicle sensors (LiDAR, cameras, GPS) monitor road conditions and obstacles\n",
    "2. **Cybersecurity**: Network monitoring detects malicious attacks on vehicle communication systems\n",
    "3. **Edge Processing**: Real-time decisions require fast, local processing with limited computational resources\n",
    "\n",
    "**Challenge**: Physical sensor data and network traffic data have different:\n",
    "- Sampling rates (telemetry: 10Hz, network: variable)\n",
    "- Data formats (numerical vs. categorical)\n",
    "- Timestamp precision (milliseconds vs. seconds)\n",
    "\n",
    "**Solution**: Data fusion techniques that align, normalize, and combine these streams for unified anomaly detection.\n",
    "\n",
    "---\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: This notebook contains code completion exercises marked with `# TODO:` comments. Fill in the missing code to complete the data fusion and preprocessing workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a9126a",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Datasets\n",
    "\n",
    "We'll create two realistic datasets representing:\n",
    "1. **Vehicle Telemetry**: Physical sensor data from autonomous driving systems\n",
    "2. **Network Traffic Logs**: Cybersecurity monitoring data from vehicle communication systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b46fd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_telemetry_data(n_samples=500, duration_minutes=30):\n",
    "    \"\"\"\n",
    "    Generate realistic vehicle telemetry data\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of telemetry records\n",
    "    - duration_minutes: Time span of the simulation\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with vehicle sensor data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Generating {n_samples} vehicle telemetry records...\")\n",
    "    \n",
    "    # Generate timestamps (every ~3.6 seconds for 30 minutes)\n",
    "    start_time = datetime.now() - timedelta(minutes=duration_minutes)\n",
    "    time_intervals = np.linspace(0, duration_minutes*60, n_samples)\n",
    "    timestamps = [start_time + timedelta(seconds=t) for t in time_intervals]\n",
    "    \n",
    "    # Simulate realistic driving patterns\n",
    "    # Speed varies between 0-100 km/h with traffic patterns\n",
    "    base_speed = 50 + 30 * np.sin(np.linspace(0, 4*np.pi, n_samples))  # Traffic patterns\n",
    "    speed = np.maximum(0, base_speed + np.random.normal(0, 5, n_samples))\n",
    "    \n",
    "    # Acceleration correlates with speed changes\n",
    "    acceleration = np.diff(np.concatenate([[speed[0]], speed])) + np.random.normal(0, 0.5, n_samples)\n",
    "    \n",
    "    # Distance to obstacle (closer in urban areas, farther on highways)\n",
    "    urban_factor = 0.5 + 0.5 * np.sin(np.linspace(0, 6*np.pi, n_samples))\n",
    "    distance_to_obstacle = 10 + 40 * urban_factor + np.random.exponential(5, n_samples)\n",
    "    \n",
    "    # GPS coordinates (simulated route)\n",
    "    base_lat, base_lon = 40.7128, -74.0060  # NYC area\n",
    "    gps_lat = base_lat + np.cumsum(np.random.normal(0, 0.0001, n_samples))\n",
    "    gps_lon = base_lon + np.cumsum(np.random.normal(0, 0.0001, n_samples))\n",
    "    \n",
    "    # Brake status (categorical: 0=off, 1=light, 2=heavy)\n",
    "    brake_prob = np.where(acceleration < -2, 0.8, 0.1)  # Brake when decelerating\n",
    "    brake_status = np.random.choice([0, 1, 2], n_samples, p=[0.7, 0.25, 0.05])\n",
    "    brake_status = np.where(np.random.random(n_samples) < brake_prob, \n",
    "                           np.random.choice([1, 2], n_samples), brake_status)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    telemetry_df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'speed': np.round(speed, 2),\n",
    "        'acceleration': np.round(acceleration, 3),\n",
    "        'distance_to_obstacle': np.round(distance_to_obstacle, 2),\n",
    "        'gps_lat': np.round(gps_lat, 6),\n",
    "        'gps_lon': np.round(gps_lon, 6),\n",
    "        'brake_status': brake_status\n",
    "    })\n",
    "    \n",
    "    print(f\"Generated telemetry data: {len(telemetry_df)} records\")\n",
    "    print(f\"   Time range: {telemetry_df['timestamp'].min()} to {telemetry_df['timestamp'].max()}\")\n",
    "    \n",
    "    return telemetry_df\n",
    "\n",
    "# Generate larger vehicle telemetry dataset for balanced training\n",
    "telemetry_data = generate_telemetry_data(n_samples=3000, duration_minutes=90)\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample Vehicle Telemetry Data:\")\n",
    "print(telemetry_data.head(10))\n",
    "print(f\"\\nTelemetry Data Shape: {telemetry_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899449d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network_data(n_samples=500, duration_minutes=30):\n",
    "    \"\"\"\n",
    "    Generate realistic network traffic logs for vehicle communication systems\n",
    "    \n",
    "    Parameters:\n",
    "    - n_samples: Number of network log entries\n",
    "    - duration_minutes: Time span of the simulation\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with network monitoring data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Generating {n_samples} network traffic records...\")\n",
    "    \n",
    "    # Generate timestamps with slight offset and variation from telemetry\n",
    "    start_time = datetime.now() - timedelta(minutes=duration_minutes)\n",
    "    # Network logs more frequent but irregular\n",
    "    time_intervals = np.sort(np.random.uniform(0, duration_minutes*60, n_samples))\n",
    "    timestamps = [start_time + timedelta(seconds=t) for t in time_intervals]\n",
    "    \n",
    "    # Source and destination IP addresses (vehicle communication)\n",
    "    vehicle_ips = ['192.168.1.10', '192.168.1.11', '192.168.1.12']  # Vehicle internal network\n",
    "    external_ips = ['8.8.8.8', '1.1.1.1', '10.0.0.1', '172.16.0.1']  # External services\n",
    "    \n",
    "    src_ips = np.random.choice(vehicle_ips + external_ips, n_samples)\n",
    "    dst_ips = np.random.choice(vehicle_ips + external_ips, n_samples)\n",
    "    \n",
    "    # Protocol types (TCP, UDP, ICMP for vehicle communications)\n",
    "    protocols = np.random.choice(['TCP', 'UDP', 'ICMP'], n_samples, p=[0.6, 0.35, 0.05])\n",
    "    \n",
    "    # Packet counts (bursts during active communication)\n",
    "    base_packets = np.random.poisson(lam=10, size=n_samples)\n",
    "    burst_factor = np.random.choice([1, 5, 20], n_samples, p=[0.8, 0.15, 0.05])  # Occasional bursts\n",
    "    packet_count = base_packets * burst_factor\n",
    "    \n",
    "    # Bytes transferred (correlates with packet count)\n",
    "    bytes_per_packet = np.random.normal(1500, 500, n_samples)  # Average Ethernet frame size\n",
    "    bytes_transferred = np.maximum(64, packet_count * np.maximum(64, bytes_per_packet))\n",
    "    \n",
    "    # Port numbers (common vehicle communication ports)\n",
    "    common_ports = [80, 443, 53, 22, 8080, 1883, 5683]  # HTTP, HTTPS, DNS, SSH, MQTT, CoAP\n",
    "    # Simplified port selection: 60% common ports, 40% random high ports\n",
    "    port = []\n",
    "    for _ in range(n_samples):\n",
    "        if np.random.random() < 0.6:\n",
    "            port.append(np.random.choice(common_ports))\n",
    "        else:\n",
    "            port.append(np.random.randint(1024, 65536))\n",
    "    port = np.array(port)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    network_df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'src_ip': src_ips,\n",
    "        'dst_ip': dst_ips,\n",
    "        'protocol': protocols,\n",
    "        'packet_count': packet_count,\n",
    "        'bytes_transferred': np.round(bytes_transferred).astype(int),\n",
    "        'port': port\n",
    "    })\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    network_df = network_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Generated network data: {len(network_df)} records\")\n",
    "    print(f\"   Time range: {network_df['timestamp'].min()} to {network_df['timestamp'].max()}\")\n",
    "    \n",
    "    return network_df\n",
    "\n",
    "# Generate larger network traffic dataset for balanced training  \n",
    "network_data = generate_network_data(n_samples=3000, duration_minutes=90)\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample Network Traffic Data:\")\n",
    "print(network_data.head(10))\n",
    "print(f\"\\nNetwork Data Shape: {network_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c65455",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing and Cleaning\n",
    "\n",
    "Before fusion, we need to:\n",
    "1. **Standardize timestamps** to enable proper synchronization\n",
    "2. **Handle missing values** realistically\n",
    "3. **Encode categorical variables** for ML compatibility\n",
    "4. **Normalize numerical features** for consistent scaling\n",
    "\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: Complete the preprocessing functions below by filling in the missing code sections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c4ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_telemetry_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess vehicle telemetry data\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing telemetry data...\")\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # TODO: 1. Standardize timestamp format\n",
    "    # HINT: Use pd.to_datetime() to convert timestamp column to datetime format\n",
    "    df_processed['timestamp'] = # TODO: Convert timestamp to datetime format\n",
    "    \n",
    "    # 2. Introduce realistic missing values (sensor failures)\n",
    "    missing_rate = 0.02  # 2% missing data\n",
    "    for col in ['speed', 'acceleration', 'distance_to_obstacle']:\n",
    "        missing_indices = np.random.choice(len(df_processed), \n",
    "                                         int(len(df_processed) * missing_rate), \n",
    "                                         replace=False)\n",
    "        df_processed.loc[missing_indices, col] = np.nan\n",
    "    \n",
    "    # TODO: 3. Handle missing values with interpolation (realistic for continuous sensor data)\n",
    "    # HINT: Use the interpolate() method with 'linear' method for each numeric column\n",
    "    numeric_cols = ['speed', 'acceleration', 'distance_to_obstacle', 'gps_lat', 'gps_lon']\n",
    "    for col in numeric_cols:\n",
    "        # TODO: Apply linear interpolation to handle missing values\n",
    "        df_processed[col] = df_processed[col].# TODO: Add interpolation method\n",
    "    \n",
    "    # 4. Add prefix to column names\n",
    "    rename_dict = {col: f'veh_{col}' for col in df_processed.columns if col != 'timestamp'}\n",
    "    df_processed = df_processed.rename(columns=rename_dict)\n",
    "    \n",
    "    print(f\"   Processed {len(df_processed)} telemetry records\")\n",
    "    print(f\"   Missing values handled via interpolation\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "def preprocess_network_data(df):\n",
    "    \"\"\"\n",
    "    Preprocess network traffic data for cybersecurity analysis\n",
    "    \"\"\"\n",
    "    print(\"Preprocessing network data...\")\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # TODO: 1. Standardize timestamp format\n",
    "    # HINT: Convert timestamp column to datetime format like in telemetry preprocessing\n",
    "    df_processed['timestamp'] = # TODO: Convert timestamp to datetime format\n",
    "    \n",
    "    # TODO: 2. Encode categorical variables\n",
    "    # HINT: Use LabelEncoder for protocol column\n",
    "    label_encoder = LabelEncoder()\n",
    "    # TODO: Apply label encoding to the 'protocol' column\n",
    "    df_processed['protocol_encoded'] = # TODO: Fit and transform protocol column\n",
    "    \n",
    "    # TODO: 3. Create derived features for network analysis\n",
    "    # HINT: Calculate bytes per packet and packets per second\n",
    "    df_processed['bytes_per_packet'] = # TODO: Calculate bytes_transferred / packet_count\n",
    "    \n",
    "    # TODO: 4. Handle potential division by zero\n",
    "    # HINT: Replace infinite values with 0 using np.where or fillna\n",
    "    df_processed['bytes_per_packet'] = # TODO: Replace inf values with 0\n",
    "    \n",
    "    # 5. Add prefix to column names (except timestamp)\n",
    "    rename_dict = {col: f'net_{col}' for col in df_processed.columns if col != 'timestamp'}\n",
    "    df_processed = df_processed.rename(columns=rename_dict)\n",
    "    \n",
    "    print(f\"   Processed {len(df_processed)} network records\")\n",
    "    print(f\"   Categorical encoding and feature engineering completed\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# TODO: Apply preprocessing functions to our datasets\n",
    "# HINT: Call the preprocessing functions on telemetry_data and network_data\n",
    "\n",
    "print(\"Step 2: Preprocessing datasets...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Preprocess telemetry data\n",
    "telemetry_processed = # TODO: Call preprocess_telemetry_data function\n",
    "\n",
    "# TODO: Preprocess network data  \n",
    "network_processed = # TODO: Call preprocess_network_data function\n",
    "\n",
    "print(\"\\nPreprocessing completed!\")\n",
    "print(f\"Telemetry processed shape: {telemetry_processed.shape}\")\n",
    "print(f\"Network processed shape: {network_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12f928",
   "metadata": {},
   "source": [
    "## Step 3: Temporal Synchronization\n",
    "\n",
    "The key challenge is synchronizing data from different sensors with varying sampling rates:\n",
    "- **Vehicle telemetry**: Regular intervals (~3.6 seconds)\n",
    "- **Network logs**: Irregular, event-driven timestamps\n",
    "\n",
    "We'll use **nearest neighbor matching** to align the datasets.\n",
    "\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: Implement the temporal synchronization algorithm to align asynchronous data streams!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synchronize_datasets(telemetry_df, network_df, time_tolerance_seconds=5):\n",
    "    \"\"\"\n",
    "    Synchronize telemetry and network data using nearest neighbor timestamp matching\n",
    "    \n",
    "    Parameters:\n",
    "    - telemetry_df: Preprocessed vehicle telemetry data\n",
    "    - network_df: Preprocessed network traffic data\n",
    "    - time_tolerance_seconds: Maximum allowed time difference for matching\n",
    "    \n",
    "    Returns:\n",
    "    - Synchronized and merged dataset\n",
    "    \"\"\"\n",
    "    print(f\"Synchronizing datasets with {time_tolerance_seconds}s tolerance...\")\n",
    "    \n",
    "    # Use telemetry timestamps as the reference (more regular)\n",
    "    merged_data = []\n",
    "    \n",
    "    for idx, telem_row in telemetry_df.iterrows():\n",
    "        telem_time = telem_row['timestamp']\n",
    "        \n",
    "        # TODO: Find closest network record within tolerance\n",
    "        # HINT: Calculate absolute time differences using (network_df['timestamp'] - telem_time).dt.total_seconds()\n",
    "        time_diffs = # TODO: Calculate time differences between network timestamps and current telemetry timestamp\n",
    "        \n",
    "        # TODO: Find the index of the minimum time difference\n",
    "        # HINT: Use idxmin() method on time_diffs\n",
    "        closest_idx = # TODO: Get index of minimum time difference\n",
    "        min_diff = time_diffs.iloc[closest_idx]\n",
    "        \n",
    "        # TODO: Only merge if within tolerance\n",
    "        # HINT: Check if min_diff <= time_tolerance_seconds\n",
    "        if # TODO: Add condition to check time tolerance:\n",
    "            # Merge the records\n",
    "            network_row = network_df.iloc[closest_idx]\n",
    "            \n",
    "            merged_row = {\n",
    "                'timestamp': telem_time,\n",
    "                'time_diff_seconds': min_diff\n",
    "            }\n",
    "            \n",
    "            # TODO: Add telemetry features (excluding timestamp)\n",
    "            # HINT: Loop through telem_row.index and add columns that are not 'timestamp'\n",
    "            for col in telem_row.index:\n",
    "                if col != 'timestamp':\n",
    "                    # TODO: Add telemetry column to merged_row\n",
    "                    merged_row[col] = # TODO: Get value from telem_row\n",
    "            \n",
    "            # TODO: Add network features (excluding timestamp) \n",
    "            # HINT: Similar to telemetry, loop through network_row.index\n",
    "            for col in network_row.index:\n",
    "                if col != 'timestamp':\n",
    "                    # TODO: Add network column to merged_row\n",
    "                    merged_row[col] = # TODO: Get value from network_row\n",
    "            \n",
    "            merged_data.append(merged_row)\n",
    "    \n",
    "    # TODO: Convert merged_data list to DataFrame\n",
    "    # HINT: Use pd.DataFrame() constructor\n",
    "    merged_df = # TODO: Create DataFrame from merged_data list\n",
    "    \n",
    "    print(f\"   Successfully synchronized {len(merged_df)} record pairs\")\n",
    "    print(f\"   Average time difference: {merged_df['time_diff_seconds'].mean():.2f}s\")\n",
    "    print(f\"   Max time difference: {merged_df['time_diff_seconds'].max():.2f}s\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# TODO: Synchronize the datasets\n",
    "# HINT: Call synchronize_datasets function with the processed datasets\n",
    "print(\"Step 3: Temporal Synchronization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "synchronized_data = # TODO: Call synchronization function\n",
    "\n",
    "print(f\"\\nSynchronized Dataset Shape: {synchronized_data.shape}\")\n",
    "print(f\"Sample Synchronized Data:\")\n",
    "print(synchronized_data.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_anomaly_labels(df, anomaly_rate=0.40, balance_classes=True):\n",
    "    \"\"\"\n",
    "    Add BALANCED anomaly labels to the synchronized dataset for fair model training\n",
    "    \n",
    "    Labels:\n",
    "    - 0: Normal operation\n",
    "    - 1: Physical anomaly (vehicle sensor/behavior issue)  \n",
    "    - 2: Network anomaly (cybersecurity threat)\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Synchronized dataset\n",
    "    - anomaly_rate: Fraction of data to label as anomalous (increased for balance)\n",
    "    - balance_classes: Whether to create balanced class distribution\n",
    "    \n",
    "    Returns:\n",
    "    - Dataset with balanced anomaly labels and injected anomalies\n",
    "    \"\"\"\n",
    "    print(f\"Adding BALANCED anomaly labels (target anomaly rate: {anomaly_rate:.1%})...\")\n",
    "    \n",
    "    df_labeled = df.copy()\n",
    "    \n",
    "    # TODO: Initialize all labels as normal (0)\n",
    "    # HINT: Create a new column 'label' and set all values to 0\n",
    "    df_labeled['label'] = # TODO: Set all labels to 0 (normal)\n",
    "    \n",
    "    if balance_classes:\n",
    "        # BALANCED APPROACH: Create roughly equal class sizes for fair training\n",
    "        total_samples = len(df_labeled)\n",
    "        \n",
    "        # Target distribution: 60% Normal, 20% Physical, 20% Network\n",
    "        # This creates a much more balanced dataset for better learning\n",
    "        normal_target = int(total_samples * 0.60)      # 60% normal\n",
    "        physical_target = int(total_samples * 0.20)    # 20% physical anomalies  \n",
    "        network_target = total_samples - normal_target - physical_target  # 20% network anomalies\n",
    "        \n",
    "        print(f\"   BALANCED class targets:\")\n",
    "        print(f\"      Normal: {normal_target} samples (60%)\")\n",
    "        print(f\"      Physical: {physical_target} samples (20%)\")\n",
    "        print(f\"      Network: {network_target} samples (20%)\")\n",
    "        \n",
    "        # TODO: Randomly select indices for physical anomalies\n",
    "        # HINT: Use np.random.choice() to select indices without replacement\n",
    "        physical_indices = # TODO: Randomly choose physical_target indices\n",
    "        \n",
    "        # TODO: Randomly select indices for network anomalies (from remaining samples)\n",
    "        # HINT: Select from indices not already chosen for physical anomalies\n",
    "        remaining_indices = # TODO: Get indices that are not in physical_indices\n",
    "        network_indices = # TODO: Randomly choose network_target indices from remaining\n",
    "        \n",
    "        # TODO: Assign labels for physical anomalies\n",
    "        # HINT: Set df_labeled.loc[physical_indices, 'label'] = 1\n",
    "        df_labeled.loc[physical_indices, 'label'] = # TODO: Set to 1 for physical anomalies\n",
    "        \n",
    "        # TODO: Assign labels for network anomalies  \n",
    "        # HINT: Set df_labeled.loc[network_indices, 'label'] = 2\n",
    "        df_labeled.loc[network_indices, 'label'] = # TODO: Set to 2 for network anomalies\n",
    "        \n",
    "        # TODO: Inject realistic anomaly patterns\n",
    "        # For physical anomalies: Modify vehicle features\n",
    "        for idx in physical_indices:\n",
    "            # TODO: Simulate erratic vehicle behavior\n",
    "            # HINT: Multiply speed by a random factor between 0.3 and 2.0\n",
    "            df_labeled.loc[idx, 'veh_speed'] *= # TODO: Random factor for erratic speed\n",
    "            # TODO: Add random acceleration anomaly\n",
    "            df_labeled.loc[idx, 'veh_acceleration'] += # TODO: Add random value between -5 and 5\n",
    "        \n",
    "        # For network anomalies: Modify network features  \n",
    "        for idx in network_indices:\n",
    "            # TODO: Simulate suspicious network activity\n",
    "            # HINT: Multiply packet_count by a factor between 5 and 20\n",
    "            df_labeled.loc[idx, 'net_packet_count'] *= # TODO: Random factor for packet bursts\n",
    "            # TODO: Modify bytes transferred proportionally\n",
    "            df_labeled.loc[idx, 'net_bytes_transferred'] *= # TODO: Same random factor as packets\n",
    "    \n",
    "    # TODO: Calculate final label distribution\n",
    "    # HINT: Use value_counts() method on the 'label' column\n",
    "    label_counts = # TODO: Get counts for each label value\n",
    "    \n",
    "    print(f\"\\n   Final label distribution:\")\n",
    "    print(f\"      Normal (0): {label_counts.get(0, 0):4d} ({label_counts.get(0, 0)/len(df_labeled)*100:.1f}%)\")\n",
    "    print(f\"      Physical (1): {label_counts.get(1, 0):4d} ({label_counts.get(1, 0)/len(df_labeled)*100:.1f}%)\")\n",
    "    print(f\"      Network (2): {label_counts.get(2, 0):4d} ({label_counts.get(2, 0)/len(df_labeled)*100:.1f}%)\")\n",
    "    \n",
    "    if balance_classes:\n",
    "        print(f\"   BALANCED dataset created for fair model training!\")\n",
    "        print(f\"   Much better class representation vs original 88%/6%/6%\")\n",
    "    \n",
    "    return df_labeled\n",
    "\n",
    "# TODO: Generate BALANCED dataset for fair model training\n",
    "print(\"CREATING BALANCED DATASET FOR FAIR MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "# TODO: Call add_anomaly_labels function\n",
    "final_dataset = # TODO: Add anomaly labels to synchronized data\n",
    "\n",
    "print(f\"\\nFinal Dataset Shape: {final_dataset.shape}\")\n",
    "print(f\"Sample Final Dataset:\")\n",
    "print(final_dataset.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4fb53",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's analyze the fused dataset to understand:\n",
    "1. **Data distribution and quality**\n",
    "2. **Correlation between vehicle and network features**\n",
    "3. **Anomaly patterns and characteristics**\n",
    "\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: Complete the data analysis functions to explore patterns in the fused dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b0586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_statistics(df):\n",
    "    \"\"\"\n",
    "    Provide comprehensive statistics about the fused dataset\n",
    "    \"\"\"\n",
    "    print(\"DATASET STATISTICS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # TODO: Basic information\n",
    "    # HINT: Use df.shape for dimensions, df['timestamp'].min()/max() for time range\n",
    "    print(f\"Dataset Shape: {# TODO: Get dataset shape}\")\n",
    "    print(f\"Time Range: {# TODO: Get min timestamp} to {# TODO: Get max timestamp}\")\n",
    "    \n",
    "    # TODO: Calculate duration in minutes\n",
    "    # HINT: Subtract min timestamp from max timestamp, then use .total_seconds()/60\n",
    "    duration_minutes = # TODO: Calculate duration between max and min timestamps in minutes\n",
    "    print(f\"Duration: {duration_minutes:.1f} minutes\")\n",
    "    \n",
    "    # TODO: Data quality metrics\n",
    "    # HINT: Use df.isnull().sum().sum() to count all missing values\n",
    "    missing_data = # TODO: Count total missing values in dataset\n",
    "    print(f\"\\nData Quality:\")\n",
    "    print(f\"   Total missing values: {missing_data}\")\n",
    "    \n",
    "    # TODO: Calculate data completeness percentage\n",
    "    # HINT: (1 - missing_data/(df.shape[0]*df.shape[1]))*100\n",
    "    completeness = # TODO: Calculate data completeness percentage\n",
    "    print(f\"   Data completeness: {completeness:.2f}%\")\n",
    "    \n",
    "    # TODO: Feature categories\n",
    "    # HINT: Use list comprehension to filter columns that start with 'veh_' and 'net_'\n",
    "    vehicle_features = # TODO: Get columns that start with 'veh_'\n",
    "    network_features = # TODO: Get columns that start with 'net_'\n",
    "    \n",
    "    print(f\"\\nVehicle Features ({len(vehicle_features)}): {vehicle_features}\")\n",
    "    print(f\"Network Features ({len(network_features)}): {network_features}\")\n",
    "    \n",
    "    # TODO: Label distribution\n",
    "    # HINT: Use value_counts() on the 'label' column and sort_index()\n",
    "    label_dist = # TODO: Get label distribution using value_counts\n",
    "    \n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    labels = ['Normal', 'Physical Anomaly', 'Network Anomaly']\n",
    "    for i, (count, label) in enumerate(zip(label_dist, labels)):\n",
    "        print(f\"   {label}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'vehicle_features': vehicle_features,\n",
    "        'network_features': network_features,\n",
    "        'label_distribution': label_dist\n",
    "    }\n",
    "\n",
    "# TODO: Analyze the dataset\n",
    "# HINT: Call analyze_dataset_statistics function with final_dataset\n",
    "analysis_results = # TODO: Call analysis function\n",
    "\n",
    "# TODO: Basic descriptive statistics\n",
    "print(\"\\nNumerical Features Statistics:\")\n",
    "# TODO: Select only numerical columns (exclude 'label' column)\n",
    "# HINT: Use select_dtypes(include=[np.number]) and remove 'label' if present\n",
    "numerical_cols = # TODO: Get numerical columns excluding 'label'\n",
    "if 'label' in numerical_cols:\n",
    "    numerical_cols.remove('label')\n",
    "    \n",
    "# TODO: Display descriptive statistics\n",
    "# HINT: Use df[numerical_cols].describe().round(3)\n",
    "# TODO: Show descriptive statistics for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c66bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis and visualization\n",
    "def visualize_correlations_and_anomalies(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of the fused dataset\n",
    "    \"\"\"\n",
    "    print(\"Creating comprehensive visualizations...\")\n",
    "    \n",
    "    # TODO: Set up subplot configuration\n",
    "    # HINT: Create a 2x2 subplot grid using plt.subplots()\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = # TODO: Create 2x2 subplot grid with figsize=(15, 12)\n",
    "    \n",
    "    # Plot 1: Correlation heatmap of numerical features\n",
    "    # TODO: Select numerical columns excluding non-feature columns\n",
    "    # HINT: Get numerical columns and exclude 'label', 'timestamp', 'time_diff_seconds'\n",
    "    numerical_features = # TODO: Get numerical columns for correlation analysis\n",
    "    exclude_cols = ['label', 'timestamp', 'time_diff_seconds']\n",
    "    correlation_cols = [col for col in numerical_features if col not in exclude_cols]\n",
    "    \n",
    "    # TODO: Calculate correlation matrix\n",
    "    # HINT: Use df[correlation_cols].corr()\n",
    "    correlation_matrix = # TODO: Calculate correlation matrix\n",
    "    \n",
    "    # TODO: Create correlation heatmap\n",
    "    # HINT: Use sns.heatmap() with annot=True, cmap='coolwarm', center=0\n",
    "    sns.heatmap(# TODO: Add correlation matrix and parameters for heatmap, ax=ax1)\n",
    "    ax1.set_title('Feature Correlation Matrix')\n",
    "    \n",
    "    # Plot 2: Label distribution\n",
    "    # TODO: Create label distribution plot\n",
    "    # HINT: Use df['label'].value_counts().plot(kind='bar')\n",
    "    label_counts = # TODO: Get label counts and create bar plot\n",
    "    ax2.set_title('Class Distribution')\n",
    "    ax2.set_xlabel('Anomaly Type')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.set_xticklabels(['Normal', 'Physical', 'Network'], rotation=0)\n",
    "    \n",
    "    # Plot 3: Vehicle vs Network features scatter\n",
    "    # TODO: Create scatter plot comparing vehicle speed vs network packet count\n",
    "    # HINT: Use different colors for different labels using c=df['label']\n",
    "    scatter = ax3.scatter(# TODO: Add x=vehicle speed, y=network packet count, c=labels, parameters)\n",
    "    ax3.set_xlabel('Vehicle Speed (veh_speed)')\n",
    "    ax3.set_ylabel('Network Packet Count (net_packet_count)')\n",
    "    ax3.set_title('Vehicle vs Network Features by Anomaly Type')\n",
    "    \n",
    "    # Plot 4: Feature distribution by anomaly type\n",
    "    # TODO: Create distribution plots for a key feature across different labels\n",
    "    feature_to_analyze = 'veh_acceleration'\n",
    "    for label in [0, 1, 2]:\n",
    "        label_names = {0: 'Normal', 1: 'Physical', 2: 'Network'}\n",
    "        # TODO: Plot density curve for the feature filtered by label\n",
    "        # HINT: Use df[df['label']==label][feature_to_analyze].plot(kind='density')\n",
    "        # TODO: Plot density for current label\n",
    "        ax4.# TODO: Add density plot for feature filtered by label, with label parameter\n",
    "    \n",
    "    ax4.set_title(f'Distribution: {feature_to_analyze}')\n",
    "    ax4.set_xlabel('Acceleration (m/sÂ²)')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Create comprehensive visualizations\n",
    "# HINT: Call visualize_correlations_and_anomalies function with final_dataset\n",
    "# TODO: Call visualization function\n",
    "\n",
    "print(\"\\nExploratory Data Analysis Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a66f5d",
   "metadata": {},
   "source": [
    "## Step 5: Feature Normalization and Final Dataset Preparation\n",
    "\n",
    "Before saving the final dataset, we'll normalize numerical features to ensure they're ready for machine learning models.\n",
    "\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: Complete the feature normalization and dataset export functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8cb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(df, exclude_cols=['timestamp', 'label', 'time_diff_seconds']):\n",
    "    \"\"\"\n",
    "    Normalize numerical features using StandardScaler\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input dataset\n",
    "    - exclude_cols: Columns to exclude from normalization\n",
    "    \n",
    "    Returns:\n",
    "    - Normalized dataset and fitted scaler\n",
    "    \"\"\"\n",
    "    print(\"Normalizing numerical features...\")\n",
    "    \n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    # TODO: Identify numerical columns to normalize\n",
    "    # HINT: Use select_dtypes(include=[np.number]) to get numerical columns\n",
    "    numerical_cols = # TODO: Get all numerical columns from dataframe\n",
    "    \n",
    "    # TODO: Filter out columns that should not be normalized\n",
    "    # HINT: Use list comprehension to exclude cols in exclude_cols\n",
    "    cols_to_normalize = # TODO: Get numerical columns that are not in exclude_cols\n",
    "    \n",
    "    print(f\"   Features to normalize: {cols_to_normalize}\")\n",
    "    \n",
    "    # TODO: Apply StandardScaler\n",
    "    # HINT: Create StandardScaler instance and use fit_transform()\n",
    "    scaler = # TODO: Create StandardScaler instance\n",
    "    df_normalized[cols_to_normalize] = # TODO: Apply fit_transform to selected columns\n",
    "    \n",
    "    print(f\"   Normalized {len(cols_to_normalize)} features\")\n",
    "    print(f\"   Features now have meanâ‰ˆ0 and stdâ‰ˆ1\")\n",
    "    \n",
    "    return df_normalized, scaler\n",
    "\n",
    "# TODO: Apply normalization to the final dataset\n",
    "print(\"Step 5: Feature Normalization and Final Preparation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Call normalize_features function\n",
    "final_normalized_dataset, feature_scaler = # TODO: Normalize final_dataset\n",
    "\n",
    "print(f\"\\nNormalized Dataset Shape: {final_normalized_dataset.shape}\")\n",
    "\n",
    "# TODO: Display normalization verification\n",
    "print(\"\\nNormalization Verification (mean should â‰ˆ 0, std should â‰ˆ 1):\")\n",
    "# TODO: Show mean and std of normalized numerical features\n",
    "# HINT: Select numerical columns, calculate mean() and std(), and round to 3 decimals\n",
    "numerical_cols = final_normalized_dataset.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude_verify = ['label', 'time_diff_seconds'] \n",
    "verify_cols = [col for col in numerical_cols if col not in exclude_verify]\n",
    "\n",
    "print(\"Means:\")\n",
    "print(# TODO: Show means of normalized columns)\n",
    "print(\"\\nStandard Deviations:\")  \n",
    "print(# TODO: Show standard deviations of normalized columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a6aeb",
   "metadata": {},
   "source": [
    "## Step 6: Save Final Dataset\n",
    "\n",
    "Save the preprocessed, fused, and normalized dataset for use in subsequent machine learning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059101c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Export the final processed dataset\n",
    "def export_processed_dataset(df, filename='combined_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Export the final processed and normalized dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Final processed dataset\n",
    "    - filename: Output filename\n",
    "    \n",
    "    Returns:\n",
    "    - Success message and dataset summary\n",
    "    \"\"\"\n",
    "    print(f\"Exporting processed dataset to {filename}...\")\n",
    "    \n",
    "    # TODO: Convert timestamp to string format for CSV compatibility\n",
    "    # HINT: Use pd.to_datetime() and dt.strftime() to format timestamps\n",
    "    df_export = df.copy()\n",
    "    df_export['timestamp'] = # TODO: Convert timestamp to string format 'YYYY-MM-DD HH:MM:SS'\n",
    "    \n",
    "    # TODO: Save to CSV file\n",
    "    # HINT: Use df.to_csv() with index=False\n",
    "    # TODO: Export dataframe to CSV file\n",
    "    \n",
    "    # TODO: Calculate and display summary statistics\n",
    "    print(f\"\\nDataset successfully exported!\")\n",
    "    print(f\"   File: {filename}\")\n",
    "    print(f\"   Shape: {df_export.shape}\")\n",
    "    print(f\"   Size: {# TODO: Calculate file size - use df_export.memory_usage(deep=True).sum() / 1024**2} MB\")\n",
    "    \n",
    "    return df_export\n",
    "\n",
    "# TODO: Create final export dataset \n",
    "print(\"Final Dataset Export\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Call export function with normalized dataset\n",
    "final_export_dataset = # TODO: Export final_normalized_dataset\n",
    "\n",
    "# TODO: Display final summary\n",
    "print(f\"\\nFINAL DATASET SUMMARY:\")\n",
    "print(f\"   Total Records: {# TODO: Get total number of records}\")\n",
    "print(f\"   Total Features: {# TODO: Get total number of features}\")\n",
    "print(f\"   Vehicle Features: {# TODO: Count columns starting with 'veh_'}\")\n",
    "print(f\"   Network Features: {# TODO: Count columns starting with 'net_'}\")\n",
    "\n",
    "# TODO: Show label distribution\n",
    "# HINT: Use value_counts().sort_index() on 'label' column\n",
    "label_counts = # TODO: Get final label distribution\n",
    "\n",
    "print(f\"\\nLabel Distribution:\")\n",
    "print(f\"   Normal (0): {label_counts.get(0, 0):,} records ({label_counts.get(0, 0)/len(final_export_dataset)*100:.1f}%)\")\n",
    "print(f\"   Physical Anomaly (1): {label_counts.get(1, 0):,} records ({label_counts.get(1, 0)/len(final_export_dataset)*100:.1f}%)\")\n",
    "print(f\"   Network Anomaly (2): {label_counts.get(2, 0):,} records ({label_counts.get(2, 0)/len(final_export_dataset)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDataset ready for Edge AI model training in next notebook!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
