{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0de8d608",
   "metadata": {},
   "source": [
    "# Module 5.4: Multimodal Edge AI Model - **HANDS-ON VERSION**\n",
    "\n",
    "## Combined Case Study: Cybersecurity, Edge AI and Autonomous Driving\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Build and train a **lightweight multimodal neural network** that processes:\n",
    "- **Vehicle telemetry features** (autonomous driving sensors)\n",
    "- **Network traffic features** (cybersecurity logs)\n",
    "\n",
    "The model will classify each timestamp into:\n",
    "- `0` = normal operation\n",
    "- `1` = physical anomaly (vehicle sensor/behavior issue)\n",
    "- `2` = network anomaly (cybersecurity threat)\n",
    "\n",
    "**Key Learning Goals:**\n",
    "- Design edge-optimized neural network architectures\n",
    "- Implement multimodal data fusion techniques\n",
    "- Train lightweight models suitable for real-time deployment\n",
    "- Evaluate performance across different anomaly types\n",
    "\n",
    "---\n",
    "\n",
    "## Multimodal Architecture Overview\n",
    "\n",
    "Our model uses a **MobileNetV2-sized dual-branch architecture** (~3.4M parameters):\n",
    "\n",
    "```\n",
    "Vehicle Telemetry    Network Traffic\n",
    "   Features             Features\n",
    "      â†“                    â†“\n",
    "   [512â†’256â†’128â†’         [512â†’256â†’128â†’\n",
    "    64â†’32â†’16]             64â†’32â†’16]\n",
    "      â†“                    â†“\n",
    "      â””â”€â”€â”€â”€ Attention â”€â”€â”€â”€â”˜\n",
    "              â†“\n",
    "   [512â†’256â†’128â†’64â†’32â†’16â†’3]\n",
    "```\n",
    "\n",
    "This enhanced design:\n",
    "1. **Large capacity**: ~3.4M parameters similar to MobileNetV2\n",
    "2. **Deep learning**: 6 layers per branch for complex pattern recognition\n",
    "3. **Attention mechanism**: Focused feature fusion for better multimodal learning\n",
    "4. **High performance**: Suitable for complex anomaly detection tasks\n",
    "\n",
    "---\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: This notebook contains code completion exercises marked with `# TODO:` comments. Fill in the missing code to build and train your multimodal Edge AI model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37009161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine Learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Multimodal Edge AI Model - Libraries Loaded Successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb371f",
   "metadata": {},
   "source": [
    "## Step 1: Load and Examine the Preprocessed Dataset\n",
    "\n",
    "Load the fused dataset from the previous notebook and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_examine_dataset(filename='combined_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Load the preprocessed dataset and examine its structure\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: Path to the CSV file created in previous notebook\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with the combined dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not Path(filename).exists():\n",
    "        print(f\"ERROR: {filename} not found!\")\n",
    "        print(\"\\nSOLUTION: Please run Notebook 01 (Data Fusion and Preprocessing) first.\")\n",
    "        print(\"   This will create the required 'combined_dataset.csv' file.\")\n",
    "        raise FileNotFoundError(f\"Dataset file {filename} not found\")\n",
    "    \n",
    "    print(f\"Loading dataset from {filename}...\")\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Examine structure\n",
    "    print(f\"\\nDataset Structure:\")\n",
    "    print(f\"   Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Identify feature categories\n",
    "    vehicle_features = [col for col in df.columns if col.startswith('veh_')]\n",
    "    network_features = [col for col in df.columns if col.startswith('net_')]\n",
    "    \n",
    "    print(f\"\\nVehicle Features ({len(vehicle_features)}):\")\n",
    "    for feature in vehicle_features:\n",
    "        print(f\"   â€¢ {feature}\")\n",
    "    \n",
    "    print(f\"\\nNetwork Features ({len(network_features)}):\")\n",
    "    for feature in network_features:\n",
    "        print(f\"   â€¢ {feature}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    label_counts = df['label'].value_counts().sort_index()\n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    labels = ['Normal', 'Physical Anomaly', 'Network Anomaly']\n",
    "    for i, (count, label_name) in enumerate(zip(label_counts, labels)):\n",
    "        print(f\"   {i}: {label_name} - {count} samples ({count/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return df, vehicle_features, network_features\n",
    "\n",
    "# Load the dataset\n",
    "dataset, vehicle_cols, network_cols = load_and_examine_dataset()\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample Data:\")\n",
    "print(dataset.head(3))\n",
    "\n",
    "# Check for missing values\n",
    "missing_data = dataset.isnull().sum().sum()\n",
    "print(f\"\\nData Quality Check:\")\n",
    "print(f\"   Missing values: {missing_data}\")\n",
    "print(f\"   Data completeness: {(1 - missing_data/dataset.size)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3737df2",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Features and Train/Test Split\n",
    "\n",
    "Separate the multimodal features and prepare data for training.\n",
    "\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: Complete the data preparation functions to organize multimodal features for neural network training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c79d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_multimodal_data(df, vehicle_features, network_features, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare multimodal data for training\n",
    "    \n",
    "    Parameters:\n",
    "    - df: Input dataset\n",
    "    - vehicle_features: List of vehicle feature column names\n",
    "    - network_features: List of network feature column names\n",
    "    - test_size: Fraction of data for testing\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - Prepared train/test splits for both modalities and labels\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Preparing multimodal data for training...\")\n",
    "    \n",
    "    # TODO: Extract features from dataframe\n",
    "    # HINT: Use df[column_list].values to extract feature arrays\n",
    "    X_vehicle = # TODO: Extract vehicle features as numpy array\n",
    "    X_network = # TODO: Extract network features as numpy array\n",
    "    y = # TODO: Extract labels as numpy array\n",
    "    \n",
    "    print(f\"   Vehicle features shape: {X_vehicle.shape}\")\n",
    "    print(f\"   Network features shape: {X_network.shape}\")\n",
    "    print(f\"   Labels shape: {y.shape}\")\n",
    "    \n",
    "    # TODO: Check for any remaining missing values\n",
    "    # HINT: Use np.isnan(array).sum() to count NaN values\n",
    "    vehicle_missing = # TODO: Count missing values in vehicle features\n",
    "    network_missing = # TODO: Count missing values in network features\n",
    "    \n",
    "    if vehicle_missing > 0 or network_missing > 0:\n",
    "        print(f\"   Found missing values: Vehicle={vehicle_missing}, Network={network_missing}\")\n",
    "        print(f\"   Filling missing values with median...\")\n",
    "        \n",
    "        # TODO: Fill missing values with median\n",
    "        # HINT: Use SimpleImputer with strategy='median'\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        imputer_vehicle = # TODO: Create SimpleImputer for vehicle features\n",
    "        imputer_network = # TODO: Create SimpleImputer for network features\n",
    "        \n",
    "        # TODO: Apply imputers to transform the data\n",
    "        X_vehicle = # TODO: Fit and transform vehicle features\n",
    "        X_network = # TODO: Fit and transform network features\n",
    "    \n",
    "    # TODO: Create train/test split\n",
    "    print(f\"\\nCreating train/test split ({(1-test_size)*100:.0f}%/{test_size*100:.0f}%)...\")\n",
    "    \n",
    "    # TODO: Generate indices and split them to maintain alignment\n",
    "    # HINT: Use np.arange(len(y)) to create indices, then train_test_split with stratify=y\n",
    "    indices = # TODO: Create array of indices from 0 to len(y)\n",
    "    train_idx, test_idx = # TODO: Split indices using train_test_split with stratification\n",
    "    \n",
    "    # TODO: Split the data using the indices\n",
    "    X_vehicle_train, X_vehicle_test = # TODO: Split vehicle features using indices\n",
    "    X_network_train, X_network_test = # TODO: Split network features using indices  \n",
    "    y_train, y_test = # TODO: Split labels using indices\n",
    "    \n",
    "    # TODO: Standardize features (important for PyTorch)\n",
    "    # HINT: Create StandardScaler instances and fit on training data\n",
    "    scaler_vehicle = # TODO: Create StandardScaler for vehicle features\n",
    "    scaler_network = # TODO: Create StandardScaler for network features\n",
    "    \n",
    "    # TODO: Fit scalers on training data and transform both train and test\n",
    "    X_vehicle_train = # TODO: Fit and transform vehicle training data\n",
    "    X_vehicle_test = # TODO: Transform vehicle test data (don't fit!)\n",
    "    X_network_train = # TODO: Fit and transform network training data\n",
    "    X_network_test = # TODO: Transform network test data (don't fit!)\n",
    "    \n",
    "    print(f\"   Train set: {len(y_train)} samples\")\n",
    "    print(f\"   Test set: {len(y_test)} samples\")\n",
    "    print(f\"   Features standardized\")\n",
    "    \n",
    "    # Check label distribution in splits\n",
    "    train_dist = np.bincount(y_train) / len(y_train) * 100\n",
    "    test_dist = np.bincount(y_test) / len(y_test) * 100\n",
    "    \n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    labels = ['Normal', 'Physical', 'Network']\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"   {label}: Train {train_dist[i]:.1f}%, Test {test_dist[i]:.1f}%\")\n",
    "    \n",
    "    # TODO: Convert to PyTorch tensors\n",
    "    # HINT: Use torch.FloatTensor for features and torch.LongTensor for labels\n",
    "    X_vehicle_train_tensor = # TODO: Convert vehicle training data to FloatTensor\n",
    "    X_vehicle_test_tensor = # TODO: Convert vehicle test data to FloatTensor\n",
    "    X_network_train_tensor = # TODO: Convert network training data to FloatTensor\n",
    "    X_network_test_tensor = # TODO: Convert network test data to FloatTensor\n",
    "    y_train_tensor = # TODO: Convert training labels to LongTensor\n",
    "    y_test_tensor = # TODO: Convert test labels to LongTensor\n",
    "    \n",
    "    return {\n",
    "        'X_vehicle_train': X_vehicle_train_tensor,\n",
    "        'X_vehicle_test': X_vehicle_test_tensor,\n",
    "        'X_network_train': X_network_train_tensor,\n",
    "        'X_network_test': X_network_test_tensor,\n",
    "        'y_train': y_train_tensor,\n",
    "        'y_test': y_test_tensor,\n",
    "        'scaler_vehicle': scaler_vehicle,\n",
    "        'scaler_network': scaler_network\n",
    "    }\n",
    "\n",
    "# TODO: Prepare the data\n",
    "# HINT: Call prepare_multimodal_data with dataset and feature column lists\n",
    "print(\"Step 2: Data Preparation for Neural Network Training\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_splits = # TODO: Call data preparation function\n",
    "\n",
    "print(f\"\\nâœ… Data preparation completed!\")\n",
    "print(f\"   Vehicle training tensor: {data_splits['X_vehicle_train'].shape}\")\n",
    "print(f\"   Network training tensor: {data_splits['X_network_train'].shape}\")\n",
    "print(f\"   Training labels: {data_splits['y_train'].shape}\")\n",
    "print(f\"   Output classes: {len(torch.unique(data_splits['y_train']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4677609",
   "metadata": {},
   "source": [
    "## Step 3: Build Multimodal Neural Network Architecture\n",
    "\n",
    "Design a MobileNetV2-sized dual-branch neural network for enhanced performance (~3.4M parameters).\n",
    "\n",
    "**ðŸ”¥ HANDS-ON PRACTICE**: Complete the neural network architecture by implementing the multimodal fusion layers and forward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6b5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEdgeAI(nn.Module):\n",
    "    \"\"\"\n",
    "    OPTIMIZED MobileNetV2-sized multimodal neural network\n",
    "    \n",
    "    Architecture with OPTIMIZED dropout rates (0.7 scale factor):\n",
    "    - Vehicle Branch: 6 layers with optimized dropout\n",
    "    - Network Branch: 6 layers with optimized dropout  \n",
    "    - Fusion: 5 layers with reduced dropout (prevent overfitting)\n",
    "    Target: ~3.4M parameters (similar to MobileNetV2)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vehicle_input_size, network_input_size, num_classes=3):\n",
    "        super(MultimodalEdgeAI, self).__init__()\n",
    "        \n",
    "        # OPTIMIZED dropout rates (0.7 scale factor from hyperparameter search)\n",
    "        dropout_scale = 0.7\n",
    "        \n",
    "        # Large Vehicle telemetry branch with OPTIMIZED dropout\n",
    "        self.vehicle_branch = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Linear(vehicle_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3 * dropout_scale),  # OPTIMIZED: 0.21\n",
    "            # Layer 2\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.25 * dropout_scale),  # OPTIMIZED: 0.175\n",
    "            # Layer 3\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2 * dropout_scale),  # OPTIMIZED: 0.14\n",
    "            # Layer 4\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.15 * dropout_scale),  # OPTIMIZED: 0.105\n",
    "            # Layer 5\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.1 * dropout_scale),  # OPTIMIZED: 0.07\n",
    "            # Layer 6\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16)\n",
    "        )\n",
    "        \n",
    "        # Large Network traffic branch with OPTIMIZED dropout\n",
    "        self.network_branch = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Linear(network_input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3 * dropout_scale),  # OPTIMIZED: 0.21\n",
    "            # Layer 2\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.25 * dropout_scale),  # OPTIMIZED: 0.175\n",
    "            # Layer 3\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2 * dropout_scale),  # OPTIMIZED: 0.14\n",
    "            # Layer 4\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.15 * dropout_scale),  # OPTIMIZED: 0.105\n",
    "            # Layer 5\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.1 * dropout_scale),  # OPTIMIZED: 0.07\n",
    "            # Layer 6\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16)\n",
    "        )\n",
    "        \n",
    "        # TODO: Attention mechanism for feature fusion\n",
    "        # HINT: Create a sequential layer that takes 32 inputs, projects to 64, applies Tanh, \n",
    "        #       then projects back to 32 and applies Softmax\n",
    "        self.attention = nn.Sequential(\n",
    "            # TODO: Add linear layer from 32 to 64\n",
    "            # TODO: Add Tanh activation\n",
    "            # TODO: Add linear layer from 64 to 32\n",
    "            # TODO: Add Softmax activation with dim=1\n",
    "        )\n",
    "        \n",
    "        # OPTIMIZED Fusion layers with reduced dropout (prevent overfitting)\n",
    "        self.fusion = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Linear(32, 512),  # 16 + 16 from both branches\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.25 * dropout_scale),  # OPTIMIZED: 0.175 (reduced from 0.4)\n",
    "            # Layer 2\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2 * dropout_scale),  # OPTIMIZED: 0.14 (reduced from 0.35)\n",
    "            # Layer 3\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.15 * dropout_scale),  # OPTIMIZED: 0.105 (reduced from 0.3)\n",
    "            # Layer 4\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.1 * dropout_scale),  # OPTIMIZED: 0.07 (reduced from 0.25)\n",
    "            # Layer 5\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.05 * dropout_scale),  # OPTIMIZED: 0.035 (reduced from 0.2)\n",
    "            # Layer 6\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.05),  # Minimal dropout for final layer\n",
    "            # Layer 7 (Output)\n",
    "            nn.Linear(16, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, vehicle_input, network_input):\n",
    "        # TODO: Process each branch through their respective networks\n",
    "        # HINT: Pass inputs through self.vehicle_branch and self.network_branch\n",
    "        vehicle_features = # TODO: Process vehicle input through vehicle branch\n",
    "        network_features = # TODO: Process network input through network branch\n",
    "        \n",
    "        # TODO: Concatenate features from both branches\n",
    "        # HINT: Use torch.cat([tensor1, tensor2], dim=1) to concatenate along feature dimension\n",
    "        fused_features = # TODO: Concatenate vehicle and network features\n",
    "        \n",
    "        # TODO: Apply attention mechanism\n",
    "        # HINT: Pass fused_features through self.attention to get weights\n",
    "        attention_weights = # TODO: Get attention weights from fused features\n",
    "        \n",
    "        # TODO: Apply attention weights to features (element-wise multiplication)\n",
    "        # HINT: Multiply fused_features by attention_weights\n",
    "        attended_features = # TODO: Apply attention weights to fused features\n",
    "        \n",
    "        # TODO: Final classification through fusion layers\n",
    "        # HINT: Pass attended_features through self.fusion\n",
    "        output = # TODO: Get final output through fusion layers\n",
    "        \n",
    "        return output\n",
    "\n",
    "def build_multimodal_model(vehicle_input_shape, network_input_shape, num_classes=3):\n",
    "    \"\"\"\n",
    "    Build a MobileNetV2-sized multimodal neural network for enhanced performance\n",
    "    \n",
    "    Parameters:\n",
    "    - vehicle_input_shape: Size of vehicle features\n",
    "    - network_input_shape: Size of network features\n",
    "    - num_classes: Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "    - PyTorch model (~3.4M parameters similar to MobileNetV2)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Building MobileNetV2-sized multimodal neural network...\")\n",
    "    \n",
    "    # TODO: Create model instance\n",
    "    # HINT: Use MultimodalEdgeAI class with the provided parameters\n",
    "    model = # TODO: Create MultimodalEdgeAI instance\n",
    "    \n",
    "    # TODO: Move model to device (CPU or GPU)\n",
    "    # HINT: Use model.to(device)\n",
    "    model = # TODO: Move model to appropriate device\n",
    "    \n",
    "    print(f\"Model built successfully!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# TODO: Build the model\n",
    "print(\"Step 3: Building Multimodal Neural Network\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# TODO: Call build_multimodal_model with appropriate parameters\n",
    "# HINT: Use shapes from data_splits tensors\n",
    "model = # TODO: Build model with vehicle and network input shapes\n",
    "\n",
    "# Display model architecture\n",
    "print(\"\\nModel Architecture Summary:\")\n",
    "print(model)\n",
    "\n",
    "# TODO: Calculate model size (important for edge deployment)\n",
    "print(\"\\nModel Statistics:\")\n",
    "# TODO: Count total parameters in the model\n",
    "# HINT: Use sum(p.numel() for p in model.parameters())\n",
    "total_params = # TODO: Count total parameters\n",
    "trainable_params = # TODO: Count trainable parameters (same calculation with p.requires_grad)\n",
    "\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: {total_params * 4 / 1024**2:.2f} MB (32-bit floats)\")\n",
    "print(f\"   Target: ~3.4M parameters (MobileNetV2-sized)\")\n",
    "print(f\"   Suitable for edge deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a35091",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model - HANDS-ON PRACTICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fee893",
   "metadata": {},
   "source": [
    "### Step 4A: Define Focal Loss and Helper Functions\n",
    "\n",
    "First, let's define the Focal Loss class and supporting components for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce8e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define Focal Loss class for handling class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        # TODO: Store loss parameters\n",
    "        # HINT: Assign alpha, gamma, and reduction to self attributes\n",
    "        self.alpha = # TODO: Store alpha parameter\n",
    "        self.gamma = # TODO: Store gamma parameter  \n",
    "        self.reduction = # TODO: Store reduction parameter\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        # TODO: Calculate standard cross entropy loss (without reduction)\n",
    "        # HINT: Use F.cross_entropy with reduction='none'\n",
    "        ce_loss = # TODO: Calculate cross entropy loss\n",
    "        \n",
    "        # TODO: Calculate probabilities from cross entropy\n",
    "        # HINT: Use torch.exp(-ce_loss) to get pt\n",
    "        pt = # TODO: Calculate pt from ce_loss\n",
    "        \n",
    "        # TODO: Calculate focal loss with focusing term\n",
    "        # HINT: focal_loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        focal_loss = # TODO: Apply focal loss formula\n",
    "        \n",
    "        # TODO: Apply alpha weighting if provided\n",
    "        if self.alpha is not None:\n",
    "            # TODO: Get alpha weights for current targets\n",
    "            # HINT: Use self.alpha[targets] to index into alpha tensor\n",
    "            alpha_t = # TODO: Get alpha weights for targets\n",
    "            # TODO: Apply alpha weighting\n",
    "            focal_loss = # TODO: Multiply focal_loss by alpha_t\n",
    "            \n",
    "        # TODO: Apply reduction (mean, sum, or none)\n",
    "        if self.reduction == 'mean':\n",
    "            return # TODO: Return mean of focal_loss\n",
    "        elif self.reduction == 'sum':\n",
    "            return # TODO: Return sum of focal_loss\n",
    "        else:\n",
    "            return # TODO: Return focal_loss without reduction\n",
    "\n",
    "def calculate_class_weights(y_train):\n",
    "    \"\"\"Use optimized class weights found by hyperparameter search.\"\"\"\n",
    "    y_train_numpy = y_train.numpy()\n",
    "    class_counts = np.bincount(y_train_numpy)\n",
    "    \n",
    "    # TODO: Set optimized class weights\n",
    "    # HINT: Weights should be [0.6, 1.4, 1.0] for [Normal, Physical, Network]\n",
    "    optimized_weights = # TODO: Define optimized weights list\n",
    "    \n",
    "    print(f\"Class Weights (from hyperparameter search):\")\n",
    "    labels = ['Normal', 'Physical', 'Network']\n",
    "    for i, (label, weight, count) in enumerate(zip(labels, optimized_weights, class_counts)):\n",
    "        print(f\"   {label:<12}: {weight:.2f} (samples: {count})\")\n",
    "    \n",
    "    # TODO: Convert to PyTorch tensor\n",
    "    # HINT: Use torch.FloatTensor(optimized_weights)\n",
    "    return # TODO: Return FloatTensor of weights\n",
    "\n",
    "print(\"Focal Loss and helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce8a85",
   "metadata": {},
   "source": [
    "### Step 4B: Main Training Function\n",
    "\n",
    "The core training loop with enhanced monitoring and stability improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multimodal_model(model, data_splits, epochs=100, batch_size=32, validation_split=0.2, learning_rate=3e-4):\n",
    "    \"\"\"\n",
    "    OPTIMIZED TRAINING with parameters found by hyperparameter search:\n",
    "    - Learning rate: 3e-4 (optimal for stability)\n",
    "    - Weight decay: 1e-4 (optimal regularization)\n",
    "    - Focal gamma: 1.5 (balanced focus on hard examples)\n",
    "    - Loss mix: 50/50 (balanced Focal/CrossEntropy)\n",
    "    - Class weights: [0.6, 1.4, 1.0] (optimal class balance)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Calculate enhanced class weights and move to device\n",
    "    # HINT: Call calculate_class_weights and use .to(device)\n",
    "    alpha_tensor = # TODO: Calculate class weights and move to device\n",
    "    \n",
    "    # TODO: Split training data for validation\n",
    "    n_train = len(data_splits['X_vehicle_train'])\n",
    "    # TODO: Calculate validation size\n",
    "    # HINT: n_val = int(n_train * validation_split)\n",
    "    n_val = # TODO: Calculate validation size\n",
    "    n_train_actual = n_train - n_val\n",
    "    \n",
    "    # TODO: Create random indices for train/val split\n",
    "    # HINT: Use torch.randperm(n_train)\n",
    "    indices = # TODO: Create random permutation of indices\n",
    "    train_indices = indices[:n_train_actual]\n",
    "    val_indices = indices[n_train_actual:]\n",
    "    \n",
    "    # TODO: Split training data using indices\n",
    "    # HINT: Use data_splits['X_vehicle_train'][train_indices], etc.\n",
    "    X_vehicle_train = # TODO: Get vehicle training data\n",
    "    X_network_train = # TODO: Get network training data  \n",
    "    y_train = # TODO: Get training labels\n",
    "    \n",
    "    # TODO: Split validation data using indices\n",
    "    X_vehicle_val = # TODO: Get vehicle validation data\n",
    "    X_network_val = # TODO: Get network validation data\n",
    "    y_val = # TODO: Get validation labels\n",
    "    \n",
    "    # TODO: Create PyTorch datasets\n",
    "    # HINT: Use TensorDataset(X_vehicle_train, X_network_train, y_train)\n",
    "    train_dataset = # TODO: Create training dataset\n",
    "    val_dataset = # TODO: Create validation dataset\n",
    "    \n",
    "    # TODO: Create data loaders\n",
    "    # HINT: Use DataLoader with batch_size and shuffle parameters\n",
    "    train_loader = # TODO: Create training data loader (shuffle=True)\n",
    "    val_loader = # TODO: Create validation data loader (shuffle=False)\n",
    "    \n",
    "    # TODO: Initialize loss functions with OPTIMIZED parameters\n",
    "    # HINT: Use FocalLoss and CrossEntropyLoss with alpha_tensor\n",
    "    focal_criterion = # TODO: Create FocalLoss with alpha=alpha_tensor, gamma=1.5\n",
    "    ce_criterion = # TODO: Create CrossEntropyLoss with weight=alpha_tensor, label_smoothing=0.05\n",
    "    \n",
    "    # TODO: Initialize optimizer with OPTIMIZED parameters\n",
    "    # HINT: Use optim.AdamW with model parameters, lr, and weight_decay=1e-4\n",
    "    optimizer = # TODO: Create AdamW optimizer\n",
    "    \n",
    "    # TODO: Initialize learning rate scheduler\n",
    "    # HINT: Use ReduceLROnPlateau with mode='min', factor=0.7, patience=20\n",
    "    scheduler = # TODO: Create learning rate scheduler\n",
    "    \n",
    "    # Initialize training history dictionary\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': [],\n",
    "        'train_precision': [], 'val_precision': [], 'train_recall': [], 'val_recall': [],\n",
    "        'train_f1': [], 'val_f1': [],\n",
    "        'learning_rate': [], 'focal_loss': [], 'ce_loss': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\\\nStarting fine-tuned training...\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience_limit = 40\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # TODO: Set model to training mode\n",
    "        # HINT: Use model.train()\n",
    "        # TODO: Set model mode for training\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_focal_loss = 0.0\n",
    "        train_ce_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        \n",
    "        for batch_vehicle, batch_network, batch_labels in train_loader:\n",
    "            # TODO: Move batch data to device\n",
    "            # HINT: Use .to(device) for each tensor\n",
    "            batch_vehicle = # TODO: Move vehicle batch to device\n",
    "            batch_network = # TODO: Move network batch to device\n",
    "            batch_labels = # TODO: Move labels to device\n",
    "            \n",
    "            # TODO: Zero gradients\n",
    "            # HINT: Use optimizer.zero_grad()\n",
    "            # TODO: Clear gradients\n",
    "            \n",
    "            # TODO: Forward pass through model\n",
    "            # HINT: Call model(batch_vehicle, batch_network)\n",
    "            outputs = # TODO: Get model outputs\n",
    "            \n",
    "            # TODO: Calculate losses\n",
    "            # HINT: Call focal_criterion and ce_criterion with outputs and labels\n",
    "            focal_loss = # TODO: Calculate focal loss\n",
    "            ce_loss = # TODO: Calculate cross entropy loss\n",
    "            \n",
    "            # TODO: Combine losses with 50/50 balance\n",
    "            # HINT: combined_loss = 0.5 * focal_loss + 0.5 * ce_loss\n",
    "            combined_loss = # TODO: Combine focal and CE losses\n",
    "            \n",
    "            # TODO: Backward pass and optimization\n",
    "            # HINT: Use combined_loss.backward(), clip gradients, optimizer.step()\n",
    "            # TODO: Compute gradients\n",
    "            # TODO: Clip gradients with max_norm=0.5\n",
    "            # TODO: Update parameters\n",
    "            \n",
    "            # Update training statistics\n",
    "            train_loss += combined_loss.item()\n",
    "            train_focal_loss += focal_loss.item()\n",
    "            train_ce_loss += ce_loss.item()\n",
    "            \n",
    "            # TODO: Calculate predictions and accuracy\n",
    "            # HINT: Use torch.max(outputs.data, 1) to get predicted classes\n",
    "            _, predicted = # TODO: Get predicted classes from outputs\n",
    "            train_total += batch_labels.size(0)\n",
    "            train_correct += (predicted == batch_labels).sum().item()\n",
    "            \n",
    "            train_predictions.extend(predicted.cpu().numpy())\n",
    "            train_targets.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        # TODO: Validation phase - Set model to evaluation mode\n",
    "        # HINT: Use model.eval()\n",
    "        # TODO: Set model mode for evaluation\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        \n",
    "        # TODO: Disable gradient computation for validation\n",
    "        # HINT: Use torch.no_grad() context manager\n",
    "        with # TODO: Create no_grad context:\n",
    "            for batch_vehicle, batch_network, batch_labels in val_loader:\n",
    "                # TODO: Move validation batch to device\n",
    "                batch_vehicle = # TODO: Move vehicle batch to device\n",
    "                batch_network = # TODO: Move network batch to device\n",
    "                batch_labels = # TODO: Move labels to device\n",
    "                \n",
    "                # TODO: Forward pass (no gradients needed)\n",
    "                outputs = # TODO: Get model outputs\n",
    "                \n",
    "                # TODO: Calculate validation losses\n",
    "                focal_loss = # TODO: Calculate focal loss\n",
    "                ce_loss = # TODO: Calculate cross entropy loss\n",
    "                combined_loss = # TODO: Combine losses (50/50)\n",
    "                \n",
    "                val_loss += combined_loss.item()\n",
    "                \n",
    "                # TODO: Calculate validation predictions\n",
    "                _, predicted = # TODO: Get predicted classes\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "                \n",
    "                val_predictions.extend(predicted.cpu().numpy())\n",
    "                val_targets.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        # TODO: Calculate average losses and accuracies\n",
    "        train_loss_avg = # TODO: Calculate average training loss\n",
    "        val_loss_avg = # TODO: Calculate average validation loss\n",
    "        train_acc = # TODO: Calculate training accuracy\n",
    "        val_acc = # TODO: Calculate validation accuracy\n",
    "        \n",
    "        # TODO: Calculate precision, recall, and F1 scores\n",
    "        from sklearn.metrics import f1_score\n",
    "        # HINT: Use precision_score, recall_score, f1_score with average='macro'\n",
    "        train_precision = # TODO: Calculate training precision\n",
    "        val_precision = # TODO: Calculate validation precision\n",
    "        train_recall = # TODO: Calculate training recall\n",
    "        val_recall = # TODO: Calculate validation recall\n",
    "        train_f1 = # TODO: Calculate training F1\n",
    "        val_f1 = # TODO: Calculate validation F1\n",
    "        \n",
    "        # TODO: Store metrics in history\n",
    "        # HINT: Append each metric to corresponding history list\n",
    "        history['train_loss'].append(train_loss_avg)\n",
    "        history['val_loss'].append(val_loss_avg)\n",
    "        # TODO: Append remaining metrics to history\n",
    "        \n",
    "        # TODO: Update learning rate scheduler\n",
    "        # HINT: Call scheduler.step(val_loss_avg)\n",
    "        # TODO: Update scheduler with validation loss\n",
    "        \n",
    "        # Print progress every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                  f\"Train Loss: {train_loss_avg:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.3f} - \"\n",
    "                  f\"Val Loss: {val_loss_avg:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.3f} - \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # TODO: Early stopping logic\n",
    "        if val_loss_avg < best_val_loss:\n",
    "            best_val_loss = val_loss_avg\n",
    "            patience_counter = 0\n",
    "            # TODO: Save best model\n",
    "            # HINT: Use torch.save(model.state_dict(), 'best_improved_model.pth')\n",
    "            # TODO: Save model state dict\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience_limit:\n",
    "                print(f\"\\\\nEarly stopping after {epoch+1} epochs (patience: {patience_limit})\")\n",
    "                # TODO: Load best model weights\n",
    "                # HINT: Use model.load_state_dict(torch.load('best_improved_model.pth'))\n",
    "                # TODO: Load best model state\n",
    "                break\n",
    "    \n",
    "    print(f\"\\\\nFine-tuned Training Completed!\")\n",
    "    print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"   Final validation accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "print(\"Complete training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3aecc6",
   "metadata": {},
   "source": [
    "### Step 4C: Training Loop Execution\n",
    "\n",
    "Execute the training with proper error handling and metrics tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32090b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Rebuild model with optimized architecture\n",
    "# HINT: Use build_multimodal_model with shapes from data_splits\n",
    "model = # TODO: Build model with vehicle and network input shapes and 3 classes\n",
    "\n",
    "# TODO: Start training with optimized parameters\n",
    "# HINT: Call train_multimodal_model with model, data_splits, epochs=100, batch_size=32\n",
    "training_history = # TODO: Train the model and get history\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac388b5b",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Training Progress - HANDS-ON PRACTICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2b226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot comprehensive training history visualizations\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create subplot figure with 2 rows and 3 columns\n",
    "    # HINT: Use plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig, axes = # TODO: Create subplot figure\n",
    "    fig.suptitle('Enhanced Multimodal Edge AI Model - Training Progress', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # TODO: Create epochs range for x-axis\n",
    "    # HINT: Use range(1, len(history['train_loss']) + 1)\n",
    "    epochs = # TODO: Create epochs range\n",
    "    \n",
    "    # TODO: Plot 1 - Loss curves\n",
    "    ax1 = axes[0, 0]\n",
    "    # TODO: Plot training and validation loss\n",
    "    # HINT: Use ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    # TODO: Plot training loss\n",
    "    # TODO: Plot validation loss\n",
    "    ax1.set_title('Model Loss (Enhanced Training)')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('CrossEntropy Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TODO: Plot 2 - Accuracy curves\n",
    "    ax2 = axes[0, 1]\n",
    "    # TODO: Plot training and validation accuracy\n",
    "    # HINT: Use ax2.plot with 'train_acc' and 'val_acc' from history\n",
    "    # TODO: Plot training accuracy\n",
    "    # TODO: Plot validation accuracy\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TODO: Plot 3 - Learning Rate\n",
    "    ax3 = axes[0, 2]\n",
    "    # TODO: Plot learning rate schedule\n",
    "    # HINT: Use ax3.plot with 'learning_rate' from history and set_yscale('log')\n",
    "    # TODO: Plot learning rate\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    # TODO: Set y-axis to log scale\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TODO: Plot 4 - Precision curves\n",
    "    ax4 = axes[1, 0]\n",
    "    # TODO: Plot training and validation precision\n",
    "    # HINT: Use 'train_precision' and 'val_precision' from history\n",
    "    # TODO: Plot training precision\n",
    "    # TODO: Plot validation precision\n",
    "    ax4.set_title('Model Precision')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Precision')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TODO: Plot 5 - Recall curves\n",
    "    ax5 = axes[1, 1]\n",
    "    # TODO: Plot training and validation recall\n",
    "    # HINT: Use 'train_recall' and 'val_recall' from history\n",
    "    # TODO: Plot training recall\n",
    "    # TODO: Plot validation recall\n",
    "    ax5.set_title('Model Recall')\n",
    "    ax5.set_xlabel('Epoch')\n",
    "    ax5.set_ylabel('Recall')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TODO: Plot 6 - Combined validation metrics\n",
    "    ax6 = axes[1, 2]\n",
    "    # TODO: Plot multiple validation metrics on same plot\n",
    "    # HINT: Plot val_acc, val_precision, val_recall from history\n",
    "    # TODO: Plot validation accuracy\n",
    "    # TODO: Plot validation precision\n",
    "    # TODO: Plot validation recall\n",
    "    ax6.set_title('Validation Metrics Combined')\n",
    "    ax6.set_xlabel('Epoch')\n",
    "    ax6.set_ylabel('Score')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    # TODO: Apply tight layout and show plot\n",
    "    # HINT: Use plt.tight_layout() and plt.show()\n",
    "    # TODO: Apply tight layout\n",
    "    # TODO: Show the plot\n",
    "    \n",
    "    # TODO: Calculate training insights\n",
    "    final_train_acc = # TODO: Get final training accuracy from history\n",
    "    final_val_acc = # TODO: Get final validation accuracy from history\n",
    "    # TODO: Find best validation accuracy\n",
    "    # HINT: Use max(history['val_acc'])\n",
    "    best_val_acc = # TODO: Get best validation accuracy\n",
    "    # TODO: Find epoch with best validation accuracy\n",
    "    # HINT: Use history['val_acc'].index(best_val_acc) + 1\n",
    "    best_val_epoch = # TODO: Get epoch with best validation accuracy\n",
    "    # TODO: Calculate overfitting gap\n",
    "    overfitting = # TODO: Calculate training - validation accuracy gap\n",
    "    \n",
    "    print(f\"\\\\nEnhanced Training Insights:\")\n",
    "    print(f\"   Final training accuracy: {final_train_acc:.3f}\")\n",
    "    print(f\"   Final validation accuracy: {final_val_acc:.3f}\")\n",
    "    print(f\"   Best validation accuracy: {best_val_acc:.3f} (epoch {best_val_epoch})\")\n",
    "    print(f\"   Overfitting gap: {overfitting:.3f}\")\n",
    "    \n",
    "    # TODO: Assess performance based on best validation accuracy\n",
    "    if best_val_acc > 0.85:\n",
    "        performance_status = # TODO: Set status for excellent performance\n",
    "    elif best_val_acc > 0.75:\n",
    "        performance_status = # TODO: Set status for good performance\n",
    "    elif best_val_acc > 0.65:\n",
    "        performance_status = # TODO: Set status for acceptable performance\n",
    "    else:\n",
    "        performance_status = # TODO: Set status for needs improvement\n",
    "    \n",
    "    print(f\"   Performance assessment: {performance_status}\")\n",
    "    \n",
    "    # TODO: Analyze overfitting level\n",
    "    if overfitting > 0.15:\n",
    "        print(f\"   High overfitting - consider more regularization\")\n",
    "    elif overfitting > 0.05:\n",
    "        print(f\"   Moderate overfitting - model is learning well\")\n",
    "    else:\n",
    "        print(f\"   Low overfitting - excellent generalization\")\n",
    "\n",
    "# TODO: Plot training history\n",
    "# HINT: Call plot_training_history with training_history\n",
    "# TODO: Plot the training history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889dce2",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Model Performance - HANDS-ON PRACTICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cc37b0",
   "metadata": {},
   "source": [
    "### Step 6A: Core Evaluation Function\n",
    "\n",
    "Define the evaluation function with balanced class-friendly parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c80e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_balanced(model, data_splits, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Streamlined evaluation for balanced datasets - no complex thresholding needed!\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained model\n",
    "    - data_splits: Data splits dictionary  \n",
    "    - temperature: Temperature scaling factor\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Evaluating model on BALANCED dataset...\")\n",
    "    # TODO: Set model to evaluation mode\n",
    "    # HINT: Use model.eval()\n",
    "    # TODO: Set model mode for evaluation\n",
    "    \n",
    "    # TODO: Prepare test data and move to device\n",
    "    # HINT: Get test data from data_splits and use .to(device)\n",
    "    X_vehicle_test = # TODO: Get vehicle test data and move to device\n",
    "    X_network_test = # TODO: Get network test data and move to device\n",
    "    y_test = # TODO: Get test labels (keep on CPU for metrics)\n",
    "    \n",
    "    # TODO: Standard prediction (no gradients needed for evaluation)\n",
    "    # HINT: Use torch.no_grad() context manager\n",
    "    with # TODO: Create no_grad context:\n",
    "        # TODO: Get model outputs\n",
    "        # HINT: Call model(X_vehicle_test, X_network_test)\n",
    "        outputs = # TODO: Get model predictions\n",
    "        \n",
    "        # TODO: Apply temperature scaling\n",
    "        # HINT: Divide outputs by temperature\n",
    "        calibrated_outputs = # TODO: Apply temperature scaling\n",
    "        \n",
    "        # TODO: Get class probabilities\n",
    "        # HINT: Use F.softmax(calibrated_outputs, dim=1)\n",
    "        y_pred_proba = # TODO: Get prediction probabilities\n",
    "        \n",
    "        # TODO: Get predicted classes\n",
    "        # HINT: Use torch.max(calibrated_outputs, 1) and take second return value\n",
    "        _, y_pred = # TODO: Get predicted class indices\n",
    "    \n",
    "    # TODO: Convert tensors to numpy for sklearn metrics\n",
    "    # HINT: Use .cpu().numpy() for tensors\n",
    "    y_pred_proba_np = # TODO: Convert probabilities to numpy\n",
    "    y_pred_np = # TODO: Convert predictions to numpy\n",
    "    y_true_np = # TODO: Convert true labels to numpy\n",
    "    \n",
    "    # TODO: Calculate evaluation metrics\n",
    "    # HINT: Use sklearn.metrics functions with average='macro' for multiclass\n",
    "    test_accuracy = # TODO: Calculate accuracy using accuracy_score\n",
    "    test_precision = # TODO: Calculate precision using precision_score with average='macro'\n",
    "    test_recall = # TODO: Calculate recall using recall_score with average='macro'\n",
    "    # TODO: Calculate test loss\n",
    "    # HINT: Use F.cross_entropy(calibrated_outputs.cpu(), y_test).item()\n",
    "    test_loss = # TODO: Calculate cross entropy loss\n",
    "    \n",
    "    print(f\"\\\\nBalanced Dataset Results:\")\n",
    "    print(f\"   Accuracy: {test_accuracy:.3f}\")\n",
    "    print(f\"   Precision: {test_precision:.3f}\")  \n",
    "    print(f\"   Recall: {test_recall:.3f}\")\n",
    "    print(f\"   Loss: {test_loss:.3f}\")\n",
    "    \n",
    "    # TODO: Return evaluation results dictionary\n",
    "    # HINT: Include accuracy, precision, recall, loss, predictions, probabilities, true_labels\n",
    "    return {\n",
    "        'accuracy': # TODO: Add accuracy\n",
    "        'precision': # TODO: Add precision \n",
    "        'recall': # TODO: Add recall\n",
    "        'loss': # TODO: Add loss\n",
    "        'predictions': # TODO: Add numpy predictions\n",
    "        'probabilities': # TODO: Add numpy probabilities\n",
    "        'true_labels': # TODO: Add numpy true labels\n",
    "    }\n",
    "\n",
    "print(\"Streamlined evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e5c5f",
   "metadata": {},
   "source": [
    "### Step 6B: Run Evaluation\n",
    "\n",
    "Execute the streamlined evaluation on our balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14765e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run streamlined evaluation\n",
    "print(\"DATASET EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# TODO: Evaluate model on test set\n",
    "# HINT: Call evaluate_model_balanced with model, data_splits, and temperature=1.0\n",
    "results = # TODO: Run evaluation and get results\n",
    "\n",
    "# TODO: Display detailed classification report\n",
    "# HINT: Define class_names list and use classification_report\n",
    "class_names = # TODO: Define list of class names ['Normal', 'Physical Anomaly', 'Network Anomaly']\n",
    "print(f\"\\\\nDetailed Classification Report:\")\n",
    "# TODO: Print classification report\n",
    "# HINT: Use classification_report(results['true_labels'], results['predictions'], target_names=class_names, digits=3)\n",
    "print(# TODO: Generate and print classification report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1af6b8",
   "metadata": {},
   "source": [
    "### Step 6C: Visualization\n",
    "\n",
    "Create focused visualizations for the balanced dataset results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111be215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comprehensive visualization and analysis function\n",
    "def visualize_confusion_matrix_and_metrics(results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for model evaluation results\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Create subplot figure with 1 row and 2 columns\n",
    "    # HINT: Use plt.subplots(1, 2, figsize=(15, 6))\n",
    "    fig, axes = # TODO: Create subplot figure\n",
    "    fig.suptitle('Multimodal Edge AI Model - Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # TODO: Plot 1 - Confusion Matrix\n",
    "    ax1 = axes[0]\n",
    "    class_names = ['Normal', 'Physical Anomaly', 'Network Anomaly']\n",
    "    # TODO: Calculate confusion matrix\n",
    "    # HINT: Use confusion_matrix(results['true_labels'], results['predictions'])\n",
    "    cm = # TODO: Calculate confusion matrix\n",
    "    \n",
    "    # TODO: Create heatmap for confusion matrix\n",
    "    # HINT: Use sns.heatmap with cm, annot=True, fmt='d', cmap='Blues'\n",
    "    # TODO: Create confusion matrix heatmap\n",
    "    ax1.set_title('Confusion Matrix')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    \n",
    "    # TODO: Plot 2 - Per-Class Performance\n",
    "    ax2 = axes[1]\n",
    "    # TODO: Calculate per-class precision, recall, and F1\n",
    "    # HINT: Use precision_score and recall_score with average=None\n",
    "    per_class_precision = # TODO: Calculate per-class precision\n",
    "    per_class_recall = # TODO: Calculate per-class recall\n",
    "    # TODO: Calculate F1 score manually\n",
    "    # HINT: F1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    per_class_f1 = # TODO: Calculate per-class F1 scores\n",
    "    \n",
    "    # TODO: Create bar chart positions\n",
    "    x_pos = np.arange(len(class_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    # TODO: Create bar charts for metrics\n",
    "    # HINT: Use ax2.bar with different x positions (x_pos - width, x_pos, x_pos + width)\n",
    "    # TODO: Plot precision bars\n",
    "    # TODO: Plot recall bars\n",
    "    # TODO: Plot F1 bars\n",
    "    \n",
    "    ax2.set_xlabel('Classes')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title('Per-Class Performance Metrics')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_ylim(0, 1.0)\n",
    "    \n",
    "    # TODO: Apply layout and show plot\n",
    "    # HINT: Use plt.tight_layout() and plt.show()\n",
    "    # TODO: Apply tight layout\n",
    "    # TODO: Show the plot\n",
    "    \n",
    "    # TODO: Print detailed per-class analysis\n",
    "    print(f\"\\\\nPer-Class Performance Analysis:\")\n",
    "    print(f\"{'Class':<15} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Support':<10}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # TODO: Calculate class support (number of samples per class)\n",
    "    # HINT: Use np.bincount(results['true_labels'])\n",
    "    class_support = # TODO: Calculate class support\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        if i < len(per_class_precision):\n",
    "            print(f\"{class_name:<15} {per_class_precision[i]:<10.3f} {per_class_recall[i]:<10.3f} \"\n",
    "                  f\"{per_class_f1[i]:<10.3f} {class_support[i]:<10d}\")\n",
    "\n",
    "# TODO: Create comprehensive evaluation visualizations\n",
    "# HINT: Call visualize_confusion_matrix_and_metrics with results\n",
    "# TODO: Visualize evaluation results\n",
    "\n",
    "# TODO: Advanced Network Anomaly Analysis\n",
    "print(f\"\\\\n\" + \"=\"*70)\n",
    "print(\"ADVANCED NETWORK ANOMALY DETECTION ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# TODO: Analyze probability distributions for each class\n",
    "# HINT: Set model to eval mode and use no_grad context\n",
    "# TODO: Set model to evaluation mode\n",
    "with # TODO: Create no_grad context:\n",
    "    # TODO: Get test data and move to device\n",
    "    X_vehicle_test = # TODO: Get vehicle test data on device\n",
    "    X_network_test = # TODO: Get network test data on device\n",
    "    y_test = # TODO: Get test labels\n",
    "    \n",
    "    # TODO: Get model outputs and probabilities\n",
    "    outputs = # TODO: Get model outputs\n",
    "    # TODO: Calculate softmax probabilities\n",
    "    # HINT: Use F.softmax(outputs, dim=1)\n",
    "    y_pred_proba = # TODO: Get prediction probabilities\n",
    "\n",
    "# TODO: Convert probabilities to numpy\n",
    "y_pred_proba_np = # TODO: Convert probabilities to numpy\n",
    "y_true_np = # TODO: Convert true labels to numpy\n",
    "\n",
    "print(f\"\\\\nProbability Distribution Analysis:\")\n",
    "for class_idx in range(3):\n",
    "    # TODO: Create mask for current class\n",
    "    class_mask = # TODO: Create boolean mask for class_idx\n",
    "    if class_mask.sum() > 0:\n",
    "        class_name = ['Normal', 'Physical Anomaly', 'Network Anomaly'][class_idx]\n",
    "        # TODO: Calculate confidence statistics for this class\n",
    "        # HINT: Get probabilities for samples of this class: y_pred_proba_np[class_mask, class_idx]\n",
    "        class_confidences = # TODO: Get confidence scores for this class\n",
    "        print(f\"   {class_name}:\")\n",
    "        print(f\"      Mean confidence: {class_confidences.mean():.3f}\")\n",
    "        print(f\"      Min confidence: {class_confidences.min():.3f}\")\n",
    "        print(f\"      Max confidence: {class_confidences.max():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
